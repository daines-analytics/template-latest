{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping of [PROJECT NAME] Using Python and BeautifulSoup Version 1\n",
    "### David Lowe\n",
    "### January 10, 2020\n",
    "\n",
    "SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping Python code leverages the BeautifulSoup module.\n",
    "\n",
    "INTRODUCTION: The Conference on Neural Information Processing Systems covers a wide range of topics in neural information processing systems and research for the biological, technological, mathematical, and theoretical applications. Neural information processing is a field which benefits from a combined view of biological, physical, mathematical, and computational sciences. This web scraping script will automatically traverse through the entire web page and collect all links to the PDF and PPTX documents. The script will also download the documents as part of the scraping process. The Python script ran in the Google Colaboratory environment and can be adapted to run in any Python environment without the Colab-specific configuration.\n",
    "\n",
    "Starting URLs: https://papers.nips.cc/book/advances-in-neural-information-processing-systems-32-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0. Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-Specific Setup - Refresh Linux package repositories and set up additional Linux and Python tools\n",
    "# !apt-get update\n",
    "# !apt install chromium-chromedriver\n",
    "# !pip install -q pymysql selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import smtplib\n",
    "import sys\n",
    "from email.message import EmailMessage\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "from requests.exceptions import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# Set up the verbose and debug flags to print detailed messages for debugging (setting True will activate!)\n",
    "verbose = True\n",
    "debug = False\n",
    "\n",
    "# Set up the flag to send status emails (setting to True will send the status emails!)\n",
    "notifyStatus = False\n",
    "\n",
    "# Set up the mountStorage flag to mount G Drive for storing files (setting True will mount the drive!)\n",
    "mountStorage = False\n",
    "\n",
    "# Set up the executeDownload flag to download files (setting True will download!)\n",
    "executeDownload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-Specific Setup - Mount Google Drive for storing downloaded files\n",
    "if (mountStorage):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the email notification function\n",
    "def email_notify(msg_text):\n",
    "    sender = os.environ.get('MAIL_SENDER')\n",
    "    receiver = os.environ.get('MAIL_RECEIVER')\n",
    "    gateway = os.environ.get('SMTP_GATEWAY')\n",
    "    smtpuser = os.environ.get('SMTP_USERNAME')\n",
    "    password = os.environ.get('SMTP_PASSWORD')\n",
    "    if sender==None or receiver==None or gateway==None or smtpuser==None or password==None:\n",
    "        sys.exit(\"Incomplete email setup info. Script Processing Aborted!!!\")\n",
    "    msg = EmailMessage()\n",
    "    msg.set_content(msg_text)\n",
    "    msg['Subject'] = 'Notification from Python Web Scraping Script'\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = receiver\n",
    "    server = smtplib.SMTP(gateway, 587)\n",
    "    server.starttls()\n",
    "    server.login(smtpuser, password)\n",
    "    server.send_message(msg)\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_to_local(doc_path):\n",
    "#    local_file = os.path.basename(doc_path)\n",
    "    local_file = doc_path.split('/')[-1]\n",
    "    with requests.get(doc_path, stream=True) as r:\n",
    "        with open(local_file, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    print('Downladed file: ' + local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_to_gdrive(doc_path):\n",
    "#    local_file = os.path.basename(doc_path)\n",
    "    local_file = doc_path.split('/')[-1]\n",
    "    gdrivePrefix = '/content/gdrive/My Drive/Colab_Downloads/'\n",
    "    dest_file = gdrivePrefix + local_file\n",
    "    with requests.get(doc_path, stream=True) as r:\n",
    "        with open(dest_file, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    print('Downladed file: ' + dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 0 Prepare Environment completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Perform the Scraping and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 1 Perform the Scraping and Processing has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the URL of desired web page to be scrapped\n",
    "starting_url = \"https://papers.nips.cc/book/advances-in-neural-information-processing-systems-32-2019\"\n",
    "website_url = \"https://papers.nips.cc\"\n",
    "\n",
    "# Creating an html document from the URL\n",
    "uastring = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:71.0) Gecko/20100101 Firefox/71.0\"\n",
    "headers={'User-Agent': uastring}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access and test the starting URL\n",
    "try:\n",
    "    s = requests.Session()\n",
    "    resp = s.get(starting_url, headers=headers)\n",
    "    if (debug): print(resp.text)\n",
    "except HTTPError as e:\n",
    "    print('The server could not serve up the web page!')\n",
    "    sys.exit(\"Script processing cannot continue!!!\")\n",
    "except ConnectionError as e:\n",
    "    print('The server could not be reached due to connection issues!')\n",
    "    sys.exit(\"Script processing cannot continue!!!\")\n",
    "\n",
    "if (resp.status_code==requests.codes.ok):\n",
    "    print('Successfully accessed the web page: ' + starting_url)\n",
    "    web_page = BeautifulSoup(resp.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gather all document links from the starting URL (One Level)\n",
    "# collection = web_page.find_all(\"a\")\n",
    "# i = 0\n",
    "\n",
    "# for item in collection:\n",
    "#     if (verbose): print(item)\n",
    "#     doc_path = item['href']\n",
    "#     if doc_path.lower().endswith(\".pdf\") | doc_path.lower().endswith(\".pptx\") | doc_path.lower().endswith(\".zip\"):\n",
    "#         i = i + 1\n",
    "#         doc_path = website_url + doc_path\n",
    "#         # Adding random wait time so we do not hammer the website needlessly\n",
    "#         print(\"Waiting \" + str(waitTime) + \" seconds to retrieve \" + doc_path)\n",
    "#         waitTime = randint(2,5)\n",
    "#         sleep(waitTime)\n",
    "#         if (executeDownload):\n",
    "#             if (mountStorage):\n",
    "#                 download_to_gdrive(doc_path)\n",
    "#             else:\n",
    "#                 download_to_local(doc_path)\n",
    "\n",
    "# print('Finished finding all available documents on the web page!')\n",
    "# print('Number of documents processed:', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all document links from the starting URL (Two Levels)\n",
    "collection = web_page.find_all('li')\n",
    "i = 0\n",
    "\n",
    "# Delete the first li element as it is not a regular list item we need\n",
    "collection.pop(0)\n",
    "\n",
    "for item in collection:\n",
    "    if (verbose): print(item)\n",
    "    doc_title = item.a.string\n",
    "    author_group = item.find_all('a', {'class':'author'})\n",
    "    author_list = []\n",
    "    for each_author in author_group:\n",
    "        author_list.append(each_author.string)\n",
    "    authors = ''.join(author_list)\n",
    "    doc_link = website_url + item.a['href']\n",
    "\n",
    "    # Adding random wait time so we do not hammer the website needlessly\n",
    "    waitTime = randint(2,5)\n",
    "    sleep(waitTime)\n",
    "    print(\"Waited \" + str(waitTime) + \" seconds to retrieve the next URL.\")\n",
    "    try:\n",
    "        s = requests.Session()\n",
    "        resp = s.get(doc_link, headers=headers)\n",
    "        if (debug): print(resp.text)\n",
    "    except HTTPError as e:\n",
    "        print('The server could not serve up the web page!')\n",
    "        sys.exit(\"Script processing cannot continue!!!\")\n",
    "    except ConnectionError as e:\n",
    "        print('The server could not be reached due to connection issues!')\n",
    "        sys.exit(\"Script processing cannot continue!!!\")\n",
    "\n",
    "    if (resp.status_code==requests.codes.ok):\n",
    "        print('Successfully accessed the document page: ' + doc_link)\n",
    "        doc_page = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "    artifact_list = doc_page.find('div', class_=\"main wrapper clearfix\").find_all('a')\n",
    "    for artifact_item in artifact_list:\n",
    "        if artifact_item.string == \"[PDF]\":\n",
    "            doc_path = website_url + artifact_item['href']\n",
    "            # Adding random wait time so we do not hammer the website needlessly\n",
    "            print(\"Waiting \" + str(waitTime) + \" seconds to retrieve \" + doc_path)\n",
    "            waitTime = randint(2,5)\n",
    "            sleep(waitTime)\n",
    "            if (executeDownload):\n",
    "                if (mountStorage):\n",
    "                    download_to_gdrive(doc_path)\n",
    "                else:\n",
    "                    download_to_local(doc_path)\n",
    "\n",
    "    abstract = doc_page.find('p', class_=\"abstract\").string\n",
    "    i = i + 1\n",
    "\n",
    "print('Finished finding all available documents on the web pages!')\n",
    "print('Number of documents processed:', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 1 Perform the Scraping and Processing completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Total time for the script:',(datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
