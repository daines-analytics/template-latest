{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Deep Learning Model for [Project Name] Using Keras Version 3\n",
    "### David Lowe\n",
    "### November 1, 2019\n",
    "Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [https://machinelearningmastery.com/]\n",
    "\n",
    "SUMMARY: [Sample Paragraph - The purpose of this project is to construct a predictive model using various machine learning algorithms and to document the end-to-end steps using a template. The Connectionist Bench dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.]\n",
    "\n",
    "INTRODUCTION: [Sample Paragraph - The data file patterns obtained by bouncing sonar signals off a metal cylinder or a rock at various angles and under various conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock. Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time.]\n",
    "\n",
    "ANALYSIS: [Sample Paragraph - The baseline performance of the model achieved an average accuracy score of 80.12%. Using the same training parameters, the model processed the test dataset with an accuracy of 82.69%, which was even better than results from the training data.]\n",
    "\n",
    "CONCLUSION: [Sample Paragraph - For this dataset, the model built using Keras and TensorFlow achieved a satisfactory result and should be considered for future modeling activities]\n",
    "\n",
    "Dataset Used: [Connectionist Bench (Sonar, Mines vs. Rocks) Data Set]\n",
    "\n",
    "Dataset ML Model: Binary classification with numerical attributes\n",
    "\n",
    "Dataset Reference: [https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+%28Sonar,+Mines+vs.+Rocks%29]\n",
    "\n",
    "One potential source of performance benchmarks: [https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/]\n",
    "\n",
    "Any deep-learning modeling project genrally can be broken down into about six major tasks:\n",
    "0. Prepare Environment\n",
    "1. Load Data\n",
    "2. Define Model\n",
    "3. Fit and Evaluate Model\n",
    "4. Optimize Model\n",
    "5. Finalize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0. Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random seed numbers for reproducible results\n",
    "seedNum = 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and packages\n",
    "import random\n",
    "random.seed(seedNum)\n",
    "import numpy as np\n",
    "np.random.seed(seedNum)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seedNum)\n",
    "import pandas as pd\n",
    "import os\n",
    "import smtplib\n",
    "from datetime import datetime\n",
    "from email.message import EmailMessage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Configure a new global `tensorflow` session\n",
    "import keras as K\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# Set up the flag to stop sending progress emails (setting to True will send status emails!)\n",
    "notifyStatus = False\n",
    "\n",
    "# Set the flag for splitting the dataset\n",
    "splitDataset = True\n",
    "splitPercentage = 0.25\n",
    "\n",
    "# Set various default Keras modeling parameters\n",
    "default_kernel_init = K.initializers.RandomNormal(seed=seedNum)\n",
    "default_loss = 'binary_crossentropy'\n",
    "default_optimizer = 'adam'\n",
    "default_epochs = 100\n",
    "default_batches = 5\n",
    "default_metrics = ['accuracy']\n",
    "\n",
    "# Set the number of folds for cross validation\n",
    "folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the email notification function\n",
    "def email_notify(msg_text):\n",
    "    sender = os.environ.get('MAIL_SENDER')\n",
    "    receiver = os.environ.get('MAIL_RECEIVER')\n",
    "    gateway = os.environ.get('SMTP_GATEWAY')\n",
    "    smtpuser = os.environ.get('SMTP_USERNAME')\n",
    "    password = os.environ.get('SMTP_PASSWORD')\n",
    "    if sender==None or receiver==None or gateway==None or smtpuser==None or password==None:\n",
    "        sys.exit(\"Incomplete email setup info. Script Processing Aborted!!!\")\n",
    "    msg = EmailMessage()\n",
    "    msg.set_content(msg_text)\n",
    "    msg['Subject'] = 'Notification from Keras Binary Classification Script'\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = receiver\n",
    "    server = smtplib.SMTP(gateway, 587)\n",
    "    server.starttls()\n",
    "    server.login(smtpuser, password)\n",
    "    server.send_message(msg)\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 0 Prepare Environment completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 1 Load Data has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02 0.0371 0.0428 ... 0.009 0.0032 'R']\n",
      " [0.0453 0.0523 0.0843 ... 0.0052 0.0044 'R']\n",
      " [0.0262 0.0582 0.1099 ... 0.0095 0.0078 'R']\n",
      " ...\n",
      " [0.0522 0.0437 0.018 ... 0.0077 0.0031 'M']\n",
      " [0.0303 0.0353 0.049 ... 0.0036 0.0048 'M']\n",
      " [0.026 0.0363 0.0136 ... 0.0061 0.0115 'M']]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df_original = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\", header=None)\n",
    "dataset = df_original.values\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_original: (208, 60) | Shape of y_original: (208,)\n"
     ]
    }
   ],
   "source": [
    "# Split the original dataset into input (X) and output (y) variables\n",
    "X_original = dataset[:,0:60].astype(float)\n",
    "y_original = dataset[:,60]\n",
    "print('Shape of X_original:', X_original.shape, '| Shape of y_original:', y_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Encode class values as integers and perform one-hot-encoding\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_original)\n",
    "y_encoded = encoder.transform(y_original)\n",
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (156, 60) | Shape of y_train: (156,)\n",
      "Shape of X_test: (52, 60) | Shape of y_test: (52,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data further into training and test datasets\n",
    "if (splitDataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_original, y_encoded, test_size=splitPercentage, random_state=seedNum)\n",
    "else:\n",
    "    X_train, y_train = X_original, y_encoded\n",
    "    X_test, y_test = X_original, y_encoded\n",
    "print('Shape of X_train:', X_train.shape, '| Shape of y_train:', y_train.shape)\n",
    "print('Shape of X_test:', X_test.shape, '| Shape of y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 1 Load Data completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 2 Define Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model required for KerasClassifier\n",
    "def create_default_model():\n",
    "    default_model = K.models.Sequential()\n",
    "    default_model.add(Dense(60, input_dim=60, kernel_initializer=default_kernel_init, activation='relu'))\n",
    "    default_model.add(Dense(1, kernel_initializer=default_kernel_init, activation='sigmoid'))\n",
    "    default_model.compile(loss=default_loss, optimizer=default_optimizer, metrics=default_metrics)\n",
    "    return default_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Keras model\n",
    "cv_model = KerasClassifier(build_fn=create_default_model, epochs=default_epochs, batch_size=default_batches, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 2 Define Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Fit and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 3 Fit and Evaluate Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Generating results using the metrics of ['accuracy']\n",
      "All cross-Validate results: [0.70588237 0.81250001 0.87500001 0.68750001 0.75000001 0.93333334\n",
      " 0.73333335 0.80000001 0.60000001 0.93333334]\n",
      "Baseline results [mean (std)]: 78.31% (10.34%)\n"
     ]
    }
   ],
   "source": [
    "# Fit and evaluate the Keras model using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seedNum)\n",
    "results = cross_val_score(cv_model, X_train, y_train, cv=kfold)\n",
    "print('Generating results using the metrics of', default_metrics)\n",
    "print('All cross-Validate results:', results)\n",
    "print('Baseline results [mean (std)]: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Pb01NDTS44-"
   },
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 3 Fit and Evaluate Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 4 Optimize Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model required for KerasClassifier\n",
    "def create_customized_model(optimizer, kernel_init):\n",
    "    customized_model = K.models.Sequential()\n",
    "    customized_model.add(Dense(60, input_dim=60, kernel_initializer=kernel_init, activation='relu'))\n",
    "    customized_model.add(Dense(1, kernel_initializer=kernel_init, activation='sigmoid'))\n",
    "    customized_model.compile(loss=default_loss, optimizer=optimizer, metrics=default_metrics)\n",
    "    return customized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.794872 using {'batch_size': 5, 'epochs': 200, 'kernel_init': 'RandomNormal', 'optimizer': 'adam'}\n",
      "0.544872 (0.054248) with: {'batch_size': 5, 'epochs': 100, 'kernel_init': 'Constant', 'optimizer': 'rmsprop'}\n",
      "0.544872 (0.054248) with: {'batch_size': 5, 'epochs': 100, 'kernel_init': 'Constant', 'optimizer': 'adam'}\n",
      "0.762821 (0.067714) with: {'batch_size': 5, 'epochs': 100, 'kernel_init': 'RandomNormal', 'optimizer': 'rmsprop'}\n",
      "0.788462 (0.060694) with: {'batch_size': 5, 'epochs': 100, 'kernel_init': 'RandomNormal', 'optimizer': 'adam'}\n",
      "0.756410 (0.079483) with: {'batch_size': 5, 'epochs': 100, 'kernel_init': 'RandomUniform', 'optimizer': 'rmsprop'}\n",
      "0.769231 (0.075656) with: {'batch_size': 5, 'epochs': 100, 'kernel_init': 'RandomUniform', 'optimizer': 'adam'}\n",
      "0.544872 (0.054248) with: {'batch_size': 5, 'epochs': 150, 'kernel_init': 'Constant', 'optimizer': 'rmsprop'}\n",
      "0.544872 (0.054248) with: {'batch_size': 5, 'epochs': 150, 'kernel_init': 'Constant', 'optimizer': 'adam'}\n",
      "0.775641 (0.079791) with: {'batch_size': 5, 'epochs': 150, 'kernel_init': 'RandomNormal', 'optimizer': 'rmsprop'}\n",
      "0.782051 (0.048032) with: {'batch_size': 5, 'epochs': 150, 'kernel_init': 'RandomNormal', 'optimizer': 'adam'}\n",
      "0.782051 (0.048766) with: {'batch_size': 5, 'epochs': 150, 'kernel_init': 'RandomUniform', 'optimizer': 'rmsprop'}\n",
      "0.769231 (0.066423) with: {'batch_size': 5, 'epochs': 150, 'kernel_init': 'RandomUniform', 'optimizer': 'adam'}\n",
      "0.544872 (0.054248) with: {'batch_size': 5, 'epochs': 200, 'kernel_init': 'Constant', 'optimizer': 'rmsprop'}\n",
      "0.544872 (0.054248) with: {'batch_size': 5, 'epochs': 200, 'kernel_init': 'Constant', 'optimizer': 'adam'}\n",
      "0.782051 (0.043515) with: {'batch_size': 5, 'epochs': 200, 'kernel_init': 'RandomNormal', 'optimizer': 'rmsprop'}\n",
      "0.794872 (0.060552) with: {'batch_size': 5, 'epochs': 200, 'kernel_init': 'RandomNormal', 'optimizer': 'adam'}\n",
      "0.775641 (0.046475) with: {'batch_size': 5, 'epochs': 200, 'kernel_init': 'RandomUniform', 'optimizer': 'rmsprop'}\n",
      "0.788462 (0.033635) with: {'batch_size': 5, 'epochs': 200, 'kernel_init': 'RandomUniform', 'optimizer': 'adam'}\n",
      "0.544872 (0.054248) with: {'batch_size': 10, 'epochs': 100, 'kernel_init': 'Constant', 'optimizer': 'rmsprop'}\n",
      "0.544872 (0.054248) with: {'batch_size': 10, 'epochs': 100, 'kernel_init': 'Constant', 'optimizer': 'adam'}\n",
      "0.769231 (0.055564) with: {'batch_size': 10, 'epochs': 100, 'kernel_init': 'RandomNormal', 'optimizer': 'rmsprop'}\n",
      "0.775641 (0.071596) with: {'batch_size': 10, 'epochs': 100, 'kernel_init': 'RandomNormal', 'optimizer': 'adam'}\n",
      "0.775641 (0.068647) with: {'batch_size': 10, 'epochs': 100, 'kernel_init': 'RandomUniform', 'optimizer': 'rmsprop'}\n",
      "0.769231 (0.060465) with: {'batch_size': 10, 'epochs': 100, 'kernel_init': 'RandomUniform', 'optimizer': 'adam'}\n",
      "0.544872 (0.054248) with: {'batch_size': 10, 'epochs': 150, 'kernel_init': 'Constant', 'optimizer': 'rmsprop'}\n",
      "0.544872 (0.054248) with: {'batch_size': 10, 'epochs': 150, 'kernel_init': 'Constant', 'optimizer': 'adam'}\n",
      "0.794872 (0.081639) with: {'batch_size': 10, 'epochs': 150, 'kernel_init': 'RandomNormal', 'optimizer': 'rmsprop'}\n",
      "0.788462 (0.063453) with: {'batch_size': 10, 'epochs': 150, 'kernel_init': 'RandomNormal', 'optimizer': 'adam'}\n",
      "0.775641 (0.068647) with: {'batch_size': 10, 'epochs': 150, 'kernel_init': 'RandomUniform', 'optimizer': 'rmsprop'}\n",
      "0.782051 (0.066678) with: {'batch_size': 10, 'epochs': 150, 'kernel_init': 'RandomUniform', 'optimizer': 'adam'}\n",
      "0.544872 (0.054248) with: {'batch_size': 10, 'epochs': 200, 'kernel_init': 'Constant', 'optimizer': 'rmsprop'}\n",
      "0.544872 (0.054248) with: {'batch_size': 10, 'epochs': 200, 'kernel_init': 'Constant', 'optimizer': 'adam'}\n",
      "0.794872 (0.067035) with: {'batch_size': 10, 'epochs': 200, 'kernel_init': 'RandomNormal', 'optimizer': 'rmsprop'}\n",
      "0.788462 (0.060694) with: {'batch_size': 10, 'epochs': 200, 'kernel_init': 'RandomNormal', 'optimizer': 'adam'}\n",
      "0.762821 (0.084063) with: {'batch_size': 10, 'epochs': 200, 'kernel_init': 'RandomUniform', 'optimizer': 'rmsprop'}\n",
      "0.775641 (0.046475) with: {'batch_size': 10, 'epochs': 200, 'kernel_init': 'RandomUniform', 'optimizer': 'adam'}\n",
      "0.544872 (0.054248) with: {'batch_size': 15, 'epochs': 100, 'kernel_init': 'Constant', 'optimizer': 'rmsprop'}\n",
      "0.544872 (0.054248) with: {'batch_size': 15, 'epochs': 100, 'kernel_init': 'Constant', 'optimizer': 'adam'}\n",
      "0.782051 (0.063501) with: {'batch_size': 15, 'epochs': 100, 'kernel_init': 'RandomNormal', 'optimizer': 'rmsprop'}\n",
      "0.775641 (0.079791) with: {'batch_size': 15, 'epochs': 100, 'kernel_init': 'RandomNormal', 'optimizer': 'adam'}\n",
      "0.769231 (0.072871) with: {'batch_size': 15, 'epochs': 100, 'kernel_init': 'RandomUniform', 'optimizer': 'rmsprop'}\n",
      "0.775641 (0.059413) with: {'batch_size': 15, 'epochs': 100, 'kernel_init': 'RandomUniform', 'optimizer': 'adam'}\n",
      "0.544872 (0.054248) with: {'batch_size': 15, 'epochs': 150, 'kernel_init': 'Constant', 'optimizer': 'rmsprop'}\n",
      "0.544872 (0.054248) with: {'batch_size': 15, 'epochs': 150, 'kernel_init': 'Constant', 'optimizer': 'adam'}\n",
      "0.769231 (0.099814) with: {'batch_size': 15, 'epochs': 150, 'kernel_init': 'RandomNormal', 'optimizer': 'rmsprop'}\n",
      "0.782051 (0.069710) with: {'batch_size': 15, 'epochs': 150, 'kernel_init': 'RandomNormal', 'optimizer': 'adam'}\n",
      "0.756410 (0.045305) with: {'batch_size': 15, 'epochs': 150, 'kernel_init': 'RandomUniform', 'optimizer': 'rmsprop'}\n",
      "0.769231 (0.060465) with: {'batch_size': 15, 'epochs': 150, 'kernel_init': 'RandomUniform', 'optimizer': 'adam'}\n",
      "0.544872 (0.054248) with: {'batch_size': 15, 'epochs': 200, 'kernel_init': 'Constant', 'optimizer': 'rmsprop'}\n",
      "0.544872 (0.054248) with: {'batch_size': 15, 'epochs': 200, 'kernel_init': 'Constant', 'optimizer': 'adam'}\n",
      "0.769231 (0.085481) with: {'batch_size': 15, 'epochs': 200, 'kernel_init': 'RandomNormal', 'optimizer': 'rmsprop'}\n",
      "0.788462 (0.052779) with: {'batch_size': 15, 'epochs': 200, 'kernel_init': 'RandomNormal', 'optimizer': 'adam'}\n",
      "0.769231 (0.095344) with: {'batch_size': 15, 'epochs': 200, 'kernel_init': 'RandomUniform', 'optimizer': 'rmsprop'}\n",
      "0.782051 (0.066678) with: {'batch_size': 15, 'epochs': 200, 'kernel_init': 'RandomUniform', 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "grid_model = KerasClassifier(build_fn=create_customized_model, verbose=0)\n",
    "\n",
    "# Perform grid search using different epochs, batch sizes, and optimizers\n",
    "optimizer_grid = ['rmsprop', 'adam']\n",
    "init_grid = ['Constant', 'RandomNormal', 'RandomUniform']\n",
    "epoch_grid = [100, 150, 200]\n",
    "batch_grid = [5, 10, 15]\n",
    "param_grid = dict(optimizer=optimizer_grid, kernel_init=init_grid, epochs=epoch_grid, batch_size=batch_grid)\n",
    "grid = GridSearchCV(estimator=grid_model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optimizer = 'rmsprop'\n",
    "best_kernel_init = 'RandomUniform'\n",
    "best_epoch = 200\n",
    "best_batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 4 Optimize Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Finalize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 5 Finalize Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final model for evaluating the test dataset\n",
    "final_model = create_customized_model(best_optimizer, best_kernel_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "156/156 [==============================] - 2s 12ms/step - loss: 0.6908 - acc: 0.5128\n",
      "Epoch 2/200\n",
      "156/156 [==============================] - 0s 417us/step - loss: 0.6839 - acc: 0.5449\n",
      "Epoch 3/200\n",
      "156/156 [==============================] - 0s 414us/step - loss: 0.6789 - acc: 0.5449\n",
      "Epoch 4/200\n",
      "156/156 [==============================] - 0s 421us/step - loss: 0.6734 - acc: 0.5449\n",
      "Epoch 5/200\n",
      "156/156 [==============================] - 0s 697us/step - loss: 0.6680 - acc: 0.5513\n",
      "Epoch 6/200\n",
      "156/156 [==============================] - 0s 421us/step - loss: 0.6619 - acc: 0.5962\n",
      "Epoch 7/200\n",
      "156/156 [==============================] - 0s 417us/step - loss: 0.6529 - acc: 0.6282\n",
      "Epoch 8/200\n",
      "156/156 [==============================] - 0s 432us/step - loss: 0.6430 - acc: 0.6667\n",
      "Epoch 9/200\n",
      "156/156 [==============================] - 0s 440us/step - loss: 0.6370 - acc: 0.6795\n",
      "Epoch 10/200\n",
      "156/156 [==============================] - 0s 417us/step - loss: 0.6250 - acc: 0.6603\n",
      "Epoch 11/200\n",
      "156/156 [==============================] - 0s 378us/step - loss: 0.6145 - acc: 0.6923\n",
      "Epoch 12/200\n",
      "156/156 [==============================] - 0s 447us/step - loss: 0.6052 - acc: 0.7179\n",
      "Epoch 13/200\n",
      "156/156 [==============================] - 0s 407us/step - loss: 0.5934 - acc: 0.7051\n",
      "Epoch 14/200\n",
      "156/156 [==============================] - 0s 441us/step - loss: 0.5857 - acc: 0.7436\n",
      "Epoch 15/200\n",
      "156/156 [==============================] - 0s 448us/step - loss: 0.5727 - acc: 0.7179\n",
      "Epoch 16/200\n",
      "156/156 [==============================] - 0s 463us/step - loss: 0.5661 - acc: 0.7756\n",
      "Epoch 17/200\n",
      "156/156 [==============================] - 0s 376us/step - loss: 0.5539 - acc: 0.7885\n",
      "Epoch 18/200\n",
      "156/156 [==============================] - 0s 423us/step - loss: 0.5451 - acc: 0.7564\n",
      "Epoch 19/200\n",
      "156/156 [==============================] - 0s 425us/step - loss: 0.5380 - acc: 0.7436\n",
      "Epoch 20/200\n",
      "156/156 [==============================] - 0s 442us/step - loss: 0.5269 - acc: 0.7756\n",
      "Epoch 21/200\n",
      "156/156 [==============================] - 0s 442us/step - loss: 0.5158 - acc: 0.7821\n",
      "Epoch 22/200\n",
      "156/156 [==============================] - 0s 458us/step - loss: 0.5057 - acc: 0.8205\n",
      "Epoch 23/200\n",
      "156/156 [==============================] - 0s 375us/step - loss: 0.4994 - acc: 0.7756\n",
      "Epoch 24/200\n",
      "156/156 [==============================] - 0s 440us/step - loss: 0.4870 - acc: 0.8205\n",
      "Epoch 25/200\n",
      "156/156 [==============================] - 0s 417us/step - loss: 0.4930 - acc: 0.7756\n",
      "Epoch 26/200\n",
      "156/156 [==============================] - 0s 468us/step - loss: 0.4810 - acc: 0.8013\n",
      "Epoch 27/200\n",
      "156/156 [==============================] - 0s 386us/step - loss: 0.4796 - acc: 0.8269\n",
      "Epoch 28/200\n",
      "156/156 [==============================] - 0s 436us/step - loss: 0.4717 - acc: 0.8333\n",
      "Epoch 29/200\n",
      "156/156 [==============================] - 0s 399us/step - loss: 0.4697 - acc: 0.8205\n",
      "Epoch 30/200\n",
      "156/156 [==============================] - 0s 437us/step - loss: 0.4635 - acc: 0.8013\n",
      "Epoch 31/200\n",
      "156/156 [==============================] - 0s 455us/step - loss: 0.4571 - acc: 0.8205\n",
      "Epoch 32/200\n",
      "156/156 [==============================] - 0s 467us/step - loss: 0.4530 - acc: 0.8141\n",
      "Epoch 33/200\n",
      "156/156 [==============================] - 0s 467us/step - loss: 0.4448 - acc: 0.8141\n",
      "Epoch 34/200\n",
      "156/156 [==============================] - 0s 419us/step - loss: 0.4512 - acc: 0.8397\n",
      "Epoch 35/200\n",
      "156/156 [==============================] - 0s 426us/step - loss: 0.4421 - acc: 0.8333\n",
      "Epoch 36/200\n",
      "156/156 [==============================] - 0s 422us/step - loss: 0.4377 - acc: 0.8269\n",
      "Epoch 37/200\n",
      "156/156 [==============================] - 0s 385us/step - loss: 0.4377 - acc: 0.8462\n",
      "Epoch 38/200\n",
      "156/156 [==============================] - 0s 440us/step - loss: 0.4292 - acc: 0.8205\n",
      "Epoch 39/200\n",
      "156/156 [==============================] - 0s 428us/step - loss: 0.4279 - acc: 0.8141\n",
      "Epoch 40/200\n",
      "156/156 [==============================] - 0s 415us/step - loss: 0.4248 - acc: 0.8269\n",
      "Epoch 41/200\n",
      "156/156 [==============================] - 0s 471us/step - loss: 0.4278 - acc: 0.8269\n",
      "Epoch 42/200\n",
      "156/156 [==============================] - 0s 489us/step - loss: 0.4229 - acc: 0.8013\n",
      "Epoch 43/200\n",
      "156/156 [==============================] - 0s 412us/step - loss: 0.4225 - acc: 0.8013\n",
      "Epoch 44/200\n",
      "156/156 [==============================] - 0s 447us/step - loss: 0.4155 - acc: 0.8077\n",
      "Epoch 45/200\n",
      "156/156 [==============================] - 0s 402us/step - loss: 0.4181 - acc: 0.8205\n",
      "Epoch 46/200\n",
      "156/156 [==============================] - 0s 367us/step - loss: 0.4145 - acc: 0.8077\n",
      "Epoch 47/200\n",
      "156/156 [==============================] - 0s 419us/step - loss: 0.4088 - acc: 0.8397\n",
      "Epoch 48/200\n",
      "156/156 [==============================] - 0s 385us/step - loss: 0.4059 - acc: 0.8397\n",
      "Epoch 49/200\n",
      "156/156 [==============================] - 0s 450us/step - loss: 0.4095 - acc: 0.8333\n",
      "Epoch 50/200\n",
      "156/156 [==============================] - 0s 454us/step - loss: 0.4049 - acc: 0.8397\n",
      "Epoch 51/200\n",
      "156/156 [==============================] - 0s 427us/step - loss: 0.4028 - acc: 0.8205\n",
      "Epoch 52/200\n",
      "156/156 [==============================] - 0s 458us/step - loss: 0.4011 - acc: 0.8333\n",
      "Epoch 53/200\n",
      "156/156 [==============================] - 0s 423us/step - loss: 0.4001 - acc: 0.8397\n",
      "Epoch 54/200\n",
      "156/156 [==============================] - 0s 424us/step - loss: 0.4012 - acc: 0.8141\n",
      "Epoch 55/200\n",
      "156/156 [==============================] - 0s 422us/step - loss: 0.4054 - acc: 0.8141\n",
      "Epoch 56/200\n",
      "156/156 [==============================] - 0s 394us/step - loss: 0.3999 - acc: 0.8013\n",
      "Epoch 57/200\n",
      "156/156 [==============================] - 0s 424us/step - loss: 0.3986 - acc: 0.8397\n",
      "Epoch 58/200\n",
      "156/156 [==============================] - 0s 443us/step - loss: 0.3930 - acc: 0.8333\n",
      "Epoch 59/200\n",
      "156/156 [==============================] - 0s 379us/step - loss: 0.3942 - acc: 0.8141\n",
      "Epoch 60/200\n",
      "156/156 [==============================] - 0s 440us/step - loss: 0.3958 - acc: 0.8013\n",
      "Epoch 61/200\n",
      "156/156 [==============================] - 0s 448us/step - loss: 0.3931 - acc: 0.8205\n",
      "Epoch 62/200\n",
      "156/156 [==============================] - 0s 418us/step - loss: 0.3877 - acc: 0.8333\n",
      "Epoch 63/200\n",
      "156/156 [==============================] - 0s 424us/step - loss: 0.3864 - acc: 0.8462\n",
      "Epoch 64/200\n",
      "156/156 [==============================] - 0s 446us/step - loss: 0.3886 - acc: 0.8269\n",
      "Epoch 65/200\n",
      "156/156 [==============================] - 0s 443us/step - loss: 0.3856 - acc: 0.8397\n",
      "Epoch 66/200\n",
      "156/156 [==============================] - 0s 373us/step - loss: 0.3808 - acc: 0.8333\n",
      "Epoch 67/200\n",
      "156/156 [==============================] - 0s 374us/step - loss: 0.3841 - acc: 0.8077\n",
      "Epoch 68/200\n",
      "156/156 [==============================] - 0s 373us/step - loss: 0.3801 - acc: 0.8269\n",
      "Epoch 69/200\n",
      "156/156 [==============================] - 0s 373us/step - loss: 0.3836 - acc: 0.8269\n",
      "Epoch 70/200\n",
      "156/156 [==============================] - 0s 442us/step - loss: 0.3799 - acc: 0.8013\n",
      "Epoch 71/200\n",
      "156/156 [==============================] - 0s 375us/step - loss: 0.3771 - acc: 0.8141\n",
      "Epoch 72/200\n",
      "156/156 [==============================] - 0s 366us/step - loss: 0.3761 - acc: 0.8333\n",
      "Epoch 73/200\n",
      "156/156 [==============================] - 0s 368us/step - loss: 0.3717 - acc: 0.8397\n",
      "Epoch 74/200\n",
      "156/156 [==============================] - 0s 370us/step - loss: 0.3780 - acc: 0.8141\n",
      "Epoch 75/200\n",
      "156/156 [==============================] - 0s 364us/step - loss: 0.3786 - acc: 0.8333\n",
      "Epoch 76/200\n",
      "156/156 [==============================] - 0s 365us/step - loss: 0.3786 - acc: 0.8269\n",
      "Epoch 77/200\n",
      "156/156 [==============================] - 0s 361us/step - loss: 0.3768 - acc: 0.8205\n",
      "Epoch 78/200\n",
      "156/156 [==============================] - 0s 366us/step - loss: 0.3678 - acc: 0.8141\n",
      "Epoch 79/200\n",
      "156/156 [==============================] - 0s 365us/step - loss: 0.3661 - acc: 0.8462\n",
      "Epoch 80/200\n",
      "156/156 [==============================] - 0s 419us/step - loss: 0.3735 - acc: 0.8269\n",
      "Epoch 81/200\n",
      "156/156 [==============================] - 0s 438us/step - loss: 0.3696 - acc: 0.8141\n",
      "Epoch 82/200\n",
      "156/156 [==============================] - 0s 437us/step - loss: 0.3707 - acc: 0.8269\n",
      "Epoch 83/200\n",
      "156/156 [==============================] - 0s 422us/step - loss: 0.3644 - acc: 0.8333\n",
      "Epoch 84/200\n",
      "156/156 [==============================] - 0s 430us/step - loss: 0.3658 - acc: 0.8269\n",
      "Epoch 85/200\n",
      "156/156 [==============================] - 0s 440us/step - loss: 0.3737 - acc: 0.8013\n",
      "Epoch 86/200\n",
      "156/156 [==============================] - 0s 424us/step - loss: 0.3658 - acc: 0.8526\n",
      "Epoch 87/200\n",
      "156/156 [==============================] - 0s 378us/step - loss: 0.3707 - acc: 0.8205\n",
      "Epoch 88/200\n",
      "156/156 [==============================] - 0s 443us/step - loss: 0.3626 - acc: 0.8205\n",
      "Epoch 89/200\n",
      "156/156 [==============================] - 0s 445us/step - loss: 0.3575 - acc: 0.8397\n",
      "Epoch 90/200\n",
      "156/156 [==============================] - 0s 434us/step - loss: 0.3635 - acc: 0.8141\n",
      "Epoch 91/200\n",
      "156/156 [==============================] - 0s 399us/step - loss: 0.3640 - acc: 0.8205\n",
      "Epoch 92/200\n",
      "156/156 [==============================] - 0s 409us/step - loss: 0.3671 - acc: 0.8269\n",
      "Epoch 93/200\n",
      "156/156 [==============================] - 0s 485us/step - loss: 0.3517 - acc: 0.8333\n",
      "Epoch 94/200\n",
      "156/156 [==============================] - 0s 446us/step - loss: 0.3522 - acc: 0.8333\n",
      "Epoch 95/200\n",
      "156/156 [==============================] - 0s 512us/step - loss: 0.3595 - acc: 0.8269\n",
      "Epoch 96/200\n",
      "156/156 [==============================] - 0s 476us/step - loss: 0.3566 - acc: 0.8462\n",
      "Epoch 97/200\n",
      "156/156 [==============================] - 0s 436us/step - loss: 0.3574 - acc: 0.8333\n",
      "Epoch 98/200\n",
      "156/156 [==============================] - 0s 429us/step - loss: 0.3538 - acc: 0.8269\n",
      "Epoch 99/200\n",
      "156/156 [==============================] - 0s 425us/step - loss: 0.3524 - acc: 0.8333\n",
      "Epoch 100/200\n",
      "156/156 [==============================] - 0s 449us/step - loss: 0.3522 - acc: 0.8269\n",
      "Epoch 101/200\n",
      "156/156 [==============================] - 0s 413us/step - loss: 0.3509 - acc: 0.8269\n",
      "Epoch 102/200\n",
      "156/156 [==============================] - 0s 429us/step - loss: 0.3429 - acc: 0.8654\n",
      "Epoch 103/200\n",
      "156/156 [==============================] - 0s 429us/step - loss: 0.3508 - acc: 0.8333\n",
      "Epoch 104/200\n",
      "156/156 [==============================] - 0s 473us/step - loss: 0.3447 - acc: 0.8205\n",
      "Epoch 105/200\n",
      "156/156 [==============================] - 0s 437us/step - loss: 0.3434 - acc: 0.8141\n",
      "Epoch 106/200\n",
      "156/156 [==============================] - 0s 500us/step - loss: 0.3399 - acc: 0.8526\n",
      "Epoch 107/200\n",
      "156/156 [==============================] - 0s 420us/step - loss: 0.3486 - acc: 0.8269\n",
      "Epoch 108/200\n",
      "156/156 [==============================] - 0s 418us/step - loss: 0.3490 - acc: 0.8141\n",
      "Epoch 109/200\n",
      "156/156 [==============================] - 0s 417us/step - loss: 0.3450 - acc: 0.8205\n",
      "Epoch 110/200\n",
      "156/156 [==============================] - 0s 441us/step - loss: 0.3430 - acc: 0.8397\n",
      "Epoch 111/200\n",
      "156/156 [==============================] - 0s 429us/step - loss: 0.3474 - acc: 0.8333\n",
      "Epoch 112/200\n",
      "156/156 [==============================] - 0s 411us/step - loss: 0.3359 - acc: 0.8526\n",
      "Epoch 113/200\n",
      "156/156 [==============================] - 0s 438us/step - loss: 0.3448 - acc: 0.8333\n",
      "Epoch 114/200\n",
      "156/156 [==============================] - 0s 434us/step - loss: 0.3412 - acc: 0.8397\n",
      "Epoch 115/200\n",
      "156/156 [==============================] - 0s 422us/step - loss: 0.3388 - acc: 0.8590\n",
      "Epoch 116/200\n",
      "156/156 [==============================] - 0s 402us/step - loss: 0.3420 - acc: 0.8526\n",
      "Epoch 117/200\n",
      "156/156 [==============================] - 0s 445us/step - loss: 0.3435 - acc: 0.8205\n",
      "Epoch 118/200\n",
      "156/156 [==============================] - 0s 424us/step - loss: 0.3398 - acc: 0.8333\n",
      "Epoch 119/200\n",
      "156/156 [==============================] - 0s 410us/step - loss: 0.3355 - acc: 0.8333\n",
      "Epoch 120/200\n",
      "156/156 [==============================] - 0s 409us/step - loss: 0.3364 - acc: 0.8333\n",
      "Epoch 121/200\n",
      "156/156 [==============================] - 0s 463us/step - loss: 0.3346 - acc: 0.8397\n",
      "Epoch 122/200\n",
      "156/156 [==============================] - 0s 408us/step - loss: 0.3274 - acc: 0.8590\n",
      "Epoch 123/200\n",
      "156/156 [==============================] - 0s 413us/step - loss: 0.3330 - acc: 0.8462\n",
      "Epoch 124/200\n",
      "156/156 [==============================] - 0s 409us/step - loss: 0.3274 - acc: 0.8397\n",
      "Epoch 125/200\n",
      "156/156 [==============================] - 0s 453us/step - loss: 0.3315 - acc: 0.8397\n",
      "Epoch 126/200\n",
      "156/156 [==============================] - 0s 418us/step - loss: 0.3351 - acc: 0.8526\n",
      "Epoch 127/200\n",
      "156/156 [==============================] - 0s 412us/step - loss: 0.3347 - acc: 0.8397\n",
      "Epoch 128/200\n",
      "156/156 [==============================] - 0s 438us/step - loss: 0.3245 - acc: 0.8462\n",
      "Epoch 129/200\n",
      "156/156 [==============================] - 0s 422us/step - loss: 0.3247 - acc: 0.8462\n",
      "Epoch 130/200\n",
      "156/156 [==============================] - 0s 545us/step - loss: 0.3305 - acc: 0.8333\n",
      "Epoch 131/200\n",
      "156/156 [==============================] - 0s 424us/step - loss: 0.3260 - acc: 0.8269\n",
      "Epoch 132/200\n",
      "156/156 [==============================] - 0s 370us/step - loss: 0.3244 - acc: 0.8654\n",
      "Epoch 133/200\n",
      "156/156 [==============================] - 0s 367us/step - loss: 0.3228 - acc: 0.8462\n",
      "Epoch 134/200\n",
      "156/156 [==============================] - 0s 371us/step - loss: 0.3261 - acc: 0.8526\n",
      "Epoch 135/200\n",
      "156/156 [==============================] - 0s 367us/step - loss: 0.3187 - acc: 0.8397\n",
      "Epoch 136/200\n",
      "156/156 [==============================] - 0s 365us/step - loss: 0.3271 - acc: 0.8526\n",
      "Epoch 137/200\n",
      "156/156 [==============================] - 0s 367us/step - loss: 0.3193 - acc: 0.8590\n",
      "Epoch 138/200\n",
      "156/156 [==============================] - 0s 366us/step - loss: 0.3230 - acc: 0.8590\n",
      "Epoch 139/200\n",
      "156/156 [==============================] - 0s 418us/step - loss: 0.3137 - acc: 0.8462\n",
      "Epoch 140/200\n",
      "156/156 [==============================] - 0s 422us/step - loss: 0.3090 - acc: 0.8974\n",
      "Epoch 141/200\n",
      "156/156 [==============================] - 0s 435us/step - loss: 0.3098 - acc: 0.8654\n",
      "Epoch 142/200\n",
      "156/156 [==============================] - 0s 407us/step - loss: 0.3159 - acc: 0.8462\n",
      "Epoch 143/200\n",
      "156/156 [==============================] - 0s 451us/step - loss: 0.3131 - acc: 0.8462\n",
      "Epoch 144/200\n",
      "156/156 [==============================] - 0s 425us/step - loss: 0.3199 - acc: 0.8590\n",
      "Epoch 145/200\n",
      "156/156 [==============================] - 0s 414us/step - loss: 0.3180 - acc: 0.8462\n",
      "Epoch 146/200\n",
      "156/156 [==============================] - 0s 518us/step - loss: 0.3141 - acc: 0.8462\n",
      "Epoch 147/200\n",
      "156/156 [==============================] - 0s 484us/step - loss: 0.3095 - acc: 0.8590\n",
      "Epoch 148/200\n",
      "156/156 [==============================] - 0s 421us/step - loss: 0.3132 - acc: 0.8590\n",
      "Epoch 149/200\n",
      "156/156 [==============================] - 0s 381us/step - loss: 0.3107 - acc: 0.8333\n",
      "Epoch 150/200\n",
      "156/156 [==============================] - 0s 422us/step - loss: 0.3110 - acc: 0.8718\n",
      "Epoch 151/200\n",
      "156/156 [==============================] - 0s 455us/step - loss: 0.3104 - acc: 0.8526\n",
      "Epoch 152/200\n",
      "156/156 [==============================] - 0s 531us/step - loss: 0.3088 - acc: 0.8397\n",
      "Epoch 153/200\n",
      "156/156 [==============================] - 0s 378us/step - loss: 0.3113 - acc: 0.8526\n",
      "Epoch 154/200\n",
      "156/156 [==============================] - 0s 417us/step - loss: 0.3011 - acc: 0.8654\n",
      "Epoch 155/200\n",
      "156/156 [==============================] - 0s 520us/step - loss: 0.3028 - acc: 0.8590\n",
      "Epoch 156/200\n",
      "156/156 [==============================] - 0s 376us/step - loss: 0.3022 - acc: 0.8590\n",
      "Epoch 157/200\n",
      "156/156 [==============================] - 0s 418us/step - loss: 0.3024 - acc: 0.8590\n",
      "Epoch 158/200\n",
      "156/156 [==============================] - 0s 380us/step - loss: 0.3017 - acc: 0.8718\n",
      "Epoch 159/200\n",
      "156/156 [==============================] - 0s 411us/step - loss: 0.3056 - acc: 0.8718\n",
      "Epoch 160/200\n",
      "156/156 [==============================] - 0s 439us/step - loss: 0.3054 - acc: 0.8526\n",
      "Epoch 161/200\n",
      "156/156 [==============================] - 0s 401us/step - loss: 0.2965 - acc: 0.8782\n",
      "Epoch 162/200\n",
      "156/156 [==============================] - 0s 441us/step - loss: 0.2959 - acc: 0.8974\n",
      "Epoch 163/200\n",
      "156/156 [==============================] - 0s 406us/step - loss: 0.2997 - acc: 0.8846\n",
      "Epoch 164/200\n",
      "156/156 [==============================] - 0s 455us/step - loss: 0.2892 - acc: 0.8782\n",
      "Epoch 165/200\n",
      "156/156 [==============================] - 0s 484us/step - loss: 0.3010 - acc: 0.8590\n",
      "Epoch 166/200\n",
      "156/156 [==============================] - 0s 382us/step - loss: 0.2970 - acc: 0.8782\n",
      "Epoch 167/200\n",
      "156/156 [==============================] - 0s 400us/step - loss: 0.3027 - acc: 0.8782\n",
      "Epoch 168/200\n",
      "156/156 [==============================] - 0s 419us/step - loss: 0.2905 - acc: 0.8654\n",
      "Epoch 169/200\n",
      "156/156 [==============================] - 0s 385us/step - loss: 0.2971 - acc: 0.8718\n",
      "Epoch 170/200\n",
      "156/156 [==============================] - 0s 424us/step - loss: 0.2862 - acc: 0.8718\n",
      "Epoch 171/200\n",
      "156/156 [==============================] - 0s 498us/step - loss: 0.2927 - acc: 0.8846\n",
      "Epoch 172/200\n",
      "156/156 [==============================] - 0s 438us/step - loss: 0.2917 - acc: 0.8590\n",
      "Epoch 173/200\n",
      "156/156 [==============================] - 0s 413us/step - loss: 0.2916 - acc: 0.8974\n",
      "Epoch 174/200\n",
      "156/156 [==============================] - 0s 410us/step - loss: 0.2880 - acc: 0.8718\n",
      "Epoch 175/200\n",
      "156/156 [==============================] - 0s 385us/step - loss: 0.2883 - acc: 0.8718\n",
      "Epoch 176/200\n",
      "156/156 [==============================] - 0s 465us/step - loss: 0.2860 - acc: 0.8782\n",
      "Epoch 177/200\n",
      "156/156 [==============================] - 0s 439us/step - loss: 0.2857 - acc: 0.8654\n",
      "Epoch 178/200\n",
      "156/156 [==============================] - 0s 438us/step - loss: 0.2882 - acc: 0.8718\n",
      "Epoch 179/200\n",
      "156/156 [==============================] - 0s 388us/step - loss: 0.2736 - acc: 0.8910\n",
      "Epoch 180/200\n",
      "156/156 [==============================] - 0s 445us/step - loss: 0.2877 - acc: 0.8590\n",
      "Epoch 181/200\n",
      "156/156 [==============================] - 0s 511us/step - loss: 0.2873 - acc: 0.8718\n",
      "Epoch 182/200\n",
      "156/156 [==============================] - 0s 432us/step - loss: 0.2753 - acc: 0.8846\n",
      "Epoch 183/200\n",
      "156/156 [==============================] - 0s 420us/step - loss: 0.2826 - acc: 0.8654\n",
      "Epoch 184/200\n",
      "156/156 [==============================] - 0s 423us/step - loss: 0.2757 - acc: 0.8782\n",
      "Epoch 185/200\n",
      "156/156 [==============================] - 0s 391us/step - loss: 0.2853 - acc: 0.8654\n",
      "Epoch 186/200\n",
      "156/156 [==============================] - 0s 453us/step - loss: 0.2843 - acc: 0.8526\n",
      "Epoch 187/200\n",
      "156/156 [==============================] - 0s 465us/step - loss: 0.2673 - acc: 0.8782\n",
      "Epoch 188/200\n",
      "156/156 [==============================] - 0s 386us/step - loss: 0.2784 - acc: 0.8718\n",
      "Epoch 189/200\n",
      "156/156 [==============================] - 0s 444us/step - loss: 0.2741 - acc: 0.8718\n",
      "Epoch 190/200\n",
      "156/156 [==============================] - 0s 454us/step - loss: 0.2749 - acc: 0.8718\n",
      "Epoch 191/200\n",
      "156/156 [==============================] - 0s 367us/step - loss: 0.2755 - acc: 0.8654\n",
      "Epoch 192/200\n",
      "156/156 [==============================] - 0s 419us/step - loss: 0.2726 - acc: 0.8782\n",
      "Epoch 193/200\n",
      "156/156 [==============================] - 0s 435us/step - loss: 0.2689 - acc: 0.8718\n",
      "Epoch 194/200\n",
      "156/156 [==============================] - 0s 432us/step - loss: 0.2717 - acc: 0.8910\n",
      "Epoch 195/200\n",
      "156/156 [==============================] - 0s 406us/step - loss: 0.2647 - acc: 0.8718\n",
      "Epoch 196/200\n",
      "156/156 [==============================] - 0s 457us/step - loss: 0.2726 - acc: 0.8782\n",
      "Epoch 197/200\n",
      "156/156 [==============================] - 0s 445us/step - loss: 0.2755 - acc: 0.8590\n",
      "Epoch 198/200\n",
      "156/156 [==============================] - 0s 374us/step - loss: 0.2671 - acc: 0.8782\n",
      "Epoch 199/200\n",
      "156/156 [==============================] - 0s 425us/step - loss: 0.2718 - acc: 0.8654\n",
      "Epoch 200/200\n",
      "156/156 [==============================] - 0s 554us/step - loss: 0.2641 - acc: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f457fe5f8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(X_train, y_train, epochs=best_epoch, batch_size=best_batch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 3,721\n",
      "Trainable params: 3,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of the final model\n",
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 1s 11ms/step\n",
      "\n",
      "acc: 82.69%\n",
      "\n",
      "loss: 45.38%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Keras model on previously unseen data\n",
    "scores = final_model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (final_model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (final_model.metrics_names[0], scores[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0365, 0.1632, 0.1636, 0.1421, 0.113, 0.1306, 0.2112, 0.2268, 0.2992, 0.3735, 0.3042, 0.0387, 0.2679, 0.5397, 0.6204, 0.7257, 0.835, 0.6888, 0.445, 0.3921, 0.5605, 0.7545, 0.8311, 1.0, 0.8762, 0.7092, 0.7009, 0.5014, 0.3942, 0.4456, 0.4072, 0.0773, 0.1423, 0.0401, 0.3597, 0.6847, 0.7076, 0.3597, 0.0612, 0.3027, 0.3966, 0.3868, 0.238, 0.2059, 0.2288, 0.1704, 0.1587, 0.1792, 0.1022, 0.0151, 0.0223, 0.011, 0.0071, 0.0205, 0.0164, 0.0063, 0.0078, 0.0094, 0.011, 0.0068] => 1 (expected 1)\n",
      "[0.0099, 0.0484, 0.0299, 0.0297, 0.0652, 0.1077, 0.2363, 0.2385, 0.0075, 0.1882, 0.1456, 0.1892, 0.3176, 0.134, 0.2169, 0.2458, 0.2589, 0.2786, 0.2298, 0.0656, 0.1441, 0.1179, 0.1668, 0.1783, 0.2476, 0.257, 0.1036, 0.5356, 0.7124, 0.6291, 0.4756, 0.6015, 0.7208, 0.6234, 0.5725, 0.7523, 0.8712, 0.9252, 0.9709, 0.9297, 0.8995, 0.7911, 0.56, 0.2838, 0.4407, 0.5507, 0.4331, 0.2905, 0.1981, 0.0779, 0.0396, 0.0173, 0.0149, 0.0115, 0.0202, 0.0139, 0.0029, 0.016, 0.0106, 0.0134] => 1 (expected 1)\n",
      "[0.0308, 0.0339, 0.0202, 0.0889, 0.157, 0.175, 0.092, 0.1353, 0.1593, 0.2795, 0.3336, 0.294, 0.1608, 0.3335, 0.4985, 0.7295, 0.735, 0.8253, 0.8793, 0.9657, 1.0, 0.8707, 0.6471, 0.5973, 0.8218, 0.7755, 0.6111, 0.4195, 0.299, 0.1354, 0.2438, 0.5624, 0.5555, 0.6963, 0.7298, 0.7022, 0.5468, 0.1421, 0.4738, 0.641, 0.4375, 0.3178, 0.2377, 0.2808, 0.1374, 0.1136, 0.1034, 0.0688, 0.0422, 0.0117, 0.007, 0.0167, 0.0127, 0.0138, 0.009, 0.0051, 0.0029, 0.0122, 0.0056, 0.002] => 0 (expected 1)\n",
      "[0.013, 0.0006, 0.0088, 0.0456, 0.0525, 0.0778, 0.0931, 0.0941, 0.1711, 0.1483, 0.1532, 0.11, 0.089, 0.1236, 0.1197, 0.1145, 0.2137, 0.2838, 0.364, 0.543, 0.6673, 0.7979, 0.9273, 0.9027, 0.9192, 1.0, 0.9821, 0.9092, 0.8184, 0.6962, 0.59, 0.5447, 0.5142, 0.5389, 0.5531, 0.5318, 0.4826, 0.379, 0.1831, 0.175, 0.1679, 0.0674, 0.0609, 0.0375, 0.0533, 0.0278, 0.0179, 0.0114, 0.0073, 0.0116, 0.0092, 0.0078, 0.0041, 0.0013, 0.0011, 0.0045, 0.0039, 0.0022, 0.0023, 0.0016] => 1 (expected 1)\n",
      "[0.0664, 0.0575, 0.0842, 0.0372, 0.0458, 0.0771, 0.0771, 0.113, 0.2353, 0.1838, 0.2869, 0.4129, 0.3647, 0.1984, 0.284, 0.4039, 0.5837, 0.6792, 0.6086, 0.4858, 0.3246, 0.2013, 0.2082, 0.1686, 0.2484, 0.2736, 0.2984, 0.4655, 0.699, 0.7474, 0.7956, 0.7981, 0.6715, 0.6942, 0.744, 0.8169, 0.8912, 1.0, 0.8753, 0.7061, 0.6803, 0.5898, 0.4618, 0.3639, 0.1492, 0.1216, 0.1306, 0.1198, 0.0578, 0.0235, 0.0135, 0.0141, 0.019, 0.0043, 0.0036, 0.0026, 0.0024, 0.0162, 0.0109, 0.0079] => 1 (expected 1)\n",
      "[0.031, 0.0221, 0.0433, 0.0191, 0.0964, 0.1827, 0.1106, 0.1702, 0.2804, 0.4432, 0.5222, 0.5611, 0.5379, 0.4048, 0.2245, 0.1784, 0.2297, 0.272, 0.5209, 0.6898, 0.8202, 0.878, 0.76, 0.7616, 0.7152, 0.7288, 0.8686, 0.9509, 0.8348, 0.573, 0.4363, 0.4289, 0.424, 0.3156, 0.1287, 0.1477, 0.2062, 0.24, 0.5173, 0.5168, 0.1491, 0.2407, 0.3415, 0.4494, 0.4624, 0.2001, 0.0775, 0.1232, 0.0783, 0.0089, 0.0249, 0.0204, 0.0059, 0.0053, 0.0079, 0.0037, 0.0015, 0.0056, 0.0067, 0.0054] => 1 (expected 0)\n",
      "[0.0329, 0.0216, 0.0386, 0.0627, 0.1158, 0.1482, 0.2054, 0.1605, 0.2532, 0.2672, 0.3056, 0.3161, 0.2314, 0.2067, 0.1804, 0.2808, 0.4423, 0.5947, 0.6601, 0.5844, 0.4539, 0.4789, 0.5646, 0.5281, 0.7115, 1.0, 0.9564, 0.609, 0.5112, 0.4, 0.0482, 0.1852, 0.2186, 0.1436, 0.1757, 0.1428, 0.1644, 0.3089, 0.3648, 0.4441, 0.3859, 0.2813, 0.1238, 0.0953, 0.1201, 0.0825, 0.0618, 0.0141, 0.0108, 0.0124, 0.0104, 0.0095, 0.0151, 0.0059, 0.0015, 0.0053, 0.0016, 0.0042, 0.0053, 0.0074] => 1 (expected 0)\n",
      "[0.0211, 0.0319, 0.0415, 0.0286, 0.0121, 0.0438, 0.1299, 0.139, 0.0695, 0.0568, 0.0869, 0.1935, 0.1478, 0.1871, 0.1994, 0.3283, 0.6861, 0.5814, 0.25, 0.1734, 0.3363, 0.5588, 0.6592, 0.7012, 0.8099, 0.8901, 0.8745, 0.7887, 0.8725, 0.9376, 0.892, 0.7508, 0.6832, 0.761, 0.9017, 1.0, 0.9123, 0.7388, 0.5915, 0.4057, 0.3019, 0.2331, 0.2931, 0.2298, 0.2391, 0.191, 0.1096, 0.03, 0.0171, 0.0383, 0.0053, 0.009, 0.0042, 0.0153, 0.0106, 0.002, 0.0105, 0.0049, 0.007, 0.008] => 0 (expected 1)\n",
      "[0.026, 0.0192, 0.0254, 0.0061, 0.0352, 0.0701, 0.1263, 0.108, 0.1523, 0.163, 0.103, 0.2187, 0.1542, 0.263, 0.294, 0.2978, 0.0699, 0.1401, 0.299, 0.3915, 0.3598, 0.2403, 0.4208, 0.5675, 0.6094, 0.6323, 0.6549, 0.7673, 1.0, 0.8463, 0.5509, 0.4444, 0.5169, 0.4268, 0.1802, 0.0791, 0.0535, 0.1906, 0.2561, 0.2153, 0.2769, 0.2841, 0.1733, 0.0815, 0.0335, 0.0933, 0.1018, 0.0309, 0.0208, 0.0318, 0.0132, 0.0118, 0.012, 0.0051, 0.007, 0.0015, 0.0035, 0.0008, 0.0044, 0.0077] => 0 (expected 1)\n",
      "[0.1313, 0.2339, 0.3059, 0.4264, 0.401, 0.1791, 0.1853, 0.0055, 0.1929, 0.2231, 0.2907, 0.2259, 0.3136, 0.3302, 0.366, 0.3956, 0.4386, 0.467, 0.5255, 0.3735, 0.2243, 0.1973, 0.4337, 0.6532, 0.507, 0.2796, 0.4163, 0.595, 0.5242, 0.4178, 0.3714, 0.2375, 0.0863, 0.1437, 0.2896, 0.4577, 0.3725, 0.3372, 0.3803, 0.4181, 0.3603, 0.2711, 0.1653, 0.1951, 0.2811, 0.2246, 0.1921, 0.15, 0.0665, 0.0193, 0.0156, 0.0362, 0.021, 0.0154, 0.018, 0.0013, 0.0106, 0.0127, 0.0178, 0.0231] => 1 (expected 0)\n",
      "[0.0261, 0.0266, 0.0223, 0.0749, 0.1364, 0.1513, 0.1316, 0.1654, 0.1864, 0.2013, 0.289, 0.365, 0.351, 0.3495, 0.4325, 0.5398, 0.6237, 0.6876, 0.7329, 0.8107, 0.8396, 0.8632, 0.8747, 0.9607, 0.9716, 0.9121, 0.8576, 0.8798, 0.772, 0.5711, 0.4264, 0.286, 0.3114, 0.2066, 0.1165, 0.0185, 0.1302, 0.248, 0.1637, 0.1103, 0.2144, 0.2033, 0.1887, 0.137, 0.1376, 0.0307, 0.0373, 0.0606, 0.0399, 0.0169, 0.0135, 0.0222, 0.0175, 0.0127, 0.0022, 0.0124, 0.0054, 0.0021, 0.0028, 0.0023] => 1 (expected 0)\n",
      "[0.0262, 0.0582, 0.1099, 0.1083, 0.0974, 0.228, 0.2431, 0.3771, 0.5598, 0.6194, 0.6333, 0.706, 0.5544, 0.532, 0.6479, 0.6931, 0.6759, 0.7551, 0.8929, 0.8619, 0.7974, 0.6737, 0.4293, 0.3648, 0.5331, 0.2413, 0.507, 0.8533, 0.6036, 0.8514, 0.8512, 0.5045, 0.1862, 0.2709, 0.4232, 0.3043, 0.6116, 0.6756, 0.5375, 0.4719, 0.4647, 0.2587, 0.2129, 0.2222, 0.2111, 0.0176, 0.1348, 0.0744, 0.013, 0.0106, 0.0033, 0.0232, 0.0166, 0.0095, 0.018, 0.0244, 0.0316, 0.0164, 0.0095, 0.0078] => 1 (expected 1)\n",
      "[0.0197, 0.0394, 0.0384, 0.0076, 0.0251, 0.0629, 0.0747, 0.0578, 0.1357, 0.1695, 0.1734, 0.247, 0.3141, 0.3297, 0.2759, 0.2056, 0.1162, 0.1884, 0.339, 0.3926, 0.4282, 0.5418, 0.6448, 0.7223, 0.7853, 0.7984, 0.8847, 0.9582, 0.899, 0.6831, 0.6108, 0.548, 0.5058, 0.4476, 0.2401, 0.1405, 0.1772, 0.1742, 0.3326, 0.4021, 0.3009, 0.2075, 0.1206, 0.0255, 0.0298, 0.0691, 0.0781, 0.0777, 0.0369, 0.0057, 0.0091, 0.0134, 0.0097, 0.0042, 0.0058, 0.0072, 0.0041, 0.0045, 0.0047, 0.0054] => 1 (expected 0)\n",
      "[0.0368, 0.0279, 0.0103, 0.0566, 0.0759, 0.0679, 0.097, 0.1473, 0.2164, 0.2544, 0.2936, 0.2935, 0.2657, 0.3187, 0.2794, 0.2534, 0.198, 0.1929, 0.2826, 0.3245, 0.3504, 0.3324, 0.4217, 0.4774, 0.4808, 0.6325, 0.8334, 0.9458, 1.0, 0.8425, 0.5524, 0.4795, 0.52, 0.3968, 0.194, 0.1519, 0.201, 0.1736, 0.1029, 0.2244, 0.3717, 0.4449, 0.3939, 0.203, 0.201, 0.2187, 0.184, 0.1477, 0.0971, 0.0224, 0.0151, 0.0105, 0.0024, 0.0018, 0.0057, 0.0092, 0.0009, 0.0086, 0.011, 0.0052] => 1 (expected 0)\n",
      "[0.0231, 0.0315, 0.017, 0.0226, 0.041, 0.0116, 0.0223, 0.0805, 0.2365, 0.2461, 0.2245, 0.152, 0.1732, 0.3099, 0.438, 0.5595, 0.682, 0.6164, 0.6803, 0.8435, 0.9921, 1.0, 0.7983, 0.5426, 0.3952, 0.5179, 0.565, 0.3042, 0.1881, 0.396, 0.2286, 0.3544, 0.4187, 0.2398, 0.1847, 0.376, 0.4331, 0.3626, 0.2519, 0.187, 0.1046, 0.2339, 0.1991, 0.11, 0.0684, 0.0303, 0.0674, 0.0785, 0.0455, 0.0246, 0.0151, 0.0125, 0.0036, 0.0123, 0.0043, 0.0114, 0.0052, 0.0091, 0.0008, 0.0092] => 1 (expected 0)\n",
      "[0.0335, 0.0258, 0.0398, 0.057, 0.0529, 0.1091, 0.1709, 0.1684, 0.1865, 0.266, 0.3188, 0.3553, 0.3116, 0.1965, 0.178, 0.2794, 0.287, 0.3969, 0.5599, 0.6936, 0.7969, 0.7452, 0.8203, 0.9261, 0.881, 0.8814, 0.9301, 0.9955, 0.8576, 0.6069, 0.3934, 0.2464, 0.1645, 0.114, 0.0956, 0.008, 0.0702, 0.0936, 0.0894, 0.1127, 0.0873, 0.102, 0.1964, 0.2256, 0.1814, 0.2012, 0.1688, 0.1037, 0.0501, 0.0136, 0.013, 0.012, 0.0039, 0.0053, 0.0062, 0.0046, 0.0045, 0.0022, 0.0005, 0.0031] => 1 (expected 0)\n",
      "[0.0235, 0.022, 0.0167, 0.0516, 0.0746, 0.1121, 0.1258, 0.1717, 0.3074, 0.3199, 0.2946, 0.2484, 0.251, 0.1806, 0.1413, 0.3019, 0.3635, 0.3887, 0.298, 0.2219, 0.1624, 0.1343, 0.2046, 0.3791, 0.5771, 0.7545, 0.8406, 0.8547, 0.9036, 1.0, 0.9646, 0.7912, 0.6412, 0.5986, 0.6835, 0.7771, 0.8084, 0.7426, 0.6295, 0.5708, 0.4433, 0.3361, 0.3795, 0.495, 0.4373, 0.2404, 0.1128, 0.1654, 0.0933, 0.0225, 0.0214, 0.0221, 0.0152, 0.0083, 0.0058, 0.0023, 0.0057, 0.0052, 0.0027, 0.0021] => 1 (expected 0)\n",
      "[0.027, 0.0163, 0.0341, 0.0247, 0.0822, 0.1256, 0.1323, 0.1584, 0.2017, 0.2122, 0.221, 0.2399, 0.2964, 0.4061, 0.5095, 0.5512, 0.6613, 0.6804, 0.652, 0.6788, 0.7811, 0.8369, 0.8969, 0.9856, 1.0, 0.9395, 0.8917, 0.8105, 0.6828, 0.5572, 0.4301, 0.3339, 0.2035, 0.0798, 0.0809, 0.1525, 0.2626, 0.2456, 0.198, 0.2412, 0.2409, 0.1901, 0.2077, 0.1767, 0.1119, 0.0779, 0.1344, 0.096, 0.0598, 0.033, 0.0197, 0.0189, 0.0204, 0.0085, 0.0043, 0.0092, 0.0138, 0.0094, 0.0105, 0.0093] => 1 (expected 0)\n",
      "[0.0408, 0.0653, 0.0397, 0.0604, 0.0496, 0.1817, 0.1178, 0.1024, 0.0583, 0.2176, 0.2459, 0.3332, 0.3087, 0.2613, 0.3232, 0.3731, 0.4203, 0.5364, 0.7062, 0.8196, 0.8835, 0.8299, 0.7609, 0.7605, 0.8367, 0.8905, 0.7652, 0.5897, 0.3037, 0.0823, 0.2787, 0.7241, 0.8032, 0.805, 0.7676, 0.7468, 0.6253, 0.173, 0.2916, 0.5003, 0.522, 0.4824, 0.4004, 0.3877, 0.1651, 0.0442, 0.0663, 0.0418, 0.0475, 0.0235, 0.0066, 0.0062, 0.0129, 0.0184, 0.0069, 0.0198, 0.0199, 0.0102, 0.007, 0.0055] => 1 (expected 1)\n",
      "[0.0221, 0.0065, 0.0164, 0.0487, 0.0519, 0.0849, 0.0812, 0.1833, 0.2228, 0.181, 0.2549, 0.2984, 0.2624, 0.1893, 0.0668, 0.2666, 0.4274, 0.6291, 0.7782, 0.7686, 0.8099, 0.8493, 0.944, 0.945, 0.9655, 0.8045, 0.4969, 0.396, 0.3856, 0.5574, 0.7309, 0.8549, 0.9425, 0.8726, 0.6673, 0.4694, 0.1546, 0.1748, 0.3607, 0.5208, 0.5177, 0.3702, 0.224, 0.0816, 0.0395, 0.0785, 0.1052, 0.1034, 0.0764, 0.0216, 0.0167, 0.0089, 0.0051, 0.0015, 0.0075, 0.0058, 0.0016, 0.007, 0.0074, 0.0038] => 0 (expected 0)\n"
     ]
    }
   ],
   "source": [
    "# Make class predictions with the model\n",
    "predictions = final_model.predict_classes(X_original)\n",
    "\n",
    "# Summarize the first 20 cases\n",
    "for i in range(20):\n",
    "\tprint('%s => %d (expected %d)' % (X_test[i].tolist(), predictions[i], y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 5 Finalize Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for the script: 0:21:38.562817\n"
     ]
    }
   ],
   "source": [
    "print ('Total time for the script:',(datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
