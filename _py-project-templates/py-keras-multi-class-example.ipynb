{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification Deep Learning Model for [PROJECT NAME] Using Keras Version 3\n",
    "### David Lowe\n",
    "### November 12, 2019\n",
    "\n",
    "Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [https://machinelearningmastery.com/]\n",
    "\n",
    "SUMMARY: The purpose of this project is to construct a predictive model using various machine learning algorithms and to document the end-to-end steps using a template. The [PROJECT NAME] dataset is a multi-class classification situation where we are trying to predict one of several (more than two) possible outcomes.\n",
    "\n",
    "INTRODUCTION: [Sample Paragraph - This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains three classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other two; the latter are NOT linearly separable from each other.]\n",
    "\n",
    "ANALYSIS: [Sample Paragraph - The baseline performance of the model achieved an average accuracy score of 97.35%. After tuning the hyperparameters, the best model processed the training dataset with an accuracy of 99.10%. Furthermore, the final model processed the test dataset with an accuracy of 94.74%, which indicated a high-variance, over-fitting issue. We may need to acquire more data and/or apply regularization techniques during training before deploying the model for production use.]\n",
    "\n",
    "CONCLUSION: For this dataset, the model built using Keras and TensorFlow achieved a satisfactory result and should be considered for future modeling activities.\n",
    "\n",
    "Dataset Used: [PROJECT NAME] Dataset\n",
    "\n",
    "Dataset ML Model: Multi-class classification with numerical attributes\n",
    "\n",
    "Dataset Reference: [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/]\n",
    "\n",
    "One potential source of performance benchmarks: [https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/]\n",
    "\n",
    "Any deep-learning modeling project genrally can be broken down into about six major tasks:\n",
    "0. Prepare Environment\n",
    "1. Load Data\n",
    "2. Define Model\n",
    "3. Fit and Evaluate Model\n",
    "4. Optimize Model\n",
    "5. Finalize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0. Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the warning message filter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed number for reproducible results\n",
    "seedNum = 888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries and packages\n",
    "import random\n",
    "random.seed(seedNum)\n",
    "import numpy as np\n",
    "np.random.seed(seedNum)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seedNum)\n",
    "import keras as K\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import smtplib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from email.message import EmailMessage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Begin the timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# Set up the verbose flag to print detailed messages for debugging (setting to True will activate)\n",
    "verbose = True\n",
    "tf.debugging.set_log_device_placement(verbose)\n",
    "\n",
    "# Set up the number of CPU cores available for multi-thread processing\n",
    "n_jobs = -1\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Set up the flag to stop sending progress emails (setting to True will send status emails!)\n",
    "notifyStatus = False\n",
    "\n",
    "# Set the number of folds for cross validation\n",
    "n_folds = 5\n",
    "\n",
    "# Set the flag for splitting the dataset\n",
    "splitDataset = True\n",
    "splitPercentage = 0.25\n",
    "\n",
    "# Set various default Keras modeling parameters\n",
    "default_kernel_init = K.initializers.RandomNormal(seed=seedNum)\n",
    "default_loss = 'categorical_crossentropy'\n",
    "default_optimizer = 'adam'\n",
    "default_epochs = 250\n",
    "default_batches = 8\n",
    "default_metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the email notification function\n",
    "def email_notify(msg_text):\n",
    "    sender = os.environ.get('MAIL_SENDER')\n",
    "    receiver = os.environ.get('MAIL_RECEIVER')\n",
    "    gateway = os.environ.get('SMTP_GATEWAY')\n",
    "    smtpuser = os.environ.get('SMTP_USERNAME')\n",
    "    password = os.environ.get('SMTP_PASSWORD')\n",
    "    if sender==None or receiver==None or gateway==None or smtpuser==None or password==None:\n",
    "        sys.exit(\"Incomplete email setup info. Script Processing Aborted!!!\")\n",
    "    msg = EmailMessage()\n",
    "    msg.set_content(msg_text)\n",
    "    msg['Subject'] = 'Notification from Keras Multi-Class Classification Script'\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = receiver\n",
    "    server = smtplib.SMTP(gateway, 587)\n",
    "    server.starttls()\n",
    "    server.login(smtpuser, password)\n",
    "    server.send_message(msg)\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 0 Prepare Environment completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 1 Load Data has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a) Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>targetVar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width    targetVar\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "5           5.4          3.9           1.7          0.4  Iris-setosa\n",
       "6           4.6          3.4           1.4          0.3  Iris-setosa\n",
       "7           5.0          3.4           1.5          0.2  Iris-setosa\n",
       "8           4.4          2.9           1.4          0.2  Iris-setosa\n",
       "9           4.9          3.1           1.5          0.1  Iris-setosa"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "# dest_file = os.path.basename(dataset_path)\n",
    "# if (os.path.isfile(dest_file) == False) :\n",
    "#     print('Downloading ' + dataset_path + ' as ' + dest_file)\n",
    "#     with urllib.request.urlopen(dataset_path) as in_resp, open(dest_file, 'wb') as out_file:\n",
    "#         shutil.copyfileobj(in_resp, out_file)\n",
    "#     print(dest_file + 'downloaded!')\n",
    "#     print('Unpacking ' + dest_file)\n",
    "#     with zipfile.ZipFile(dest_file, 'r') as zip_ref:\n",
    "#         zip_ref.extractall('.')\n",
    "#     print(dest_file + 'unpacked!')\n",
    "\n",
    "# inputFile = dest_file\n",
    "# attrNames = ['attr' + str(i).zfill(2) for i in range(1,10)]\n",
    "# colNames = ['id'] + attrNames + ['target']\n",
    "colNames = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'targetVar']\n",
    "Xy_original = pd.read_csv(dataset_path, names=colNames, sep=',', header=None, index_col=False, na_values=['?'])\n",
    "\n",
    "# Take a peek at the dataframe after the import\n",
    "Xy_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      "sepal_length    150 non-null float64\n",
      "sepal_width     150 non-null float64\n",
      "petal_length    150 non-null float64\n",
      "petal_width     150 non-null float64\n",
      "targetVar       150 non-null object\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "Xy_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667\n",
       "std        0.828066     0.433594      1.764420     0.763161\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length    0\n",
      "sepal_width     0\n",
      "petal_length    0\n",
      "petal_width     0\n",
      "targetVar       0\n",
      "dtype: int64\n",
      "Total number of NaN in the dataframe:  0\n"
     ]
    }
   ],
   "source": [
    "print(Xy_original.isnull().sum())\n",
    "print('Total number of NaN in the dataframe: ', Xy_original.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b) Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>targetVar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width    targetVar\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "5           5.4          3.9           1.7          0.4  Iris-setosa\n",
       "6           4.6          3.4           1.4          0.3  Iris-setosa\n",
       "7           5.0          3.4           1.5          0.2  Iris-setosa\n",
       "8           4.4          2.9           1.4          0.2  Iris-setosa\n",
       "9           4.9          3.1           1.5          0.1  Iris-setosa"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the class column to the name of targetVar if required\n",
    "# Xy_original = Xy_original.rename(columns={'old_name': 'targetVar'})\n",
    "\n",
    "# Dropping features\n",
    "# Xy_original.drop(columns=['attribute_name'], inplace=True)\n",
    "\n",
    "# Impute missing values\n",
    "# Xy_original['col_name'].fillna('someValue', inplace=True)\n",
    "# Xy_original['attribute_name'].fillna(value=Xy_original['attribute_name'].median(), inplace=True)\n",
    "\n",
    "# Convert columns from one data type to another\n",
    "# Xy_original.column_name = Xy_original.column_name.astype('int')\n",
    "# Xy_original.column_name = Xy_original.column_name.astype('category')\n",
    "\n",
    "# Convert features with Y/N levels into categorical feature of 1/0\n",
    "# def reClassSomecol(target):\n",
    "#     if (target == 'Y'): return 1\n",
    "#     else: return 0\n",
    "# Xy_original['targetVar'] = Xy_original['target'].apply(reClassSomecol)\n",
    "# Xy_original.drop(columns=['target'], inplace=True)\n",
    "\n",
    "# Take a peek at the dataframe after the cleaning\n",
    "Xy_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      "sepal_length    150 non-null float64\n",
      "sepal_width     150 non-null float64\n",
      "petal_length    150 non-null float64\n",
      "petal_width     150 non-null float64\n",
      "targetVar       150 non-null object\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "Xy_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667\n",
       "std        0.828066     0.433594      1.764420     0.763161\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length    0\n",
      "sepal_width     0\n",
      "petal_length    0\n",
      "petal_width     0\n",
      "targetVar       0\n",
      "dtype: int64\n",
      "Total number of NaN in the dataframe:  0\n"
     ]
    }
   ],
   "source": [
    "print(Xy_original.isnull().sum())\n",
    "print('Total number of NaN in the dataframe: ', Xy_original.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c) Feature Scaling and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use variable totCol to hold the number of columns in the dataframe\n",
    "totCol = len(Xy_original.columns)\n",
    "\n",
    "# Set up variable totAttr for the total number of attribute columns\n",
    "totAttr = totCol-1\n",
    "\n",
    "# targetCol variable indicates the column location of the target/class variable\n",
    "# If the first column, set targetCol to 1. If the last column, set targetCol to totCol\n",
    "# If (targetCol <> 1) and (targetCol <> totCol), be aware when slicing up the dataframes for visualization\n",
    "targetCol = totCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xy_original.shape: (150, 5) X_original.shape: (150, 4) y_original.shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "# We create attribute-only and target-only datasets (X_original and y_original)\n",
    "# for various visualization and cleaning/transformation operations\n",
    "\n",
    "if targetCol == totCol:\n",
    "    X_original = Xy_original.iloc[:,0:totAttr]\n",
    "    y_original = Xy_original.iloc[:,totAttr]\n",
    "else:\n",
    "    X_original = Xy_original.iloc[:,1:totCol]\n",
    "    y_original = Xy_original.iloc[:,0]\n",
    "\n",
    "print(\"Xy_original.shape: {} X_original.shape: {} y_original.shape: {}\".format(Xy_original.shape, X_original.shape, y_original.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the number of row and columns for visualization display. dispRow * dispCol should be >= totAttr\n",
    "dispCol = 4\n",
    "if totAttr % dispCol == 0 :\n",
    "    dispRow = totAttr // dispCol\n",
    "else :\n",
    "    dispRow = (totAttr // dispCol) + 1\n",
    "    \n",
    "# Set figure width to display the data visualization plots\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = dispCol*4\n",
    "fig_size[1] = dispRow*4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms for each attribute before pre-processing\n",
    "X_original.hist(layout=(dispRow,dispCol))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.00681170e-01,  1.03205722e+00, -1.34127240e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.14301691e+00, -1.24957601e-01, -1.34127240e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.38535265e+00,  3.37848329e-01, -1.39813811e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.50652052e+00,  1.06445364e-01, -1.28440670e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.02184904e+00,  1.26346019e+00, -1.34127240e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-5.37177559e-01,  1.95766909e+00, -1.17067529e+00,\n",
       "        -1.05003079e+00],\n",
       "       [-1.50652052e+00,  8.00654259e-01, -1.34127240e+00,\n",
       "        -1.18150376e+00],\n",
       "       [-1.02184904e+00,  8.00654259e-01, -1.28440670e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.74885626e+00, -3.56360566e-01, -1.34127240e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.14301691e+00,  1.06445364e-01, -1.28440670e+00,\n",
       "        -1.44444970e+00],\n",
       "       [-5.37177559e-01,  1.49486315e+00, -1.28440670e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.26418478e+00,  8.00654259e-01, -1.22754100e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.26418478e+00, -1.24957601e-01, -1.34127240e+00,\n",
       "        -1.44444970e+00],\n",
       "       [-1.87002413e+00, -1.24957601e-01, -1.51186952e+00,\n",
       "        -1.44444970e+00],\n",
       "       [-5.25060772e-02,  2.18907205e+00, -1.45500381e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.73673948e-01,  3.11468391e+00, -1.28440670e+00,\n",
       "        -1.05003079e+00],\n",
       "       [-5.37177559e-01,  1.95766909e+00, -1.39813811e+00,\n",
       "        -1.05003079e+00],\n",
       "       [-9.00681170e-01,  1.03205722e+00, -1.34127240e+00,\n",
       "        -1.18150376e+00],\n",
       "       [-1.73673948e-01,  1.72626612e+00, -1.17067529e+00,\n",
       "        -1.18150376e+00],\n",
       "       [-9.00681170e-01,  1.72626612e+00, -1.28440670e+00,\n",
       "        -1.18150376e+00],\n",
       "       [-5.37177559e-01,  8.00654259e-01, -1.17067529e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-9.00681170e-01,  1.49486315e+00, -1.28440670e+00,\n",
       "        -1.05003079e+00],\n",
       "       [-1.50652052e+00,  1.26346019e+00, -1.56873522e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-9.00681170e-01,  5.69251294e-01, -1.17067529e+00,\n",
       "        -9.18557817e-01],\n",
       "       [-1.26418478e+00,  8.00654259e-01, -1.05694388e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.02184904e+00, -1.24957601e-01, -1.22754100e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.02184904e+00,  8.00654259e-01, -1.22754100e+00,\n",
       "        -1.05003079e+00],\n",
       "       [-7.79513300e-01,  1.03205722e+00, -1.28440670e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-7.79513300e-01,  8.00654259e-01, -1.34127240e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.38535265e+00,  3.37848329e-01, -1.22754100e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.26418478e+00,  1.06445364e-01, -1.22754100e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-5.37177559e-01,  8.00654259e-01, -1.28440670e+00,\n",
       "        -1.05003079e+00],\n",
       "       [-7.79513300e-01,  2.42047502e+00, -1.28440670e+00,\n",
       "        -1.44444970e+00],\n",
       "       [-4.16009689e-01,  2.65187798e+00, -1.34127240e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.14301691e+00,  1.06445364e-01, -1.28440670e+00,\n",
       "        -1.44444970e+00],\n",
       "       [-1.02184904e+00,  3.37848329e-01, -1.45500381e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-4.16009689e-01,  1.03205722e+00, -1.39813811e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.14301691e+00,  1.06445364e-01, -1.28440670e+00,\n",
       "        -1.44444970e+00],\n",
       "       [-1.74885626e+00, -1.24957601e-01, -1.39813811e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-9.00681170e-01,  8.00654259e-01, -1.28440670e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.02184904e+00,  1.03205722e+00, -1.39813811e+00,\n",
       "        -1.18150376e+00],\n",
       "       [-1.62768839e+00, -1.74477836e+00, -1.39813811e+00,\n",
       "        -1.18150376e+00],\n",
       "       [-1.74885626e+00,  3.37848329e-01, -1.39813811e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.02184904e+00,  1.03205722e+00, -1.22754100e+00,\n",
       "        -7.87084847e-01],\n",
       "       [-9.00681170e-01,  1.72626612e+00, -1.05694388e+00,\n",
       "        -1.05003079e+00],\n",
       "       [-1.26418478e+00, -1.24957601e-01, -1.34127240e+00,\n",
       "        -1.18150376e+00],\n",
       "       [-9.00681170e-01,  1.72626612e+00, -1.22754100e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.50652052e+00,  3.37848329e-01, -1.34127240e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-6.58345429e-01,  1.49486315e+00, -1.28440670e+00,\n",
       "        -1.31297673e+00],\n",
       "       [-1.02184904e+00,  5.69251294e-01, -1.34127240e+00,\n",
       "        -1.31297673e+00],\n",
       "       [ 1.40150837e+00,  3.37848329e-01,  5.35295827e-01,\n",
       "         2.64698913e-01],\n",
       "       [ 6.74501145e-01,  3.37848329e-01,  4.21564419e-01,\n",
       "         3.96171883e-01],\n",
       "       [ 1.28034050e+00,  1.06445364e-01,  6.49027235e-01,\n",
       "         3.96171883e-01],\n",
       "       [-4.16009689e-01, -1.74477836e+00,  1.37235899e-01,\n",
       "         1.33225943e-01],\n",
       "       [ 7.95669016e-01, -5.87763531e-01,  4.78430123e-01,\n",
       "         3.96171883e-01],\n",
       "       [-1.73673948e-01, -5.87763531e-01,  4.21564419e-01,\n",
       "         1.33225943e-01],\n",
       "       [ 5.53333275e-01,  5.69251294e-01,  5.35295827e-01,\n",
       "         5.27644853e-01],\n",
       "       [-1.14301691e+00, -1.51337539e+00, -2.60824029e-01,\n",
       "        -2.61192967e-01],\n",
       "       [ 9.16836886e-01, -3.56360566e-01,  4.78430123e-01,\n",
       "         1.33225943e-01],\n",
       "       [-7.79513300e-01, -8.19166497e-01,  8.03701950e-02,\n",
       "         2.64698913e-01],\n",
       "       [-1.02184904e+00, -2.43898725e+00, -1.47092621e-01,\n",
       "        -2.61192967e-01],\n",
       "       [ 6.86617933e-02, -1.24957601e-01,  2.50967307e-01,\n",
       "         3.96171883e-01],\n",
       "       [ 1.89829664e-01, -1.97618132e+00,  1.37235899e-01,\n",
       "        -2.61192967e-01],\n",
       "       [ 3.10997534e-01, -3.56360566e-01,  5.35295827e-01,\n",
       "         2.64698913e-01],\n",
       "       [-2.94841818e-01, -3.56360566e-01, -9.02269170e-02,\n",
       "         1.33225943e-01],\n",
       "       [ 1.03800476e+00,  1.06445364e-01,  3.64698715e-01,\n",
       "         2.64698913e-01],\n",
       "       [-2.94841818e-01, -1.24957601e-01,  4.21564419e-01,\n",
       "         3.96171883e-01],\n",
       "       [-5.25060772e-02, -8.19166497e-01,  1.94101603e-01,\n",
       "        -2.61192967e-01],\n",
       "       [ 4.32165405e-01, -1.97618132e+00,  4.21564419e-01,\n",
       "         3.96171883e-01],\n",
       "       [-2.94841818e-01, -1.28197243e+00,  8.03701950e-02,\n",
       "        -1.29719997e-01],\n",
       "       [ 6.86617933e-02,  3.37848329e-01,  5.92161531e-01,\n",
       "         7.90590793e-01],\n",
       "       [ 3.10997534e-01, -5.87763531e-01,  1.37235899e-01,\n",
       "         1.33225943e-01],\n",
       "       [ 5.53333275e-01, -1.28197243e+00,  6.49027235e-01,\n",
       "         3.96171883e-01],\n",
       "       [ 3.10997534e-01, -5.87763531e-01,  5.35295827e-01,\n",
       "         1.75297293e-03],\n",
       "       [ 6.74501145e-01, -3.56360566e-01,  3.07833011e-01,\n",
       "         1.33225943e-01],\n",
       "       [ 9.16836886e-01, -1.24957601e-01,  3.64698715e-01,\n",
       "         2.64698913e-01],\n",
       "       [ 1.15917263e+00, -5.87763531e-01,  5.92161531e-01,\n",
       "         2.64698913e-01],\n",
       "       [ 1.03800476e+00, -1.24957601e-01,  7.05892939e-01,\n",
       "         6.59117823e-01],\n",
       "       [ 1.89829664e-01, -3.56360566e-01,  4.21564419e-01,\n",
       "         3.96171883e-01],\n",
       "       [-1.73673948e-01, -1.05056946e+00, -1.47092621e-01,\n",
       "        -2.61192967e-01],\n",
       "       [-4.16009689e-01, -1.51337539e+00,  2.35044910e-02,\n",
       "        -1.29719997e-01],\n",
       "       [-4.16009689e-01, -1.51337539e+00, -3.33612130e-02,\n",
       "        -2.61192967e-01],\n",
       "       [-5.25060772e-02, -8.19166497e-01,  8.03701950e-02,\n",
       "         1.75297293e-03],\n",
       "       [ 1.89829664e-01, -8.19166497e-01,  7.62758643e-01,\n",
       "         5.27644853e-01],\n",
       "       [-5.37177559e-01, -1.24957601e-01,  4.21564419e-01,\n",
       "         3.96171883e-01],\n",
       "       [ 1.89829664e-01,  8.00654259e-01,  4.21564419e-01,\n",
       "         5.27644853e-01],\n",
       "       [ 1.03800476e+00,  1.06445364e-01,  5.35295827e-01,\n",
       "         3.96171883e-01],\n",
       "       [ 5.53333275e-01, -1.74477836e+00,  3.64698715e-01,\n",
       "         1.33225943e-01],\n",
       "       [-2.94841818e-01, -1.24957601e-01,  1.94101603e-01,\n",
       "         1.33225943e-01],\n",
       "       [-4.16009689e-01, -1.28197243e+00,  1.37235899e-01,\n",
       "         1.33225943e-01],\n",
       "       [-4.16009689e-01, -1.05056946e+00,  3.64698715e-01,\n",
       "         1.75297293e-03],\n",
       "       [ 3.10997534e-01, -1.24957601e-01,  4.78430123e-01,\n",
       "         2.64698913e-01],\n",
       "       [-5.25060772e-02, -1.05056946e+00,  1.37235899e-01,\n",
       "         1.75297293e-03],\n",
       "       [-1.02184904e+00, -1.74477836e+00, -2.60824029e-01,\n",
       "        -2.61192967e-01],\n",
       "       [-2.94841818e-01, -8.19166497e-01,  2.50967307e-01,\n",
       "         1.33225943e-01],\n",
       "       [-1.73673948e-01, -1.24957601e-01,  2.50967307e-01,\n",
       "         1.75297293e-03],\n",
       "       [-1.73673948e-01, -3.56360566e-01,  2.50967307e-01,\n",
       "         1.33225943e-01],\n",
       "       [ 4.32165405e-01, -3.56360566e-01,  3.07833011e-01,\n",
       "         1.33225943e-01],\n",
       "       [-9.00681170e-01, -1.28197243e+00, -4.31421141e-01,\n",
       "        -1.29719997e-01],\n",
       "       [-1.73673948e-01, -5.87763531e-01,  1.94101603e-01,\n",
       "         1.33225943e-01],\n",
       "       [ 5.53333275e-01,  5.69251294e-01,  1.27454998e+00,\n",
       "         1.71090158e+00],\n",
       "       [-5.25060772e-02, -8.19166497e-01,  7.62758643e-01,\n",
       "         9.22063763e-01],\n",
       "       [ 1.52267624e+00, -1.24957601e-01,  1.21768427e+00,\n",
       "         1.18500970e+00],\n",
       "       [ 5.53333275e-01, -3.56360566e-01,  1.04708716e+00,\n",
       "         7.90590793e-01],\n",
       "       [ 7.95669016e-01, -1.24957601e-01,  1.16081857e+00,\n",
       "         1.31648267e+00],\n",
       "       [ 2.12851559e+00, -1.24957601e-01,  1.61574420e+00,\n",
       "         1.18500970e+00],\n",
       "       [-1.14301691e+00, -1.28197243e+00,  4.21564419e-01,\n",
       "         6.59117823e-01],\n",
       "       [ 1.76501198e+00, -3.56360566e-01,  1.44514709e+00,\n",
       "         7.90590793e-01],\n",
       "       [ 1.03800476e+00, -1.28197243e+00,  1.16081857e+00,\n",
       "         7.90590793e-01],\n",
       "       [ 1.64384411e+00,  1.26346019e+00,  1.33141568e+00,\n",
       "         1.71090158e+00],\n",
       "       [ 7.95669016e-01,  3.37848329e-01,  7.62758643e-01,\n",
       "         1.05353673e+00],\n",
       "       [ 6.74501145e-01, -8.19166497e-01,  8.76490051e-01,\n",
       "         9.22063763e-01],\n",
       "       [ 1.15917263e+00, -1.24957601e-01,  9.90221459e-01,\n",
       "         1.18500970e+00],\n",
       "       [-1.73673948e-01, -1.28197243e+00,  7.05892939e-01,\n",
       "         1.05353673e+00],\n",
       "       [-5.25060772e-02, -5.87763531e-01,  7.62758643e-01,\n",
       "         1.57942861e+00],\n",
       "       [ 6.74501145e-01,  3.37848329e-01,  8.76490051e-01,\n",
       "         1.44795564e+00],\n",
       "       [ 7.95669016e-01, -1.24957601e-01,  9.90221459e-01,\n",
       "         7.90590793e-01],\n",
       "       [ 2.24968346e+00,  1.72626612e+00,  1.67260991e+00,\n",
       "         1.31648267e+00],\n",
       "       [ 2.24968346e+00, -1.05056946e+00,  1.78634131e+00,\n",
       "         1.44795564e+00],\n",
       "       [ 1.89829664e-01, -1.97618132e+00,  7.05892939e-01,\n",
       "         3.96171883e-01],\n",
       "       [ 1.28034050e+00,  3.37848329e-01,  1.10395287e+00,\n",
       "         1.44795564e+00],\n",
       "       [-2.94841818e-01, -5.87763531e-01,  6.49027235e-01,\n",
       "         1.05353673e+00],\n",
       "       [ 2.24968346e+00, -5.87763531e-01,  1.67260991e+00,\n",
       "         1.05353673e+00],\n",
       "       [ 5.53333275e-01, -8.19166497e-01,  6.49027235e-01,\n",
       "         7.90590793e-01],\n",
       "       [ 1.03800476e+00,  5.69251294e-01,  1.10395287e+00,\n",
       "         1.18500970e+00],\n",
       "       [ 1.64384411e+00,  3.37848329e-01,  1.27454998e+00,\n",
       "         7.90590793e-01],\n",
       "       [ 4.32165405e-01, -5.87763531e-01,  5.92161531e-01,\n",
       "         7.90590793e-01],\n",
       "       [ 3.10997534e-01, -1.24957601e-01,  6.49027235e-01,\n",
       "         7.90590793e-01],\n",
       "       [ 6.74501145e-01, -5.87763531e-01,  1.04708716e+00,\n",
       "         1.18500970e+00],\n",
       "       [ 1.64384411e+00, -1.24957601e-01,  1.16081857e+00,\n",
       "         5.27644853e-01],\n",
       "       [ 1.88617985e+00, -5.87763531e-01,  1.33141568e+00,\n",
       "         9.22063763e-01],\n",
       "       [ 2.49201920e+00,  1.72626612e+00,  1.50201279e+00,\n",
       "         1.05353673e+00],\n",
       "       [ 6.74501145e-01, -5.87763531e-01,  1.04708716e+00,\n",
       "         1.31648267e+00],\n",
       "       [ 5.53333275e-01, -5.87763531e-01,  7.62758643e-01,\n",
       "         3.96171883e-01],\n",
       "       [ 3.10997534e-01, -1.05056946e+00,  1.04708716e+00,\n",
       "         2.64698913e-01],\n",
       "       [ 2.24968346e+00, -1.24957601e-01,  1.33141568e+00,\n",
       "         1.44795564e+00],\n",
       "       [ 5.53333275e-01,  8.00654259e-01,  1.04708716e+00,\n",
       "         1.57942861e+00],\n",
       "       [ 6.74501145e-01,  1.06445364e-01,  9.90221459e-01,\n",
       "         7.90590793e-01],\n",
       "       [ 1.89829664e-01, -1.24957601e-01,  5.92161531e-01,\n",
       "         7.90590793e-01],\n",
       "       [ 1.28034050e+00,  1.06445364e-01,  9.33355755e-01,\n",
       "         1.18500970e+00],\n",
       "       [ 1.03800476e+00,  1.06445364e-01,  1.04708716e+00,\n",
       "         1.57942861e+00],\n",
       "       [ 1.28034050e+00,  1.06445364e-01,  7.62758643e-01,\n",
       "         1.44795564e+00],\n",
       "       [-5.25060772e-02, -8.19166497e-01,  7.62758643e-01,\n",
       "         9.22063763e-01],\n",
       "       [ 1.15917263e+00,  3.37848329e-01,  1.21768427e+00,\n",
       "         1.44795564e+00],\n",
       "       [ 1.03800476e+00,  5.69251294e-01,  1.10395287e+00,\n",
       "         1.71090158e+00],\n",
       "       [ 1.03800476e+00, -1.24957601e-01,  8.19624347e-01,\n",
       "         1.44795564e+00],\n",
       "       [ 5.53333275e-01, -1.28197243e+00,  7.05892939e-01,\n",
       "         9.22063763e-01],\n",
       "       [ 7.95669016e-01, -1.24957601e-01,  8.19624347e-01,\n",
       "         1.05353673e+00],\n",
       "       [ 4.32165405e-01,  8.00654259e-01,  9.33355755e-01,\n",
       "         1.44795564e+00],\n",
       "       [ 6.86617933e-02, -1.24957601e-01,  7.62758643e-01,\n",
       "         7.90590793e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply feature scaling and transformation\n",
    "\n",
    "# X_original['some_feature'] = preprocessing.scale(X_original['some_feature'])\n",
    "preprocessing.scale(X_original, copy=False)\n",
    "# X_original['some_feature'] = preprocessing.normalize(X_original['some_feature'])\n",
    "# preprocessing.normalize(X_original, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms for each attribute after pre-processing\n",
    "X_original.hist(layout=(dispRow,dispCol))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot-encoding before splitting into trainig and test\n",
    "# X_original = pd.get_dummies(X_original)\n",
    "# print(X_original.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.d) Splitting Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Encode class values as integers and perform one-hot-encoding\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(y_original)\n",
    "y_transformed = encoder.transform(y_original)\n",
    "y_encoded = np_utils.to_categorical(y_transformed)\n",
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (112, 4) X_train.type: <class 'numpy.ndarray'>\n",
      "y_train.shape: (112, 3) y_train.type: <class 'numpy.ndarray'>\n",
      "X_test.shape: (38, 4) X_test.type: <class 'numpy.ndarray'>\n",
      "y_test.shape: (38, 3) y_test.type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_encoded = X_original.values\n",
    "if (splitDataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=splitPercentage, random_state=seedNum)\n",
    "else:\n",
    "    X_train, y_train = X_encoded, y_encoded\n",
    "    X_test, y_test = X_encoded, y_encoded\n",
    "print(\"X_train.shape: {} X_train.type: {}\".format(X_train.shape, type(X_train)))\n",
    "print(\"y_train.shape: {} y_train.type: {}\".format(y_train.shape, type(y_train)))\n",
    "print(\"X_test.shape: {} X_test.type: {}\".format(X_test.shape, type(X_test)))\n",
    "print(\"y_test.shape: {} y_test.type: {}\".format(y_test.shape, type(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 1 Load Data completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 2 Define Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model required for KerasClassifier\n",
    "def create_default_model():\n",
    "    default_model = K.models.Sequential()\n",
    "    default_model.add(Dense(8, input_dim=4, kernel_initializer=default_kernel_init, activation='relu'))\n",
    "    default_model.add(Dense(3, kernel_initializer=default_kernel_init, activation='softmax'))\n",
    "    default_model.compile(loss=default_loss, optimizer=default_optimizer, metrics=default_metrics)\n",
    "    return default_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Keras model\n",
    "cv_model = KerasClassifier(build_fn=create_default_model, epochs=default_epochs, batch_size=default_batches, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 2 Define Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Fit and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 3 Fit and Evaluate Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_742 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_13220 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_13978 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_26456 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_27214 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_39692 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_40450 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_52928 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_53686 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_66164 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Generating results using the metrics of ['accuracy']\n",
      "All cross-Validate results: [0.95652175 0.95652175 0.95454544 1.         1.        ]\n",
      "Baseline results [mean (std)]: 97.35% (2.16%)\n",
      "Total time for performing cross-validation of the default model: 0:00:32.451961\n"
     ]
    }
   ],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "# Fit and evaluate the Keras model using 10-fold cross validation\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seedNum)\n",
    "results = cross_val_score(cv_model, X_train, y_train, cv=kfold)\n",
    "print('Generating results using the metrics of', default_metrics)\n",
    "print('All cross-Validate results:', results)\n",
    "print('Baseline results [mean (std)]: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
    "\n",
    "print('Total time for performing cross-validation of the default model:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 3 Fit and Evaluate Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 4 Optimize Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model required for KerasClassifier\n",
    "def create_customized_model(optimizer, kernel_init):\n",
    "    customized_model = K.models.Sequential()\n",
    "    customized_model.add(Dense(8, input_dim=4, kernel_initializer=kernel_init, activation='relu'))\n",
    "    customized_model.add(Dense(3, kernel_initializer=kernel_init, activation='softmax'))\n",
    "    customized_model.compile(loss=default_loss, optimizer=optimizer, metrics=default_metrics)\n",
    "    return customized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  7.9min finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op __inference_keras_scratch_graph_67993 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Best: 0.991071 using {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.955357 (0.039641) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 500, 'batch_size': 16}\n",
      "0.955357 (0.027563) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 750, 'batch_size': 16}\n",
      "0.955357 (0.027563) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 750, 'batch_size': 16}\n",
      "0.973214 (0.021964) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 500, 'batch_size': 8}\n",
      "0.982143 (0.021798) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 750, 'batch_size': 16}\n",
      "0.964286 (0.033000) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 500, 'batch_size': 8}\n",
      "0.982143 (0.021798) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 1000, 'batch_size': 16}\n",
      "0.955357 (0.027563) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 750, 'batch_size': 8}\n",
      "0.982143 (0.021798) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 750, 'batch_size': 32}\n",
      "0.955357 (0.027563) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 1000, 'batch_size': 8}\n",
      "0.964286 (0.033000) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 1000, 'batch_size': 16}\n",
      "0.973214 (0.021558) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 750, 'batch_size': 32}\n",
      "0.982143 (0.035127) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 750, 'batch_size': 16}\n",
      "0.964286 (0.033000) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 500, 'batch_size': 8}\n",
      "0.955357 (0.039641) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 1000, 'batch_size': 8}\n",
      "0.955357 (0.039641) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 750, 'batch_size': 32}\n",
      "0.991071 (0.017564) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.973214 (0.021964) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 1000, 'batch_size': 16}\n",
      "0.973214 (0.021964) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 500, 'batch_size': 32}\n",
      "0.946429 (0.033171) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 500, 'batch_size': 8}\n",
      "0.964286 (0.033000) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 500, 'batch_size': 32}\n",
      "0.982143 (0.021390) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 500, 'batch_size': 16}\n",
      "0.982143 (0.035127) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 500, 'batch_size': 32}\n",
      "0.946429 (0.033171) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 500, 'batch_size': 32}\n",
      "0.964286 (0.017680) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 500, 'batch_size': 8}\n",
      "0.973214 (0.021558) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 750, 'batch_size': 32}\n",
      "0.973214 (0.021964) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 1000, 'batch_size': 16}\n",
      "0.964286 (0.033000) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.982143 (0.021798) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 750, 'batch_size': 32}\n",
      "0.955357 (0.028507) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.964286 (0.018172) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.973214 (0.021964) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 750, 'batch_size': 16}\n",
      "0.955357 (0.028507) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4090>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 1000, 'batch_size': 8}\n",
      "0.982143 (0.021798) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0bdc7f4690>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 500, 'batch_size': 16}\n",
      "0.964286 (0.033000) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 750, 'batch_size': 32}\n",
      "0.946429 (0.033436) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 500, 'batch_size': 16}\n",
      "0.964286 (0.033792) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 1000, 'batch_size': 8}\n",
      "0.964286 (0.018172) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f0c2007af10>, 'epochs': 500, 'batch_size': 16}\n",
      "0.982143 (0.021798) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f0c2007af90>, 'epochs': 500, 'batch_size': 32}\n",
      "0.955357 (0.028507) with: {'optimizer': <keras.optimizers.Adam object at 0x7f0c2007a2d0>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f0c2007a750>, 'epochs': 1000, 'batch_size': 8}\n",
      "Total time for performing grid-search of the best parameters: 0:08:07.861672\n"
     ]
    }
   ],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "# create model\n",
    "grid_model = KerasClassifier(build_fn=create_customized_model, verbose=0)\n",
    "\n",
    "# Perform grid search using different epochs, batch sizes, and optimizers\n",
    "optz_1 = K.optimizers.Adam(learning_rate=0.001)\n",
    "optz_2 = K.optimizers.Adam(learning_rate=0.005)\n",
    "optz_3 = K.optimizers.Adam(learning_rate=0.009)\n",
    "optimizer_grid = [optz_1, optz_2, optz_3]\n",
    "init_1 = K.initializers.RandomNormal(seed=seedNum)\n",
    "init_2 = K.initializers.glorot_normal(seed=seedNum)\n",
    "init_3 = K.initializers.Orthogonal(seed=seedNum)\n",
    "init_grid = [init_1, init_2, init_3]\n",
    "epoch_grid = [500, 750, 1000]\n",
    "batch_grid = [8, 16, 32]\n",
    "param_grid = dict(optimizer=optimizer_grid, kernel_init=init_grid, epochs=epoch_grid, batch_size=batch_grid)\n",
    "# grid = GridSearchCV(estimator=grid_model, param_grid=param_grid, cv=n_folds, n_jobs=n_jobs, verbose=3)\n",
    "n_iter = int(len(optimizer_grid) * len(init_grid) * len(epoch_grid) * len(batch_grid) * 0.5)\n",
    "grid = RandomizedSearchCV(estimator=grid_model, param_distributions=param_grid, n_iter=n_iter, cv=n_folds, n_jobs=n_jobs, verbose=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print('Total time for performing grid-search of the best parameters:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optimizer = grid_result.best_params_[\"optimizer\"]\n",
    "best_kernel_init = grid_result.best_params_[\"kernel_init\"]\n",
    "best_epoch = grid_result.best_params_[\"epochs\"]\n",
    "best_batch = grid_result.best_params_[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forming the final model using: optimizer=<keras.optimizers.Adam object at 0x7f0c2007a2d0>, kernel=<keras.initializers.RandomNormal object at 0x7f0c2007af90>, epochs=1000, batch_size=32\n",
      "Epoch 1/1000\n",
      "Executing op __inference_keras_scratch_graph_90695 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.0606 - accuracy: 0.6161\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.7080 - accuracy: 0.8214\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.4186 - accuracy: 0.7768\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.2976 - accuracy: 0.8661\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 0s 134us/step - loss: 0.2681 - accuracy: 0.8750\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.2335 - accuracy: 0.9018\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.2037 - accuracy: 0.9286\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.1599 - accuracy: 0.9464\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.1294 - accuracy: 0.9643\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.1070 - accuracy: 0.9732\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0897 - accuracy: 0.9821\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0797 - accuracy: 0.9732\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 0s 139us/step - loss: 0.0641 - accuracy: 0.9821\n",
      "Epoch 14/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0611 - accuracy: 1.0000\n",
      "Epoch 15/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.0501 - accuracy: 0.9911\n",
      "Epoch 16/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0474 - accuracy: 0.9732\n",
      "Epoch 17/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 0.0403 - accuracy: 0.9821\n",
      "Epoch 18/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0411 - accuracy: 1.0000\n",
      "Epoch 19/1000\n",
      "112/112 [==============================] - 0s 133us/step - loss: 0.0396 - accuracy: 1.0000\n",
      "Epoch 20/1000\n",
      "112/112 [==============================] - 0s 224us/step - loss: 0.0340 - accuracy: 0.9911\n",
      "Epoch 21/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0326 - accuracy: 0.9821\n",
      "Epoch 22/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 0.0307 - accuracy: 0.9911\n",
      "Epoch 23/1000\n",
      "112/112 [==============================] - 0s 143us/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 24/1000\n",
      "112/112 [==============================] - 0s 131us/step - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 25/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 26/1000\n",
      "112/112 [==============================] - 0s 131us/step - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 27/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 28/1000\n",
      "112/112 [==============================] - 0s 134us/step - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 29/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0258 - accuracy: 0.9911\n",
      "Epoch 30/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 31/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 32/1000\n",
      "112/112 [==============================] - 0s 127us/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 33/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 34/1000\n",
      "112/112 [==============================] - 0s 133us/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 35/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 36/1000\n",
      "112/112 [==============================] - 0s 145us/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 37/1000\n",
      "112/112 [==============================] - 0s 127us/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 38/1000\n",
      "112/112 [==============================] - 0s 127us/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 39/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 40/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 41/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 42/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 43/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 44/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 45/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 46/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 47/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 48/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 49/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 50/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 51/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 52/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 53/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 54/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 55/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 56/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 57/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 58/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 59/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 60/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 61/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 62/1000\n",
      "112/112 [==============================] - 0s 138us/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 63/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 64/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 65/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 66/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 67/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 68/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 69/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 70/1000\n",
      "112/112 [==============================] - 0s 131us/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 71/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 72/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 73/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 74/1000\n",
      "112/112 [==============================] - 0s 131us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 75/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 76/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 77/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 78/1000\n",
      "112/112 [==============================] - 0s 129us/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 79/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 80/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 81/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 82/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 83/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 84/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 85/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 86/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 87/1000\n",
      "112/112 [==============================] - 0s 138us/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 88/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 89/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 90/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 91/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 92/1000\n",
      "112/112 [==============================] - 0s 152us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 93/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 94/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 95/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 96/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 97/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 98/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 99/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 100/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 101/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 102/1000\n",
      "112/112 [==============================] - 0s 90us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 103/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 104/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 105/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 106/1000\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 107/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 108/1000\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 109/1000\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 110/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 111/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 112/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 113/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 114/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 115/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 116/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 117/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 118/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 119/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 120/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 121/1000\n",
      "112/112 [==============================] - 0s 92us/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 122/1000\n",
      "112/112 [==============================] - 0s 244us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 123/1000\n",
      "112/112 [==============================] - 0s 231us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 124/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 125/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 126/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 127/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 128/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 129/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 130/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 131/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 132/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 133/1000\n",
      "112/112 [==============================] - 0s 127us/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 134/1000\n",
      "112/112 [==============================] - 0s 138us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 135/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 136/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 137/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 138/1000\n",
      "112/112 [==============================] - 0s 127us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 139/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 140/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 141/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 142/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 143/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 144/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 145/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 146/1000\n",
      "112/112 [==============================] - 0s 132us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 147/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 148/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 149/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 150/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 151/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 152/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 153/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 154/1000\n",
      "112/112 [==============================] - 0s 130us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 155/1000\n",
      "112/112 [==============================] - 0s 134us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 156/1000\n",
      "112/112 [==============================] - 0s 138us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 157/1000\n",
      "112/112 [==============================] - 0s 131us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 158/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 159/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 160/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 161/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 162/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 163/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 164/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 165/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 166/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 167/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 168/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 169/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 170/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 171/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 172/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 173/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 174/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 175/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 176/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 177/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 178/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 179/1000\n",
      "112/112 [==============================] - 0s 134us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 180/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 181/1000\n",
      "112/112 [==============================] - 0s 127us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 182/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 183/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 184/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 185/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 186/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 187/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 188/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 189/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 190/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 191/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 192/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 193/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 194/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 195/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 196/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 197/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 198/1000\n",
      "112/112 [==============================] - 0s 131us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 199/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 200/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 201/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 202/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 203/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 204/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 205/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 206/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 207/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 208/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 209/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 210/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 211/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 212/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 213/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 214/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 215/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 216/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 217/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 218/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 219/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 220/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 221/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 222/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 223/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 224/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 225/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 226/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 227/1000\n",
      "112/112 [==============================] - 0s 97us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 228/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 229/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 230/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 231/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 232/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 233/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 234/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 235/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 236/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 237/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 238/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 239/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 240/1000\n",
      "112/112 [==============================] - 0s 174us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 241/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 242/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 243/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 244/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 245/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 246/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 247/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 248/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 249/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 250/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 251/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 252/1000\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 253/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 254/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 255/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 256/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 257/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 258/1000\n",
      "112/112 [==============================] - 0s 130us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 259/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 260/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 261/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 262/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 263/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 264/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 265/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 266/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 267/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 268/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 269/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 270/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 271/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 272/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 273/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 274/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 275/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 276/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 277/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 278/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 279/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 280/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 281/1000\n",
      "112/112 [==============================] - 0s 132us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 282/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 283/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 284/1000\n",
      "112/112 [==============================] - 0s 138us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 285/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 286/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 287/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 288/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 289/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 290/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 291/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 292/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 293/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 294/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 295/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 296/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 297/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 298/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 299/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 300/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 301/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 302/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 303/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 304/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 305/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 306/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 307/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 308/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 309/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 310/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 311/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 312/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 313/1000\n",
      "112/112 [==============================] - 0s 192us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 314/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 315/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 316/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 317/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 318/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 319/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 320/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 321/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 322/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 323/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 324/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 325/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 326/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 327/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 328/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 329/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 330/1000\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 331/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 332/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 333/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 334/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 335/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 336/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 337/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 338/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 339/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 340/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 341/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 342/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 343/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 344/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 345/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 346/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 347/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 348/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 349/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 350/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 351/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 352/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 353/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 354/1000\n",
      "112/112 [==============================] - 0s 140us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 355/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 356/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 357/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 358/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 359/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 360/1000\n",
      "112/112 [==============================] - 0s 139us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 361/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 362/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 363/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 364/1000\n",
      "112/112 [==============================] - 0s 134us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 365/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 366/1000\n",
      "112/112 [==============================] - 0s 149us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 367/1000\n",
      "112/112 [==============================] - 0s 133us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 368/1000\n",
      "112/112 [==============================] - 0s 132us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 369/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 370/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 371/1000\n",
      "112/112 [==============================] - 0s 143us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 372/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 373/1000\n",
      "112/112 [==============================] - 0s 131us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 374/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 375/1000\n",
      "112/112 [==============================] - 0s 135us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 376/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 377/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 378/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 379/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 380/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 381/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 382/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 383/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 384/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 385/1000\n",
      "112/112 [==============================] - 0s 129us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 386/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 387/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 388/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 389/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 390/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 391/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 392/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 393/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 394/1000\n",
      "112/112 [==============================] - 0s 141us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 395/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 396/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 397/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 398/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 399/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 400/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 401/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 402/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 403/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 404/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 405/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 406/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 407/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 408/1000\n",
      "112/112 [==============================] - 0s 171us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 409/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 410/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 411/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 412/1000\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.00 - 0s 125us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 413/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 414/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 415/1000\n",
      "112/112 [==============================] - 0s 96us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 416/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 417/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 418/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 419/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 420/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 421/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 422/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 423/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 424/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 425/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 426/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 427/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 428/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 429/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 430/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 431/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 432/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 433/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 434/1000\n",
      "112/112 [==============================] - 0s 127us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 435/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 436/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 437/1000\n",
      "112/112 [==============================] - 0s 97us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 438/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 439/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 440/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 441/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 442/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 443/1000\n",
      "112/112 [==============================] - 0s 130us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 444/1000\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 445/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 446/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 447/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 448/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 449/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 450/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 451/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 452/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 453/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 454/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 455/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 456/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 457/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 458/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 459/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 460/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 461/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 462/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 463/1000\n",
      "112/112 [==============================] - 0s 183us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 464/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 465/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 466/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 467/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 468/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 469/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 470/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 471/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 472/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 473/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 474/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 475/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 476/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 477/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 478/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 479/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 480/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 481/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 482/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 483/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 484/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 485/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 486/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 487/1000\n",
      "112/112 [==============================] - 0s 98us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 488/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 489/1000\n",
      "112/112 [==============================] - 0s 154us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 490/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 491/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 492/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 493/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 494/1000\n",
      "112/112 [==============================] - 0s 130us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 495/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 496/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 497/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 498/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 499/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 500/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 501/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 502/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 503/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 504/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 505/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 506/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 507/1000\n",
      "112/112 [==============================] - 0s 188us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 508/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 509/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 510/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 511/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 512/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 513/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 514/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 515/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 516/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 517/1000\n",
      "112/112 [==============================] - 0s 139us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 518/1000\n",
      "112/112 [==============================] - 0s 165us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 519/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 520/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 521/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 522/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 523/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 524/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 525/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 526/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 527/1000\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 528/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 529/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 530/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 531/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 532/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 533/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 534/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 535/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 536/1000\n",
      "112/112 [==============================] - 0s 131us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 537/1000\n",
      "112/112 [==============================] - 0s 211us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 538/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 539/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 540/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 541/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 542/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 543/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 544/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 545/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 546/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 547/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 548/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 549/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 550/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 551/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 552/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 553/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 554/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 555/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 556/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 557/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 558/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 559/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 560/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 561/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 562/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 563/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 564/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 565/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 566/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 567/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 568/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 569/1000\n",
      "112/112 [==============================] - 0s 152us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 570/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 571/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 572/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 573/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 574/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 575/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 576/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 577/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 578/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 579/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 580/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 581/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 582/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 583/1000\n",
      "112/112 [==============================] - 0s 135us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 584/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 585/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 586/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 587/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 588/1000\n",
      "112/112 [==============================] - 0s 98us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 589/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 590/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 591/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 592/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 593/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 594/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 595/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 596/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 597/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 598/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 599/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 600/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 601/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 602/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 603/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 604/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 605/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 606/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 607/1000\n",
      "112/112 [==============================] - 0s 134us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 608/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 609/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 610/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 611/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 612/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 613/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 614/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 615/1000\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 616/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 617/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 618/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 619/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 620/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 621/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 622/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 623/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 624/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 625/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 626/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 627/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 628/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 629/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 630/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 631/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 632/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 633/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 634/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 635/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 636/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 637/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 638/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 639/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 640/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 641/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 642/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 643/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 644/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 645/1000\n",
      "112/112 [==============================] - ETA: 0s - loss: 5.0793e-04 - accuracy: 1.00 - 0s 118us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 646/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 647/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 648/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 649/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 650/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 651/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 652/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 653/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 654/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 655/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 656/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 657/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 658/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 9.7775e-04 - accuracy: 1.0000\n",
      "Epoch 659/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 9.9014e-04 - accuracy: 1.0000\n",
      "Epoch 660/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 661/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 662/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 663/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 664/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 665/1000\n",
      "112/112 [==============================] - 0s 144us/step - loss: 9.7997e-04 - accuracy: 1.0000\n",
      "Epoch 666/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 667/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 668/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 669/1000\n",
      "112/112 [==============================] - 0s 127us/step - loss: 9.9123e-04 - accuracy: 1.0000\n",
      "Epoch 670/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 9.9625e-04 - accuracy: 1.0000\n",
      "Epoch 671/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 9.7889e-04 - accuracy: 1.0000\n",
      "Epoch 672/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 9.5567e-04 - accuracy: 1.0000\n",
      "Epoch 673/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 9.8527e-04 - accuracy: 1.0000\n",
      "Epoch 674/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 9.8840e-04 - accuracy: 1.0000\n",
      "Epoch 675/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 9.9520e-04 - accuracy: 1.0000\n",
      "Epoch 676/1000\n",
      "112/112 [==============================] - 0s 129us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 677/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 9.4760e-04 - accuracy: 1.0000\n",
      "Epoch 678/1000\n",
      "112/112 [==============================] - 0s 206us/step - loss: 9.8276e-04 - accuracy: 1.0000\n",
      "Epoch 679/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 9.6680e-04 - accuracy: 1.0000\n",
      "Epoch 680/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 9.5258e-04 - accuracy: 1.0000\n",
      "Epoch 681/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 9.4628e-04 - accuracy: 1.0000\n",
      "Epoch 682/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 9.3995e-04 - accuracy: 1.0000\n",
      "Epoch 683/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 9.3969e-04 - accuracy: 1.0000\n",
      "Epoch 684/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 9.4934e-04 - accuracy: 1.0000\n",
      "Epoch 685/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 9.3564e-04 - accuracy: 1.0000\n",
      "Epoch 686/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 9.4103e-04 - accuracy: 1.0000\n",
      "Epoch 687/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 9.2198e-04 - accuracy: 1.0000\n",
      "Epoch 688/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 9.7246e-04 - accuracy: 1.0000\n",
      "Epoch 689/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 9.6719e-04 - accuracy: 1.0000\n",
      "Epoch 690/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 9.2508e-04 - accuracy: 1.0000\n",
      "Epoch 691/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 9.0557e-04 - accuracy: 1.0000\n",
      "Epoch 692/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 9.0860e-04 - accuracy: 1.0000\n",
      "Epoch 693/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 9.1888e-04 - accuracy: 1.0000\n",
      "Epoch 694/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 9.2590e-04 - accuracy: 1.0000\n",
      "Epoch 695/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 9.0771e-04 - accuracy: 1.0000\n",
      "Epoch 696/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 9.1285e-04 - accuracy: 1.0000\n",
      "Epoch 697/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 8.8793e-04 - accuracy: 1.0000\n",
      "Epoch 698/1000\n",
      "112/112 [==============================] - 0s 166us/step - loss: 9.0499e-04 - accuracy: 1.0000\n",
      "Epoch 699/1000\n",
      "112/112 [==============================] - 0s 95us/step - loss: 9.1855e-04 - accuracy: 1.0000\n",
      "Epoch 700/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 9.2995e-04 - accuracy: 1.0000\n",
      "Epoch 701/1000\n",
      "112/112 [==============================] - 0s 94us/step - loss: 9.4372e-04 - accuracy: 1.0000\n",
      "Epoch 702/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 8.8506e-04 - accuracy: 1.0000\n",
      "Epoch 703/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 8.9367e-04 - accuracy: 1.0000\n",
      "Epoch 704/1000\n",
      "112/112 [==============================] - 0s 182us/step - loss: 8.7530e-04 - accuracy: 1.0000\n",
      "Epoch 705/1000\n",
      "112/112 [==============================] - 0s 97us/step - loss: 8.9410e-04 - accuracy: 1.0000\n",
      "Epoch 706/1000\n",
      "112/112 [==============================] - 0s 189us/step - loss: 8.7284e-04 - accuracy: 1.0000\n",
      "Epoch 707/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 8.9203e-04 - accuracy: 1.0000\n",
      "Epoch 708/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 8.9263e-04 - accuracy: 1.0000\n",
      "Epoch 709/1000\n",
      "112/112 [==============================] - 0s 97us/step - loss: 8.9141e-04 - accuracy: 1.0000\n",
      "Epoch 710/1000\n",
      "112/112 [==============================] - 0s 92us/step - loss: 8.9902e-04 - accuracy: 1.0000\n",
      "Epoch 711/1000\n",
      "112/112 [==============================] - 0s 98us/step - loss: 8.7505e-04 - accuracy: 1.0000\n",
      "Epoch 712/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 8.6308e-04 - accuracy: 1.0000\n",
      "Epoch 713/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 8.9216e-04 - accuracy: 1.0000\n",
      "Epoch 714/1000\n",
      "112/112 [==============================] - 0s 96us/step - loss: 8.7735e-04 - accuracy: 1.0000\n",
      "Epoch 715/1000\n",
      "112/112 [==============================] - 0s 96us/step - loss: 8.7047e-04 - accuracy: 1.0000\n",
      "Epoch 716/1000\n",
      "112/112 [==============================] - 0s 165us/step - loss: 8.6641e-04 - accuracy: 1.0000\n",
      "Epoch 717/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 8.2094e-04 - accuracy: 1.0000\n",
      "Epoch 718/1000\n",
      "112/112 [==============================] - 0s 97us/step - loss: 8.5551e-04 - accuracy: 1.0000\n",
      "Epoch 719/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 8.6446e-04 - accuracy: 1.0000\n",
      "Epoch 720/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 9.0960e-04 - accuracy: 1.0000\n",
      "Epoch 721/1000\n",
      "112/112 [==============================] - 0s 157us/step - loss: 9.3802e-04 - accuracy: 1.0000\n",
      "Epoch 722/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 9.0626e-04 - accuracy: 1.0000\n",
      "Epoch 723/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 8.4330e-04 - accuracy: 1.0000\n",
      "Epoch 724/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 8.5711e-04 - accuracy: 1.0000\n",
      "Epoch 725/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 8.2019e-04 - accuracy: 1.0000\n",
      "Epoch 726/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 8.2107e-04 - accuracy: 1.0000\n",
      "Epoch 727/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 8.3711e-04 - accuracy: 1.0000\n",
      "Epoch 728/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 8.0286e-04 - accuracy: 1.0000\n",
      "Epoch 729/1000\n",
      "112/112 [==============================] - 0s 96us/step - loss: 8.2560e-04 - accuracy: 1.0000\n",
      "Epoch 730/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 8.4101e-04 - accuracy: 1.0000\n",
      "Epoch 731/1000\n",
      "112/112 [==============================] - 0s 98us/step - loss: 8.4911e-04 - accuracy: 1.0000\n",
      "Epoch 732/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 8.4394e-04 - accuracy: 1.0000\n",
      "Epoch 733/1000\n",
      "112/112 [==============================] - 0s 132us/step - loss: 8.1719e-04 - accuracy: 1.0000\n",
      "Epoch 734/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 8.2224e-04 - accuracy: 1.0000\n",
      "Epoch 735/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 8.1773e-04 - accuracy: 1.0000\n",
      "Epoch 736/1000\n",
      "112/112 [==============================] - 0s 132us/step - loss: 8.1049e-04 - accuracy: 1.0000\n",
      "Epoch 737/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 8.1577e-04 - accuracy: 1.0000\n",
      "Epoch 738/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 7.9538e-04 - accuracy: 1.0000\n",
      "Epoch 739/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 7.9193e-04 - accuracy: 1.0000\n",
      "Epoch 740/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 7.8742e-04 - accuracy: 1.0000\n",
      "Epoch 741/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 7.8525e-04 - accuracy: 1.0000\n",
      "Epoch 742/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 7.7791e-04 - accuracy: 1.0000\n",
      "Epoch 743/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 7.9964e-04 - accuracy: 1.0000\n",
      "Epoch 744/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 7.8434e-04 - accuracy: 1.0000\n",
      "Epoch 745/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 7.7902e-04 - accuracy: 1.0000\n",
      "Epoch 746/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 7.7358e-04 - accuracy: 1.0000\n",
      "Epoch 747/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 7.7753e-04 - accuracy: 1.0000\n",
      "Epoch 748/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 7.6951e-04 - accuracy: 1.0000\n",
      "Epoch 749/1000\n",
      "112/112 [==============================] - 0s 133us/step - loss: 7.7996e-04 - accuracy: 1.0000\n",
      "Epoch 750/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 8.0200e-04 - accuracy: 1.0000\n",
      "Epoch 751/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 7.7788e-04 - accuracy: 1.0000\n",
      "Epoch 752/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 7.9204e-04 - accuracy: 1.0000\n",
      "Epoch 753/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 7.7421e-04 - accuracy: 1.0000\n",
      "Epoch 754/1000\n",
      "112/112 [==============================] - 0s 99us/step - loss: 7.6472e-04 - accuracy: 1.0000\n",
      "Epoch 755/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 7.5470e-04 - accuracy: 1.0000\n",
      "Epoch 756/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 7.7432e-04 - accuracy: 1.0000\n",
      "Epoch 757/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 8.0609e-04 - accuracy: 1.0000\n",
      "Epoch 758/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 7.7457e-04 - accuracy: 1.0000\n",
      "Epoch 759/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 7.4768e-04 - accuracy: 1.0000\n",
      "Epoch 760/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 7.5512e-04 - accuracy: 1.0000\n",
      "Epoch 761/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 7.3600e-04 - accuracy: 1.0000\n",
      "Epoch 762/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 7.9527e-04 - accuracy: 1.0000\n",
      "Epoch 763/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 8.0325e-04 - accuracy: 1.0000\n",
      "Epoch 764/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 7.8438e-04 - accuracy: 1.0000\n",
      "Epoch 765/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 7.7376e-04 - accuracy: 1.0000\n",
      "Epoch 766/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 7.5127e-04 - accuracy: 1.0000\n",
      "Epoch 767/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 7.2517e-04 - accuracy: 1.0000\n",
      "Epoch 768/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 7.3632e-04 - accuracy: 1.0000\n",
      "Epoch 769/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 7.5160e-04 - accuracy: 1.0000\n",
      "Epoch 770/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 7.4654e-04 - accuracy: 1.0000\n",
      "Epoch 771/1000\n",
      "112/112 [==============================] - 0s 151us/step - loss: 7.2964e-04 - accuracy: 1.0000\n",
      "Epoch 772/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 7.3893e-04 - accuracy: 1.0000\n",
      "Epoch 773/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 7.6175e-04 - accuracy: 1.0000\n",
      "Epoch 774/1000\n",
      "112/112 [==============================] - 0s 143us/step - loss: 7.4835e-04 - accuracy: 1.0000\n",
      "Epoch 775/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 7.4055e-04 - accuracy: 1.0000\n",
      "Epoch 776/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 7.3029e-04 - accuracy: 1.0000\n",
      "Epoch 777/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 7.0987e-04 - accuracy: 1.0000\n",
      "Epoch 778/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 7.0002e-04 - accuracy: 1.0000\n",
      "Epoch 779/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 7.2060e-04 - accuracy: 1.0000\n",
      "Epoch 780/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 7.2729e-04 - accuracy: 1.0000\n",
      "Epoch 781/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 7.2133e-04 - accuracy: 1.0000\n",
      "Epoch 782/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 7.0230e-04 - accuracy: 1.0000\n",
      "Epoch 783/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 6.8856e-04 - accuracy: 1.0000\n",
      "Epoch 784/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 7.1773e-04 - accuracy: 1.0000\n",
      "Epoch 785/1000\n",
      "112/112 [==============================] - 0s 131us/step - loss: 7.1348e-04 - accuracy: 1.0000\n",
      "Epoch 786/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 7.1475e-04 - accuracy: 1.0000\n",
      "Epoch 787/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 6.8840e-04 - accuracy: 1.0000\n",
      "Epoch 788/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 6.9758e-04 - accuracy: 1.0000\n",
      "Epoch 789/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 6.9886e-04 - accuracy: 1.0000\n",
      "Epoch 790/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 6.8963e-04 - accuracy: 1.0000\n",
      "Epoch 791/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 6.9053e-04 - accuracy: 1.0000\n",
      "Epoch 792/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 6.7640e-04 - accuracy: 1.0000\n",
      "Epoch 793/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 7.0611e-04 - accuracy: 1.0000\n",
      "Epoch 794/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 6.7485e-04 - accuracy: 1.0000\n",
      "Epoch 795/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 6.7264e-04 - accuracy: 1.0000\n",
      "Epoch 796/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 6.9525e-04 - accuracy: 1.0000\n",
      "Epoch 797/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 6.8019e-04 - accuracy: 1.0000\n",
      "Epoch 798/1000\n",
      "112/112 [==============================] - 0s 96us/step - loss: 6.7786e-04 - accuracy: 1.0000\n",
      "Epoch 799/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 6.7933e-04 - accuracy: 1.0000\n",
      "Epoch 800/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 6.7065e-04 - accuracy: 1.0000\n",
      "Epoch 801/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 6.7848e-04 - accuracy: 1.0000\n",
      "Epoch 802/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 6.8266e-04 - accuracy: 1.0000\n",
      "Epoch 803/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 6.6703e-04 - accuracy: 1.0000\n",
      "Epoch 804/1000\n",
      "112/112 [==============================] - 0s 127us/step - loss: 6.6230e-04 - accuracy: 1.0000\n",
      "Epoch 805/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 6.6027e-04 - accuracy: 1.0000\n",
      "Epoch 806/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 6.5677e-04 - accuracy: 1.0000\n",
      "Epoch 807/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 6.6228e-04 - accuracy: 1.0000\n",
      "Epoch 808/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 6.7471e-04 - accuracy: 1.0000\n",
      "Epoch 809/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 6.7863e-04 - accuracy: 1.0000\n",
      "Epoch 810/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 6.8236e-04 - accuracy: 1.0000\n",
      "Epoch 811/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 6.5197e-04 - accuracy: 1.0000\n",
      "Epoch 812/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 6.6620e-04 - accuracy: 1.0000\n",
      "Epoch 813/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 6.6265e-04 - accuracy: 1.0000\n",
      "Epoch 814/1000\n",
      "112/112 [==============================] - 0s 185us/step - loss: 6.3798e-04 - accuracy: 1.0000\n",
      "Epoch 815/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 6.4388e-04 - accuracy: 1.0000\n",
      "Epoch 816/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 6.5234e-04 - accuracy: 1.0000\n",
      "Epoch 817/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 6.7060e-04 - accuracy: 1.0000\n",
      "Epoch 818/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 6.7582e-04 - accuracy: 1.0000\n",
      "Epoch 819/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 6.6518e-04 - accuracy: 1.0000\n",
      "Epoch 820/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 6.3115e-04 - accuracy: 1.0000\n",
      "Epoch 821/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 6.1461e-04 - accuracy: 1.0000\n",
      "Epoch 822/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 6.4212e-04 - accuracy: 1.0000\n",
      "Epoch 823/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 6.4591e-04 - accuracy: 1.0000\n",
      "Epoch 824/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 6.3924e-04 - accuracy: 1.0000\n",
      "Epoch 825/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 6.3324e-04 - accuracy: 1.0000\n",
      "Epoch 826/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 6.3266e-04 - accuracy: 1.0000\n",
      "Epoch 827/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 6.2537e-04 - accuracy: 1.0000\n",
      "Epoch 828/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 6.4259e-04 - accuracy: 1.0000\n",
      "Epoch 829/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 6.2783e-04 - accuracy: 1.0000\n",
      "Epoch 830/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 6.3296e-04 - accuracy: 1.0000\n",
      "Epoch 831/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 6.5341e-04 - accuracy: 1.0000\n",
      "Epoch 832/1000\n",
      "112/112 [==============================] - 0s 163us/step - loss: 6.0534e-04 - accuracy: 1.0000\n",
      "Epoch 833/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 6.2801e-04 - accuracy: 1.0000\n",
      "Epoch 834/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 6.1652e-04 - accuracy: 1.0000\n",
      "Epoch 835/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 6.1021e-04 - accuracy: 1.0000\n",
      "Epoch 836/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 6.0788e-04 - accuracy: 1.0000\n",
      "Epoch 837/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 6.0596e-04 - accuracy: 1.0000\n",
      "Epoch 838/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 6.0229e-04 - accuracy: 1.0000\n",
      "Epoch 839/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 6.0711e-04 - accuracy: 1.0000\n",
      "Epoch 840/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 5.9137e-04 - accuracy: 1.0000\n",
      "Epoch 841/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 6.0016e-04 - accuracy: 1.0000\n",
      "Epoch 842/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 5.9169e-04 - accuracy: 1.0000\n",
      "Epoch 843/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 5.8569e-04 - accuracy: 1.0000\n",
      "Epoch 844/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 6.2049e-04 - accuracy: 1.0000\n",
      "Epoch 845/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 6.2902e-04 - accuracy: 1.0000\n",
      "Epoch 846/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 6.0614e-04 - accuracy: 1.0000\n",
      "Epoch 847/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 5.8934e-04 - accuracy: 1.0000\n",
      "Epoch 848/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 5.9620e-04 - accuracy: 1.0000\n",
      "Epoch 849/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 6.0053e-04 - accuracy: 1.0000\n",
      "Epoch 850/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 6.1674e-04 - accuracy: 1.0000\n",
      "Epoch 851/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 5.7869e-04 - accuracy: 1.0000\n",
      "Epoch 852/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 5.7478e-04 - accuracy: 1.0000\n",
      "Epoch 853/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 5.7948e-04 - accuracy: 1.0000\n",
      "Epoch 854/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 5.7969e-04 - accuracy: 1.0000\n",
      "Epoch 855/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 5.8511e-04 - accuracy: 1.0000\n",
      "Epoch 856/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 6.0916e-04 - accuracy: 1.0000\n",
      "Epoch 857/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 5.7261e-04 - accuracy: 1.0000\n",
      "Epoch 858/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 5.6513e-04 - accuracy: 1.0000\n",
      "Epoch 859/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 5.7840e-04 - accuracy: 1.0000\n",
      "Epoch 860/1000\n",
      "112/112 [==============================] - 0s 139us/step - loss: 5.7277e-04 - accuracy: 1.0000\n",
      "Epoch 861/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 5.6256e-04 - accuracy: 1.0000\n",
      "Epoch 862/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 5.6800e-04 - accuracy: 1.0000\n",
      "Epoch 863/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 5.6608e-04 - accuracy: 1.0000\n",
      "Epoch 864/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 5.6581e-04 - accuracy: 1.0000\n",
      "Epoch 865/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 5.6847e-04 - accuracy: 1.0000\n",
      "Epoch 866/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 5.6048e-04 - accuracy: 1.0000\n",
      "Epoch 867/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 5.4855e-04 - accuracy: 1.0000\n",
      "Epoch 868/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 5.5001e-04 - accuracy: 1.0000\n",
      "Epoch 869/1000\n",
      "112/112 [==============================] - 0s 97us/step - loss: 5.6119e-04 - accuracy: 1.0000\n",
      "Epoch 870/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 5.5513e-04 - accuracy: 1.0000\n",
      "Epoch 871/1000\n",
      "112/112 [==============================] - 0s 124us/step - loss: 5.4968e-04 - accuracy: 1.0000\n",
      "Epoch 872/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 5.3088e-04 - accuracy: 1.0000\n",
      "Epoch 873/1000\n",
      "112/112 [==============================] - 0s 210us/step - loss: 5.8251e-04 - accuracy: 1.0000\n",
      "Epoch 874/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 5.6022e-04 - accuracy: 1.0000\n",
      "Epoch 875/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 5.4090e-04 - accuracy: 1.0000\n",
      "Epoch 876/1000\n",
      "112/112 [==============================] - 0s 151us/step - loss: 5.3776e-04 - accuracy: 1.0000\n",
      "Epoch 877/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 5.4026e-04 - accuracy: 1.0000\n",
      "Epoch 878/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 5.5911e-04 - accuracy: 1.0000\n",
      "Epoch 879/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 5.5821e-04 - accuracy: 1.0000\n",
      "Epoch 880/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 5.5132e-04 - accuracy: 1.0000\n",
      "Epoch 881/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 5.6854e-04 - accuracy: 1.0000\n",
      "Epoch 882/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 5.3354e-04 - accuracy: 1.0000\n",
      "Epoch 883/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 5.3642e-04 - accuracy: 1.0000\n",
      "Epoch 884/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 5.4718e-04 - accuracy: 1.0000\n",
      "Epoch 885/1000\n",
      "112/112 [==============================] - 0s 97us/step - loss: 5.2446e-04 - accuracy: 1.0000\n",
      "Epoch 886/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 5.3168e-04 - accuracy: 1.0000\n",
      "Epoch 887/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 5.3216e-04 - accuracy: 1.0000\n",
      "Epoch 888/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 5.2311e-04 - accuracy: 1.0000\n",
      "Epoch 889/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 5.3949e-04 - accuracy: 1.0000\n",
      "Epoch 890/1000\n",
      "112/112 [==============================] - 0s 136us/step - loss: 5.2763e-04 - accuracy: 1.0000\n",
      "Epoch 891/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 5.3590e-04 - accuracy: 1.0000\n",
      "Epoch 892/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 5.3240e-04 - accuracy: 1.0000\n",
      "Epoch 893/1000\n",
      "112/112 [==============================] - 0s 100us/step - loss: 5.2542e-04 - accuracy: 1.0000\n",
      "Epoch 894/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 5.1678e-04 - accuracy: 1.0000\n",
      "Epoch 895/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 5.1982e-04 - accuracy: 1.0000\n",
      "Epoch 896/1000\n",
      "112/112 [==============================] - 0s 97us/step - loss: 5.1787e-04 - accuracy: 1.0000\n",
      "Epoch 897/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 5.0769e-04 - accuracy: 1.0000\n",
      "Epoch 898/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 5.1628e-04 - accuracy: 1.0000\n",
      "Epoch 899/1000\n",
      "112/112 [==============================] - 0s 129us/step - loss: 5.1181e-04 - accuracy: 1.0000\n",
      "Epoch 900/1000\n",
      "112/112 [==============================] - 0s 99us/step - loss: 5.1629e-04 - accuracy: 1.0000\n",
      "Epoch 901/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 5.0808e-04 - accuracy: 1.0000\n",
      "Epoch 902/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 5.2361e-04 - accuracy: 1.0000\n",
      "Epoch 903/1000\n",
      "112/112 [==============================] - 0s 98us/step - loss: 5.1688e-04 - accuracy: 1.0000\n",
      "Epoch 904/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 5.2415e-04 - accuracy: 1.0000\n",
      "Epoch 905/1000\n",
      "112/112 [==============================] - 0s 110us/step - loss: 5.0218e-04 - accuracy: 1.0000\n",
      "Epoch 906/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 5.0334e-04 - accuracy: 1.0000\n",
      "Epoch 907/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 5.1096e-04 - accuracy: 1.0000\n",
      "Epoch 908/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 4.9360e-04 - accuracy: 1.0000\n",
      "Epoch 909/1000\n",
      "112/112 [==============================] - 0s 122us/step - loss: 5.0066e-04 - accuracy: 1.0000\n",
      "Epoch 910/1000\n",
      "112/112 [==============================] - 0s 153us/step - loss: 4.9483e-04 - accuracy: 1.0000\n",
      "Epoch 911/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 4.9281e-04 - accuracy: 1.0000\n",
      "Epoch 912/1000\n",
      "112/112 [==============================] - 0s 105us/step - loss: 5.0587e-04 - accuracy: 1.0000\n",
      "Epoch 913/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 4.9143e-04 - accuracy: 1.0000\n",
      "Epoch 914/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 4.8550e-04 - accuracy: 1.0000\n",
      "Epoch 915/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 4.9138e-04 - accuracy: 1.0000\n",
      "Epoch 916/1000\n",
      "112/112 [==============================] - 0s 142us/step - loss: 4.8181e-04 - accuracy: 1.0000\n",
      "Epoch 917/1000\n",
      "112/112 [==============================] - 0s 134us/step - loss: 4.8856e-04 - accuracy: 1.0000\n",
      "Epoch 918/1000\n",
      "112/112 [==============================] - 0s 130us/step - loss: 4.7898e-04 - accuracy: 1.0000\n",
      "Epoch 919/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 4.8669e-04 - accuracy: 1.0000\n",
      "Epoch 920/1000\n",
      "112/112 [==============================] - 0s 126us/step - loss: 4.9655e-04 - accuracy: 1.0000\n",
      "Epoch 921/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 4.8047e-04 - accuracy: 1.0000\n",
      "Epoch 922/1000\n",
      "112/112 [==============================] - 0s 204us/step - loss: 4.8931e-04 - accuracy: 1.0000\n",
      "Epoch 923/1000\n",
      "112/112 [==============================] - 0s 120us/step - loss: 4.7888e-04 - accuracy: 1.0000\n",
      "Epoch 924/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 4.8898e-04 - accuracy: 1.0000\n",
      "Epoch 925/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 4.6417e-04 - accuracy: 1.0000\n",
      "Epoch 926/1000\n",
      "112/112 [==============================] - 0s 131us/step - loss: 4.7191e-04 - accuracy: 1.0000\n",
      "Epoch 927/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 4.8820e-04 - accuracy: 1.0000\n",
      "Epoch 928/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 4.8893e-04 - accuracy: 1.0000\n",
      "Epoch 929/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 4.8699e-04 - accuracy: 1.0000\n",
      "Epoch 930/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 4.7839e-04 - accuracy: 1.0000\n",
      "Epoch 931/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 4.6839e-04 - accuracy: 1.0000\n",
      "Epoch 932/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 5.0592e-04 - accuracy: 1.0000\n",
      "Epoch 933/1000\n",
      "112/112 [==============================] - 0s 133us/step - loss: 4.7852e-04 - accuracy: 1.0000\n",
      "Epoch 934/1000\n",
      "112/112 [==============================] - 0s 142us/step - loss: 4.6493e-04 - accuracy: 1.0000\n",
      "Epoch 935/1000\n",
      "112/112 [==============================] - 0s 127us/step - loss: 4.5949e-04 - accuracy: 1.0000\n",
      "Epoch 936/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 4.7528e-04 - accuracy: 1.0000\n",
      "Epoch 937/1000\n",
      "112/112 [==============================] - 0s 155us/step - loss: 4.6174e-04 - accuracy: 1.0000\n",
      "Epoch 938/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 4.6089e-04 - accuracy: 1.0000\n",
      "Epoch 939/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 4.5142e-04 - accuracy: 1.0000\n",
      "Epoch 940/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 4.5064e-04 - accuracy: 1.0000\n",
      "Epoch 941/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 4.5945e-04 - accuracy: 1.0000\n",
      "Epoch 942/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 4.6099e-04 - accuracy: 1.0000\n",
      "Epoch 943/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 4.4694e-04 - accuracy: 1.0000\n",
      "Epoch 944/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 4.5708e-04 - accuracy: 1.0000\n",
      "Epoch 945/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 4.4631e-04 - accuracy: 1.0000\n",
      "Epoch 946/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 4.4311e-04 - accuracy: 1.0000\n",
      "Epoch 947/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 4.4575e-04 - accuracy: 1.0000\n",
      "Epoch 948/1000\n",
      "112/112 [==============================] - 0s 118us/step - loss: 4.5847e-04 - accuracy: 1.0000\n",
      "Epoch 949/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 4.4301e-04 - accuracy: 1.0000\n",
      "Epoch 950/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 4.6177e-04 - accuracy: 1.0000\n",
      "Epoch 951/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 4.5323e-04 - accuracy: 1.0000\n",
      "Epoch 952/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 4.5047e-04 - accuracy: 1.0000\n",
      "Epoch 953/1000\n",
      "112/112 [==============================] - 0s 139us/step - loss: 4.3234e-04 - accuracy: 1.0000\n",
      "Epoch 954/1000\n",
      "112/112 [==============================] - 0s 101us/step - loss: 4.3261e-04 - accuracy: 1.0000\n",
      "Epoch 955/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 4.7278e-04 - accuracy: 1.0000\n",
      "Epoch 956/1000\n",
      "112/112 [==============================] - 0s 121us/step - loss: 4.6046e-04 - accuracy: 1.0000\n",
      "Epoch 957/1000\n",
      "112/112 [==============================] - 0s 139us/step - loss: 4.3502e-04 - accuracy: 1.0000\n",
      "Epoch 958/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 4.3069e-04 - accuracy: 1.0000\n",
      "Epoch 959/1000\n",
      "112/112 [==============================] - 0s 103us/step - loss: 4.3896e-04 - accuracy: 1.0000\n",
      "Epoch 960/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 4.4378e-04 - accuracy: 1.0000\n",
      "Epoch 961/1000\n",
      "112/112 [==============================] - 0s 134us/step - loss: 4.4915e-04 - accuracy: 1.0000\n",
      "Epoch 962/1000\n",
      "112/112 [==============================] - 0s 117us/step - loss: 4.3512e-04 - accuracy: 1.0000\n",
      "Epoch 963/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 4.2983e-04 - accuracy: 1.0000\n",
      "Epoch 964/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 4.2278e-04 - accuracy: 1.0000\n",
      "Epoch 965/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 4.5300e-04 - accuracy: 1.0000\n",
      "Epoch 966/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 4.3757e-04 - accuracy: 1.0000\n",
      "Epoch 967/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 4.3206e-04 - accuracy: 1.0000\n",
      "Epoch 968/1000\n",
      "112/112 [==============================] - 0s 109us/step - loss: 4.2571e-04 - accuracy: 1.0000\n",
      "Epoch 969/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 4.4806e-04 - accuracy: 1.0000\n",
      "Epoch 970/1000\n",
      "112/112 [==============================] - 0s 113us/step - loss: 4.2493e-04 - accuracy: 1.0000\n",
      "Epoch 971/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 4.1673e-04 - accuracy: 1.0000\n",
      "Epoch 972/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 4.1947e-04 - accuracy: 1.0000\n",
      "Epoch 973/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 4.1769e-04 - accuracy: 1.0000\n",
      "Epoch 974/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 4.1411e-04 - accuracy: 1.0000\n",
      "Epoch 975/1000\n",
      "112/112 [==============================] - 0s 128us/step - loss: 4.1411e-04 - accuracy: 1.0000\n",
      "Epoch 976/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 4.1093e-04 - accuracy: 1.0000\n",
      "Epoch 977/1000\n",
      "112/112 [==============================] - 0s 115us/step - loss: 4.0694e-04 - accuracy: 1.0000\n",
      "Epoch 978/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 4.0668e-04 - accuracy: 1.0000\n",
      "Epoch 979/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 4.0502e-04 - accuracy: 1.0000\n",
      "Epoch 980/1000\n",
      "112/112 [==============================] - 0s 104us/step - loss: 4.1200e-04 - accuracy: 1.0000\n",
      "Epoch 981/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 4.0407e-04 - accuracy: 1.0000\n",
      "Epoch 982/1000\n",
      "112/112 [==============================] - 0s 112us/step - loss: 4.0808e-04 - accuracy: 1.0000\n",
      "Epoch 983/1000\n",
      "112/112 [==============================] - 0s 111us/step - loss: 4.1836e-04 - accuracy: 1.0000\n",
      "Epoch 984/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 4.1747e-04 - accuracy: 1.0000\n",
      "Epoch 985/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 4.0423e-04 - accuracy: 1.0000\n",
      "Epoch 986/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 3.9987e-04 - accuracy: 1.0000\n",
      "Epoch 987/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 3.9583e-04 - accuracy: 1.0000\n",
      "Epoch 988/1000\n",
      "112/112 [==============================] - 0s 114us/step - loss: 4.1981e-04 - accuracy: 1.0000\n",
      "Epoch 989/1000\n",
      "112/112 [==============================] - 0s 119us/step - loss: 3.9148e-04 - accuracy: 1.0000\n",
      "Epoch 990/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 4.0238e-04 - accuracy: 1.0000\n",
      "Epoch 991/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 4.3236e-04 - accuracy: 1.0000\n",
      "Epoch 992/1000\n",
      "112/112 [==============================] - 0s 102us/step - loss: 3.9233e-04 - accuracy: 1.0000\n",
      "Epoch 993/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 3.9663e-04 - accuracy: 1.0000\n",
      "Epoch 994/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 4.0795e-04 - accuracy: 1.0000\n",
      "Epoch 995/1000\n",
      "112/112 [==============================] - 0s 106us/step - loss: 4.1660e-04 - accuracy: 1.0000\n",
      "Epoch 996/1000\n",
      "112/112 [==============================] - 0s 116us/step - loss: 4.1187e-04 - accuracy: 1.0000\n",
      "Epoch 997/1000\n",
      "112/112 [==============================] - 0s 108us/step - loss: 3.9118e-04 - accuracy: 1.0000\n",
      "Epoch 998/1000\n",
      "112/112 [==============================] - 0s 123us/step - loss: 4.0378e-04 - accuracy: 1.0000\n",
      "Epoch 999/1000\n",
      "112/112 [==============================] - 0s 125us/step - loss: 3.8850e-04 - accuracy: 1.0000\n",
      "Epoch 1000/1000\n",
      "112/112 [==============================] - 0s 107us/step - loss: 3.9360e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0cd89f7810>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the final model for evaluating the test dataset\n",
    "print('Forming the final model using: optimizer=%s, kernel=%s, epochs=%d, batch_size=%d'\n",
    "      % (best_optimizer, best_kernel_init, best_epoch, best_batch))\n",
    "final_model = create_customized_model(best_optimizer, best_kernel_init)\n",
    "final_model.fit(X_train, y_train, epochs=best_epoch, batch_size=best_batch, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of the final model\n",
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'sequential_7', 'layers': [{'class_name': 'Dense', 'config': {'name': 'dense_13', 'trainable': True, 'batch_input_shape': (None, 4), 'dtype': 'float32', 'units': 8, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'RandomNormal', 'config': {'mean': 0.0, 'stddev': 0.05, 'seed': 892}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_14', 'trainable': True, 'dtype': 'float32', 'units': 3, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'RandomNormal', 'config': {'mean': 0.0, 'stddev': 0.05, 'seed': 892}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n"
     ]
    }
   ],
   "source": [
    "# Display the configuration of the final model\n",
    "print(final_model.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op __inference_keras_scratch_graph_112773 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "38/38 [==============================] - 0s 2ms/step\n",
      "\n",
      "accuracy: 94.74%\n",
      "\n",
      "loss: 0.27\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Keras model on previously unseen data\n",
    "scores = final_model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (final_model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f\" % (final_model.metrics_names[0], scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 4 Optimize Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Finalize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 5 Finalize Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op __inference_keras_scratch_graph_112803 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Data item #0 predicted to be ['Iris-versicolor'] (expected ['Iris-versicolor'])\n",
      "Data item #1 predicted to be ['Iris-virginica'] (expected ['Iris-versicolor'])\n",
      "Data item #2 predicted to be ['Iris-virginica'] (expected ['Iris-virginica'])\n",
      "Data item #3 predicted to be ['Iris-virginica'] (expected ['Iris-virginica'])\n",
      "Data item #4 predicted to be ['Iris-virginica'] (expected ['Iris-virginica'])\n",
      "Data item #5 predicted to be ['Iris-versicolor'] (expected ['Iris-versicolor'])\n",
      "Data item #6 predicted to be ['Iris-versicolor'] (expected ['Iris-versicolor'])\n",
      "Data item #7 predicted to be ['Iris-virginica'] (expected ['Iris-virginica'])\n",
      "Data item #8 predicted to be ['Iris-virginica'] (expected ['Iris-virginica'])\n",
      "Data item #9 predicted to be ['Iris-versicolor'] (expected ['Iris-versicolor'])\n",
      "Data item #10 predicted to be ['Iris-setosa'] (expected ['Iris-setosa'])\n",
      "Data item #11 predicted to be ['Iris-versicolor'] (expected ['Iris-versicolor'])\n",
      "Data item #12 predicted to be ['Iris-setosa'] (expected ['Iris-setosa'])\n",
      "Data item #13 predicted to be ['Iris-versicolor'] (expected ['Iris-versicolor'])\n",
      "Data item #14 predicted to be ['Iris-versicolor'] (expected ['Iris-versicolor'])\n",
      "Data item #15 predicted to be ['Iris-versicolor'] (expected ['Iris-versicolor'])\n",
      "Data item #16 predicted to be ['Iris-versicolor'] (expected ['Iris-versicolor'])\n",
      "Data item #17 predicted to be ['Iris-setosa'] (expected ['Iris-setosa'])\n",
      "Data item #18 predicted to be ['Iris-setosa'] (expected ['Iris-setosa'])\n",
      "Data item #19 predicted to be ['Iris-setosa'] (expected ['Iris-setosa'])\n"
     ]
    }
   ],
   "source": [
    "# Make class predictions with the model\n",
    "predictions = final_model.predict_classes(X_test)\n",
    "\n",
    "# Summarize the first 20 cases\n",
    "for i in range(20):\n",
    "\tprint('Data item #%d predicted to be %s (expected %s)' % (i, encoder.inverse_transform([predictions[i]]), encoder.inverse_transform([np.argmax(y_test[i])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 5 Finalize Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for the script: 0:09:05.342208\n"
     ]
    }
   ],
   "source": [
    "print ('Total time for the script:',(datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
