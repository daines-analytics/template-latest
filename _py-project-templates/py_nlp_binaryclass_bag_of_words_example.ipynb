{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BthRezPlR9Ow"
   },
   "source": [
    "# Text Classification Model for [PROJECT NAME] Using TensorFlow Version 1\n",
    "### David Lowe\n",
    "### January 1, 2021\n",
    "\n",
    "Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [https://machinelearningmastery.com/]\n",
    "\n",
    "SUMMARY: This project aims to construct a text classification model using a neural network and document the end-to-end steps using a template. The [PROJECT NAME] dataset is a binary classification situation where we attempt to predict one of the two possible outcomes.\n",
    "\n",
    "INTRODUCTION: [SAMPLE PARAGRAPH: The Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in the early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available as part of their research on natural language processing. The dataset comprises 1,000 positive and 1,000 negative movie reviews drawn from an archive of the rec.arts.movies.reviews newsgroup hosted at IMDB. We will use the last 100 positive reviews and the last 100 negative reviews as a test set (200 reviews) and the remaining 1,800 reviews as the training dataset. We will construct a bag-of-words model and analyzed it with a simple TensorFlow deep learning network.]\n",
    "\n",
    "ANALYSIS: [SAMPLE PARAGRAPH: In this modeling iteration, the baseline model's performance achieved an average accuracy score of 86.06% after 25 epochs with ten iterations of cross-validation. Furthermore, the final model processed the test dataset with an accuracy measurement of 86.50%.]\n",
    "\n",
    "CONCLUSION: In this modeling iteration, the bag-of-words TensorFlow model appeared to be suitable for modeling this dataset. We should consider experimenting with TensorFlow for further modeling.\n",
    "\n",
    "Dataset Used: [PROJECT NAME]\n",
    "\n",
    "Dataset ML Model: Binary class text classification with text-oriented features\n",
    "\n",
    "Dataset Reference: [http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz]\n",
    "\n",
    "One potential source of performance benchmarks: [https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/]\n",
    "\n",
    "A deep-learning text classification project generally can be broken down into five major tasks:\n",
    "\n",
    "1. Prepare Environment\n",
    "2. Load and Prepare Text Data\n",
    "3. Define and Train Models\n",
    "4. Evaluate and Optimize Models\n",
    "5. Finalize Model and Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZamo1ynR9Oz"
   },
   "source": [
    "# Task 1 - Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2N3QHMbUR9Oz",
    "outputId": "2e8152bf-215e-4bc1-f19f-3a208e1c9bfa"
   },
   "outputs": [],
   "source": [
    "# # Install the packages to support accessing environment variable and SQL databases\n",
    "# !pip install python-dotenv PyMySQL boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOWG9MDAR9O2",
    "outputId": "1e1308b7-ac4e-4b11-deeb-4e3877712fa7"
   },
   "outputs": [],
   "source": [
    "# # Retrieve GPU configuration information from Colab\n",
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#     print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "#     print('and then re-execute this cell.')\n",
    "# else:\n",
    "#     print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryCwWMZuR9O4",
    "outputId": "6bb39b0f-00e8-440a-e98c-1947cfd617b4"
   },
   "outputs": [],
   "source": [
    "# # Retrieve memory configuration information from Colab\n",
    "# from psutil import virtual_memory\n",
    "# ram_gb = virtual_memory().total / 1e9\n",
    "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "# if ram_gb < 20:\n",
    "#     print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
    "#     print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "#     print('re-execute this cell.')\n",
    "# else:\n",
    "#     print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfPiU9LaR9O7",
    "outputId": "6a68c8f3-d61e-4bb9-e6af-e2f946c184f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of available CPUs is: 4\n"
     ]
    }
   ],
   "source": [
    "# Retrieve CPU information from the system\n",
    "ncpu = !nproc\n",
    "print(\"The number of available CPUs is:\", ncpu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUw0fDwqR9O-"
   },
   "source": [
    "## 1.a) Load libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UpJD2SzRR9O-"
   },
   "outputs": [],
   "source": [
    "# Set the random seed number for reproducible results\n",
    "seedNum = 888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yDoBs19R9PA",
    "outputId": "5c5761c9-d774-4ba8-ba19-79a10c9f6a6e"
   },
   "outputs": [],
   "source": [
    "# Load libraries and packages\n",
    "import random\n",
    "random.seed(seedNum)\n",
    "import numpy as np\n",
    "np.random.seed(seedNum)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import shutil\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seedNum)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcIz9xXxR9PC"
   },
   "source": [
    "## 1.b) Set up the controlling parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YecJPSrfR9PC",
    "outputId": "bc0304a5-e471-4568-826e-750a5220e2a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n",
      "TensorFlow version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Begin the timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# Set up the number of CPU cores available for multi-thread processing\n",
    "n_jobs = 1\n",
    "\n",
    "# Set up the flag to stop sending progress emails (setting to True will send status emails!)\n",
    "notifyStatus = False\n",
    "\n",
    "# Set Pandas options\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# Set the percentage sizes for splitting the dataset\n",
    "test_set_size = 0.2\n",
    "val_set_size = 0.25\n",
    "\n",
    "# Set the number of folds for cross validation\n",
    "n_folds = 5\n",
    "n_iterations = 2\n",
    "\n",
    "# Set various default modeling parameters\n",
    "default_loss = 'binary_crossentropy'\n",
    "default_metrics = ['accuracy']\n",
    "default_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "default_kernel_init = tf.keras.initializers.GlorotUniform(seed=seedNum)\n",
    "default_epochs = 25\n",
    "default_batch_size = 16\n",
    "\n",
    "# Define the labels to use for graphing the data\n",
    "train_metric = \"accuracy\"\n",
    "validation_metric = \"val_accuracy\"\n",
    "train_loss = \"loss\"\n",
    "validation_loss = \"val_loss\"\n",
    "\n",
    "# Check the number of GPUs accessible through TensorFlow\n",
    "print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Print out the TensorFlow version for confirmation\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ggPdMUQfR9PE"
   },
   "outputs": [],
   "source": [
    "# Set up the parent directory location for loading the dotenv files\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# gdrivePrefix = '/content/gdrive/My Drive/Colab_Downloads/'\n",
    "# env_path = '/content/gdrive/My Drive/Colab Notebooks/'\n",
    "# dotenv_path = env_path + \"python_script.env\"\n",
    "# load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "# Set up the dotenv file for retrieving environment variables\n",
    "# env_path = \"/Users/david/PycharmProjects/\"\n",
    "# dotenv_path = env_path + \"python_script.env\"\n",
    "# load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1j-y-SvmR9PG"
   },
   "outputs": [],
   "source": [
    "# Set up the email notification function\n",
    "def status_notify(msg_text):\n",
    "    access_key = os.environ.get('SNS_ACCESS_KEY')\n",
    "    secret_key = os.environ.get('SNS_SECRET_KEY')\n",
    "    aws_region = os.environ.get('SNS_AWS_REGION')\n",
    "    topic_arn = os.environ.get('SNS_TOPIC_ARN')\n",
    "    if (access_key is None) or (secret_key is None) or (aws_region is None):\n",
    "        sys.exit(\"Incomplete notification setup info. Script Processing Aborted!!!\")\n",
    "    sns = boto3.client('sns', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=aws_region)\n",
    "    response = sns.publish(TopicArn=topic_arn, Message=msg_text)\n",
    "    if response['ResponseMetadata']['HTTPStatusCode'] != 200 :\n",
    "        print('Status notification not OK with HTTP status code:', response['ResponseMetadata']['HTTPStatusCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KDY6p9pWR9PI"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 1 - Prepare Environment has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YQtFgiRNR9PJ"
   },
   "outputs": [],
   "source": [
    "# Reset the random number generators\n",
    "def reset_random(x):\n",
    "    random.seed(x)\n",
    "    np.random.seed(x)\n",
    "    tf.random.set_seed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "id": "7auUp3NTR9PL",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 1 - Prepare Environment completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkkVYtk_R9PN"
   },
   "source": [
    "# Task 2 - Load and Prepare Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "id": "pX4nz8fYR9PN",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 2 - Load and Prepare Text Data has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a) Download Text Data Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "72uuLoG4R9PP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'review_polarity.tar.gz': No such file or directory\n",
      "rm: cannot remove 'vocabulary.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Clean up the old files and download directories before receiving new ones\n",
    "!rm -rf staging/\n",
    "!rm review_polarity.tar.gz\n",
    "!rm vocabulary.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpFEjo72R9PR",
    "outputId": "dd05f54a-fac8-40d3-e4a2-724354b083dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-20 19:09:59--  https://dainesanalytics.com/datasets/cornell-movie-review-polarity/review_polarity.tar.gz\n",
      "Resolving dainesanalytics.com (dainesanalytics.com)... 13.225.146.37, 13.225.146.126, 13.225.146.46, ...\n",
      "Connecting to dainesanalytics.com (dainesanalytics.com)|13.225.146.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3127238 (3.0M) [application/x-gzip]\n",
      "Saving to: ‘review_polarity.tar.gz’\n",
      "\n",
      "review_polarity.tar 100%[===================>]   2.98M  14.8MB/s    in 0.2s    \n",
      "\n",
      "2020-12-20 19:10:00 (14.8 MB/s) - ‘review_polarity.tar.gz’ saved [3127238/3127238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dainesanalytics.com/datasets/cornell-movie-review-polarity/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b) Splitting Data for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "toRUbKUuR9PV"
   },
   "outputs": [],
   "source": [
    "staging_dir = 'staging/'\n",
    "!mkdir staging/\n",
    "!mkdir staging/testing/\n",
    "!mkdir staging/testing/pos/\n",
    "!mkdir staging/testing/neg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tJJNvd7fR9PX"
   },
   "outputs": [],
   "source": [
    "local_archive = 'review_polarity.tar.gz'\n",
    "shutil.unpack_archive(local_archive, staging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "z-RNe5RDR9PY"
   },
   "outputs": [],
   "source": [
    "training_dir = 'staging/txt_sentoken/'\n",
    "testing_dir = 'staging/testing/'\n",
    "classA_name = 'pos'\n",
    "classB_name = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images for pos : 1000\n",
      "Training samples for pos : ['cv000_29590.txt', 'cv001_18431.txt', 'cv002_15918.txt', 'cv003_11664.txt', 'cv004_11636.txt', 'cv005_29443.txt', 'cv006_15448.txt', 'cv007_4968.txt', 'cv008_29435.txt', 'cv009_29592.txt']\n",
      "Number of training images for neg : 1000\n",
      "Training samples for neg : ['cv000_29416.txt', 'cv001_19502.txt', 'cv002_17424.txt', 'cv003_12683.txt', 'cv004_12641.txt', 'cv005_29357.txt', 'cv006_17022.txt', 'cv007_4992.txt', 'cv008_29326.txt', 'cv009_29417.txt']\n"
     ]
    }
   ],
   "source": [
    "# Brief listing of training text files for both classes before splitting\n",
    "training_classA_dir = os.path.join(training_dir, classA_name)\n",
    "training_classA_files = os.listdir(training_classA_dir)\n",
    "print('Number of training images for', classA_name, ':', len(training_classA_files))\n",
    "print('Training samples for', classA_name, ':', training_classA_files[:10])\n",
    "\n",
    "training_classB_dir = os.path.join(training_dir, classB_name)\n",
    "training_classB_files = os.listdir(training_classB_dir)\n",
    "print('Number of training images for', classB_name, ':', len(training_classB_files))\n",
    "print('Training samples for', classB_name, ':', training_classB_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving file from staging/txt_sentoken/pos/cv900_10331.txt to staging/testing/pos/cv900_10331.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv901_11017.txt to staging/testing/pos/cv901_11017.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv902_12256.txt to staging/testing/pos/cv902_12256.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv903_17822.txt to staging/testing/pos/cv903_17822.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv904_24353.txt to staging/testing/pos/cv904_24353.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv905_29114.txt to staging/testing/pos/cv905_29114.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv906_11491.txt to staging/testing/pos/cv906_11491.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv907_3541.txt to staging/testing/pos/cv907_3541.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv908_16009.txt to staging/testing/pos/cv908_16009.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv909_9960.txt to staging/testing/pos/cv909_9960.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv910_20488.txt to staging/testing/pos/cv910_20488.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv911_20260.txt to staging/testing/pos/cv911_20260.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv912_5674.txt to staging/testing/pos/cv912_5674.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv913_29252.txt to staging/testing/pos/cv913_29252.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv914_28742.txt to staging/testing/pos/cv914_28742.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv915_8841.txt to staging/testing/pos/cv915_8841.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv916_15467.txt to staging/testing/pos/cv916_15467.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv917_29715.txt to staging/testing/pos/cv917_29715.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv918_2693.txt to staging/testing/pos/cv918_2693.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv919_16380.txt to staging/testing/pos/cv919_16380.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv920_29622.txt to staging/testing/pos/cv920_29622.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv921_12747.txt to staging/testing/pos/cv921_12747.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv922_10073.txt to staging/testing/pos/cv922_10073.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv923_11051.txt to staging/testing/pos/cv923_11051.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv924_29540.txt to staging/testing/pos/cv924_29540.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv925_8969.txt to staging/testing/pos/cv925_8969.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv926_17059.txt to staging/testing/pos/cv926_17059.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv927_10681.txt to staging/testing/pos/cv927_10681.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv928_9168.txt to staging/testing/pos/cv928_9168.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv929_16908.txt to staging/testing/pos/cv929_16908.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv930_13475.txt to staging/testing/pos/cv930_13475.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv931_17563.txt to staging/testing/pos/cv931_17563.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv932_13401.txt to staging/testing/pos/cv932_13401.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv933_23776.txt to staging/testing/pos/cv933_23776.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv934_19027.txt to staging/testing/pos/cv934_19027.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv935_23841.txt to staging/testing/pos/cv935_23841.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv936_15954.txt to staging/testing/pos/cv936_15954.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv937_9811.txt to staging/testing/pos/cv937_9811.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv938_10220.txt to staging/testing/pos/cv938_10220.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv939_10583.txt to staging/testing/pos/cv939_10583.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv940_17705.txt to staging/testing/pos/cv940_17705.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv941_10246.txt to staging/testing/pos/cv941_10246.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv942_17082.txt to staging/testing/pos/cv942_17082.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv943_22488.txt to staging/testing/pos/cv943_22488.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv944_13521.txt to staging/testing/pos/cv944_13521.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv945_12160.txt to staging/testing/pos/cv945_12160.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv946_18658.txt to staging/testing/pos/cv946_18658.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv947_10601.txt to staging/testing/pos/cv947_10601.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv948_24606.txt to staging/testing/pos/cv948_24606.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv949_20112.txt to staging/testing/pos/cv949_20112.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv950_12350.txt to staging/testing/pos/cv950_12350.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv951_10926.txt to staging/testing/pos/cv951_10926.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv952_25240.txt to staging/testing/pos/cv952_25240.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv953_6836.txt to staging/testing/pos/cv953_6836.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv954_18628.txt to staging/testing/pos/cv954_18628.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv955_25001.txt to staging/testing/pos/cv955_25001.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv956_11609.txt to staging/testing/pos/cv956_11609.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv957_8737.txt to staging/testing/pos/cv957_8737.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv958_12162.txt to staging/testing/pos/cv958_12162.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv959_14611.txt to staging/testing/pos/cv959_14611.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv960_29007.txt to staging/testing/pos/cv960_29007.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv961_5682.txt to staging/testing/pos/cv961_5682.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv962_9803.txt to staging/testing/pos/cv962_9803.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv963_6895.txt to staging/testing/pos/cv963_6895.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv964_6021.txt to staging/testing/pos/cv964_6021.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv965_26071.txt to staging/testing/pos/cv965_26071.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv966_28832.txt to staging/testing/pos/cv966_28832.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv967_5788.txt to staging/testing/pos/cv967_5788.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv968_24218.txt to staging/testing/pos/cv968_24218.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv969_13250.txt to staging/testing/pos/cv969_13250.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv970_18450.txt to staging/testing/pos/cv970_18450.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv971_10874.txt to staging/testing/pos/cv971_10874.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv972_26417.txt to staging/testing/pos/cv972_26417.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv973_10066.txt to staging/testing/pos/cv973_10066.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv974_22941.txt to staging/testing/pos/cv974_22941.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv975_10981.txt to staging/testing/pos/cv975_10981.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv976_10267.txt to staging/testing/pos/cv976_10267.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv977_4938.txt to staging/testing/pos/cv977_4938.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv978_20929.txt to staging/testing/pos/cv978_20929.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv979_18921.txt to staging/testing/pos/cv979_18921.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv980_10953.txt to staging/testing/pos/cv980_10953.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv981_14989.txt to staging/testing/pos/cv981_14989.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv982_21103.txt to staging/testing/pos/cv982_21103.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv983_22928.txt to staging/testing/pos/cv983_22928.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv984_12767.txt to staging/testing/pos/cv984_12767.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv985_6359.txt to staging/testing/pos/cv985_6359.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv986_13527.txt to staging/testing/pos/cv986_13527.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv987_6965.txt to staging/testing/pos/cv987_6965.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv988_18740.txt to staging/testing/pos/cv988_18740.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv989_15824.txt to staging/testing/pos/cv989_15824.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv990_11591.txt to staging/testing/pos/cv990_11591.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv991_18645.txt to staging/testing/pos/cv991_18645.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv992_11962.txt to staging/testing/pos/cv992_11962.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv993_29737.txt to staging/testing/pos/cv993_29737.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv994_12270.txt to staging/testing/pos/cv994_12270.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv995_21821.txt to staging/testing/pos/cv995_21821.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv996_11592.txt to staging/testing/pos/cv996_11592.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv997_5046.txt to staging/testing/pos/cv997_5046.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv998_14111.txt to staging/testing/pos/cv998_14111.txt\n",
      "Moving file from staging/txt_sentoken/pos/cv999_13106.txt to staging/testing/pos/cv999_13106.txt\n",
      "Number of pos files moved: 100 \n",
      "\n",
      "Moving file from staging/txt_sentoken/neg/cv900_10800.txt to staging/testing/neg/cv900_10800.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv901_11934.txt to staging/testing/neg/cv901_11934.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv902_13217.txt to staging/testing/neg/cv902_13217.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv903_18981.txt to staging/testing/neg/cv903_18981.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv904_25663.txt to staging/testing/neg/cv904_25663.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv905_28965.txt to staging/testing/neg/cv905_28965.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv906_12332.txt to staging/testing/neg/cv906_12332.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv907_3193.txt to staging/testing/neg/cv907_3193.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv908_17779.txt to staging/testing/neg/cv908_17779.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv909_9973.txt to staging/testing/neg/cv909_9973.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv910_21930.txt to staging/testing/neg/cv910_21930.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv911_21695.txt to staging/testing/neg/cv911_21695.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv912_5562.txt to staging/testing/neg/cv912_5562.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv913_29127.txt to staging/testing/neg/cv913_29127.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv914_2856.txt to staging/testing/neg/cv914_2856.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv915_9342.txt to staging/testing/neg/cv915_9342.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv916_17034.txt to staging/testing/neg/cv916_17034.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv917_29484.txt to staging/testing/neg/cv917_29484.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv918_27080.txt to staging/testing/neg/cv918_27080.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv919_18155.txt to staging/testing/neg/cv919_18155.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv920_29423.txt to staging/testing/neg/cv920_29423.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv921_13988.txt to staging/testing/neg/cv921_13988.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv922_10185.txt to staging/testing/neg/cv922_10185.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv923_11951.txt to staging/testing/neg/cv923_11951.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv924_29397.txt to staging/testing/neg/cv924_29397.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv925_9459.txt to staging/testing/neg/cv925_9459.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv926_18471.txt to staging/testing/neg/cv926_18471.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv927_11471.txt to staging/testing/neg/cv927_11471.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv928_9478.txt to staging/testing/neg/cv928_9478.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv929_1841.txt to staging/testing/neg/cv929_1841.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv930_14949.txt to staging/testing/neg/cv930_14949.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv931_18783.txt to staging/testing/neg/cv931_18783.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv932_14854.txt to staging/testing/neg/cv932_14854.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv933_24953.txt to staging/testing/neg/cv933_24953.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv934_20426.txt to staging/testing/neg/cv934_20426.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv935_24977.txt to staging/testing/neg/cv935_24977.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv936_17473.txt to staging/testing/neg/cv936_17473.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv937_9816.txt to staging/testing/neg/cv937_9816.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv938_10706.txt to staging/testing/neg/cv938_10706.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv939_11247.txt to staging/testing/neg/cv939_11247.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv940_18935.txt to staging/testing/neg/cv940_18935.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv941_10718.txt to staging/testing/neg/cv941_10718.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv942_18509.txt to staging/testing/neg/cv942_18509.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv943_23547.txt to staging/testing/neg/cv943_23547.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv944_15042.txt to staging/testing/neg/cv944_15042.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv945_13012.txt to staging/testing/neg/cv945_13012.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv946_20084.txt to staging/testing/neg/cv946_20084.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv947_11316.txt to staging/testing/neg/cv947_11316.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv948_25870.txt to staging/testing/neg/cv948_25870.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv949_21565.txt to staging/testing/neg/cv949_21565.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv950_13478.txt to staging/testing/neg/cv950_13478.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv951_11816.txt to staging/testing/neg/cv951_11816.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv952_26375.txt to staging/testing/neg/cv952_26375.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv953_7078.txt to staging/testing/neg/cv953_7078.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv954_19932.txt to staging/testing/neg/cv954_19932.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv955_26154.txt to staging/testing/neg/cv955_26154.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv956_12547.txt to staging/testing/neg/cv956_12547.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv957_9059.txt to staging/testing/neg/cv957_9059.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv958_13020.txt to staging/testing/neg/cv958_13020.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv959_16218.txt to staging/testing/neg/cv959_16218.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv960_28877.txt to staging/testing/neg/cv960_28877.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv961_5578.txt to staging/testing/neg/cv961_5578.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv962_9813.txt to staging/testing/neg/cv962_9813.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv963_7208.txt to staging/testing/neg/cv963_7208.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv964_5794.txt to staging/testing/neg/cv964_5794.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv965_26688.txt to staging/testing/neg/cv965_26688.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv966_28671.txt to staging/testing/neg/cv966_28671.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv967_5626.txt to staging/testing/neg/cv967_5626.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv968_25413.txt to staging/testing/neg/cv968_25413.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv969_14760.txt to staging/testing/neg/cv969_14760.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv970_19532.txt to staging/testing/neg/cv970_19532.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv971_11790.txt to staging/testing/neg/cv971_11790.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv972_26837.txt to staging/testing/neg/cv972_26837.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv973_10171.txt to staging/testing/neg/cv973_10171.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv974_24303.txt to staging/testing/neg/cv974_24303.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv975_11920.txt to staging/testing/neg/cv975_11920.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv976_10724.txt to staging/testing/neg/cv976_10724.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv977_4776.txt to staging/testing/neg/cv977_4776.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv978_22192.txt to staging/testing/neg/cv978_22192.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv979_2029.txt to staging/testing/neg/cv979_2029.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv980_11851.txt to staging/testing/neg/cv980_11851.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv981_16679.txt to staging/testing/neg/cv981_16679.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv982_22209.txt to staging/testing/neg/cv982_22209.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv983_24219.txt to staging/testing/neg/cv983_24219.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv984_14006.txt to staging/testing/neg/cv984_14006.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv985_5964.txt to staging/testing/neg/cv985_5964.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv986_15092.txt to staging/testing/neg/cv986_15092.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv987_7394.txt to staging/testing/neg/cv987_7394.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv988_20168.txt to staging/testing/neg/cv988_20168.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv989_17297.txt to staging/testing/neg/cv989_17297.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv990_12443.txt to staging/testing/neg/cv990_12443.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv991_19973.txt to staging/testing/neg/cv991_19973.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv992_12806.txt to staging/testing/neg/cv992_12806.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv993_29565.txt to staging/testing/neg/cv993_29565.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv994_13229.txt to staging/testing/neg/cv994_13229.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv995_23113.txt to staging/testing/neg/cv995_23113.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv996_12447.txt to staging/testing/neg/cv996_12447.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv997_5152.txt to staging/testing/neg/cv997_5152.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv998_15691.txt to staging/testing/neg/cv998_15691.txt\n",
      "Moving file from staging/txt_sentoken/neg/cv999_14636.txt to staging/testing/neg/cv999_14636.txt\n",
      "Number of neg files moved: 100\n"
     ]
    }
   ],
   "source": [
    "# Move the testing files from the training directories\n",
    "testing_classA_dir = os.path.join(testing_dir, classA_name)\n",
    "testing_classB_dir = os.path.join(testing_dir, classB_name)\n",
    "file_prefix = 'cv9'\n",
    "\n",
    "i = 0\n",
    "for text_file in training_classA_files:\n",
    "    if text_file.startswith(file_prefix):\n",
    "        source_file = training_classA_dir + '/' + text_file\n",
    "        dest_file = testing_classA_dir + '/' + text_file\n",
    "        print('Moving file from', source_file, 'to', dest_file)\n",
    "        shutil.move(source_file, dest_file)\n",
    "        i = i + 1\n",
    "print('Number of', classA_name, 'files moved:', i, '\\n')\n",
    "\n",
    "i = 0\n",
    "for text_file in training_classB_files:\n",
    "    if text_file.startswith(file_prefix):\n",
    "        source_file = training_classB_dir + '/' + text_file\n",
    "        dest_file = testing_classB_dir + '/' + text_file\n",
    "        print('Moving file from', source_file, 'to', dest_file)\n",
    "        shutil.move(source_file, dest_file)\n",
    "        i = i + 1\n",
    "print('Number of', classB_name, 'files moved:', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files for pos : 900\n",
      "Training samples for pos : ['cv000_29590.txt', 'cv001_18431.txt', 'cv002_15918.txt', 'cv003_11664.txt', 'cv004_11636.txt', 'cv005_29443.txt', 'cv006_15448.txt', 'cv007_4968.txt', 'cv008_29435.txt', 'cv009_29592.txt']\n",
      "Number of training files for neg : 900\n",
      "Training samples for neg : ['cv000_29416.txt', 'cv001_19502.txt', 'cv002_17424.txt', 'cv003_12683.txt', 'cv004_12641.txt', 'cv005_29357.txt', 'cv006_17022.txt', 'cv007_4992.txt', 'cv008_29326.txt', 'cv009_29417.txt']\n"
     ]
    }
   ],
   "source": [
    "# Brief listing of training text files for both classes after splitting\n",
    "training_classA_files = os.listdir(training_classA_dir)\n",
    "print('Number of training files for', classA_name, ':', len(training_classA_files))\n",
    "print('Training samples for', classA_name, ':', training_classA_files[:10])\n",
    "\n",
    "training_classB_files = os.listdir(training_classB_dir)\n",
    "print('Number of training files for', classB_name, ':', len(training_classB_files))\n",
    "print('Training samples for', classB_name, ':', training_classB_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing files for pos : 100\n",
      "Test samples for pos : ['cv900_10331.txt', 'cv901_11017.txt', 'cv902_12256.txt', 'cv903_17822.txt', 'cv904_24353.txt', 'cv905_29114.txt', 'cv906_11491.txt', 'cv907_3541.txt', 'cv908_16009.txt', 'cv909_9960.txt']\n",
      "Number of testing files for neg : 100\n",
      "Test samples for neg : ['cv900_10800.txt', 'cv901_11934.txt', 'cv902_13217.txt', 'cv903_18981.txt', 'cv904_25663.txt', 'cv905_28965.txt', 'cv906_12332.txt', 'cv907_3193.txt', 'cv908_17779.txt', 'cv909_9973.txt']\n"
     ]
    }
   ],
   "source": [
    "# Brief listing of testing text files for both classes after splitting\n",
    "testing_classA_files = os.listdir(testing_classA_dir)\n",
    "print('Number of testing files for', classA_name, ':', len(testing_classA_files))\n",
    "print('Test samples for', classA_name, ':', testing_classA_files[:10])\n",
    "\n",
    "testing_classB_files = os.listdir(testing_classB_dir)\n",
    "print('Number of testing files for', classB_name, ':', len(testing_classB_files))\n",
    "print('Test samples for', classB_name, ':', testing_classB_files[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.c) Load Document and Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6ajSXvMR9Pb",
    "outputId": "d055509f-c17a-43d0-bffd-4406b1039775"
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def build_vocab_from_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    i = 0\n",
    "    print('Processing the text files and showing the first 10...')\n",
    "    for filename in os.listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        i = i + 1\n",
    "        if i < 10: print('Loaded %s' % path)\n",
    "    print('Total number of text files loaded:', i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the text files and showing the first 10...\n",
      "Loaded staging/txt_sentoken/pos/cv000_29590.txt\n",
      "Loaded staging/txt_sentoken/pos/cv001_18431.txt\n",
      "Loaded staging/txt_sentoken/pos/cv002_15918.txt\n",
      "Loaded staging/txt_sentoken/pos/cv003_11664.txt\n",
      "Loaded staging/txt_sentoken/pos/cv004_11636.txt\n",
      "Loaded staging/txt_sentoken/pos/cv005_29443.txt\n",
      "Loaded staging/txt_sentoken/pos/cv006_15448.txt\n",
      "Loaded staging/txt_sentoken/pos/cv007_4968.txt\n",
      "Loaded staging/txt_sentoken/pos/cv008_29435.txt\n",
      "Total number of text files loaded: 900 \n",
      "\n",
      "Processing the text files and showing the first 10...\n",
      "Loaded staging/txt_sentoken/neg/cv000_29416.txt\n",
      "Loaded staging/txt_sentoken/neg/cv001_19502.txt\n",
      "Loaded staging/txt_sentoken/neg/cv002_17424.txt\n",
      "Loaded staging/txt_sentoken/neg/cv003_12683.txt\n",
      "Loaded staging/txt_sentoken/neg/cv004_12641.txt\n",
      "Loaded staging/txt_sentoken/neg/cv005_29357.txt\n",
      "Loaded staging/txt_sentoken/neg/cv006_17022.txt\n",
      "Loaded staging/txt_sentoken/neg/cv007_4992.txt\n",
      "Loaded staging/txt_sentoken/neg/cv008_29326.txt\n",
      "Total number of text files loaded: 900 \n",
      "\n",
      "The total number of words in the vocabulary: 44276\n",
      "The top 50 words in the vocabulary:\n",
      " [('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "build_vocab_from_docs(training_classA_dir, vocab)\n",
    "build_vocab_from_docs(training_classB_dir, vocab)\n",
    "# print the size of the vocab\n",
    "print('The total number of words in the vocabulary:', len(vocab))\n",
    "# print the top words in the vocab\n",
    "top_words = 50\n",
    "print('The top', top_words, 'words in the vocabulary:\\n', vocab.most_common(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words with the minimum appearance: 25767\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print('The number of words with the minimum appearance:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocabulary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.d) Create Tokenizer and Encode the Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs_to_lines(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in os.listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the vocabulary: 25767\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocabulary.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print('Number of tokens in the vocabulary:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare bag of words encoding of docs\n",
    "def encode_data(train_docs, val_docs, mode='binary'):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training data set\n",
    "    train_encoded = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode validation data set\n",
    "    val_encoded = tokenizer.texts_to_matrix(val_docs, mode=mode)\n",
    "    return train_encoded, val_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive reviews processed: 900\n",
      "The number of negative reviews processed: 900\n",
      "The shape of the encoded training classes: (1800,)\n"
     ]
    }
   ],
   "source": [
    "# Load all training cases\n",
    "positive_train_cases = process_docs_to_lines(training_classA_dir, vocab)\n",
    "print('The number of positive reviews processed:', len(positive_train_cases))\n",
    "negative_train_cases = process_docs_to_lines(training_classB_dir, vocab)\n",
    "print('The number of negative reviews processed:', len(negative_train_cases))\n",
    "training_docs =  negative_train_cases + positive_train_cases\n",
    "y_train = np.array([0 for _ in range(len(negative_train_cases))] + [1 for _ in range(len(positive_train_cases))])\n",
    "print('The shape of the encoded training classes:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive reviews processed: 100\n",
      "The number of negative reviews processed: 100\n",
      "The shape of the encoded test classes: (200,)\n"
     ]
    }
   ],
   "source": [
    "# load all validation cases\n",
    "positive_test_cases = process_docs_to_lines(testing_classA_dir, vocab)\n",
    "print('The number of positive reviews processed:', len(positive_test_cases))\n",
    "negative_test_cases = process_docs_to_lines(testing_classB_dir, vocab)\n",
    "print('The number of negative reviews processed:', len(negative_test_cases))\n",
    "testing_docs = negative_test_cases + positive_test_cases\n",
    "y_test = np.array([0 for _ in range(len(negative_test_cases))] + [1 for _ in range(len(positive_test_cases))])\n",
    "print('The shape of the encoded test classes:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training dataset: (1800, 25768)\n",
      "The shape of the encoded validation dataset: (200, 25768)\n"
     ]
    }
   ],
   "source": [
    "# encode training and validation datasets\n",
    "X_train, X_test = encode_data(training_docs, testing_docs)\n",
    "print('The shape of the encoded training dataset:', X_train.shape)\n",
    "print('The shape of the encoded validation dataset:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "JdeBWbYeR9Pi"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 2 - Load and Prepare Text Data completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGVOq6P6R9Pj"
   },
   "source": [
    "# Task 3 - Define and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "W52KEpCWR9Pk"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 3 - Define and Train Models has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the default numbers of input/output for modeling\n",
    "num_inputs = X_train.shape[1]\n",
    "num_outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the baseline model for benchmarking\n",
    "def create_nn_model(n_inputs=num_inputs, n_outputs=num_outputs, layer1_nodes=50, layer1_dropout=0, opt_param=default_optimizer, init_param=default_kernel_init):\n",
    "    nn_model = keras.Sequential([\n",
    "        keras.layers.Dense(layer1_nodes, input_shape=(n_inputs,), activation='relu', kernel_initializer=init_param),\n",
    "#         keras.layers.Dropout(layer1_dropout),\n",
    "        keras.layers.Dense(n_outputs, activation='sigmoid', kernel_initializer=init_param)\n",
    "    ])\n",
    "    nn_model.compile(loss=default_loss, optimizer=opt_param, metrics=default_metrics)\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy measurement from iteration 1 >>> 87.22%\n",
      "Accuracy measurement from iteration 2 >>> 84.72%\n",
      "Accuracy measurement from iteration 3 >>> 85.28%\n",
      "Accuracy measurement from iteration 4 >>> 83.89%\n",
      "Accuracy measurement from iteration 5 >>> 86.11%\n",
      "Accuracy measurement from iteration 6 >>> 86.67%\n",
      "Accuracy measurement from iteration 7 >>> 83.33%\n",
      "Accuracy measurement from iteration 8 >>> 86.94%\n",
      "Accuracy measurement from iteration 9 >>> 87.22%\n",
      "Accuracy measurement from iteration 10 >>> 86.67%\n",
      "Average model accuracy from all iterations: 85.81% (1.35%)\n",
      "Total time for model fitting and cross validating: 0:02:50.603388\n"
     ]
    }
   ],
   "source": [
    "# Initialize the default model and get a baseline result\n",
    "startTimeModule = datetime.now()\n",
    "results = list()\n",
    "iteration = 0\n",
    "cv = RepeatedKFold(n_splits=n_folds, n_repeats=n_iterations, random_state=seedNum)\n",
    "for train_ix, val_ix in cv.split(X_train):\n",
    "    feature_train, feature_validation = X_train[train_ix], X_train[val_ix]\n",
    "    target_train, target_validation = y_train[train_ix], y_train[val_ix]\n",
    "    reset_random(seedNum)\n",
    "    baseline_model = create_nn_model()\n",
    "    baseline_model.fit(feature_train, target_train, epochs=default_epochs, batch_size=default_batch_size, verbose=0)\n",
    "    model_metric = baseline_model.evaluate(feature_validation, target_validation, verbose=0)[1]\n",
    "    iteration = iteration + 1\n",
    "    print('Accuracy measurement from iteration %d >>> %.2f%%' % (iteration, model_metric*100))\n",
    "    results.append(model_metric)\n",
    "validation_score = np.mean(results)\n",
    "validation_variance = np.std(results)\n",
    "print('Average model accuracy from all iterations: %.2f%% (%.2f%%)' % (validation_score*100, validation_variance*100))\n",
    "print('Total time for model fitting and cross validating:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "id": "5vzQkIfyR9Ps",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 3 - Define and Train Models completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tmuo6X06R9Pu"
   },
   "source": [
    "# Task 4 - Evaluate and Optimize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "id": "h-Xp746uR9Pu",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 4 - Evaluate and Optimize Models has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a) Alternate Model One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training dataset: (1800, 25768)\n",
      "The shape of the encoded validation dataset: (200, 25768)\n"
     ]
    }
   ],
   "source": [
    "# encode training and validation datasets\n",
    "X_train, X_test = encode_data(training_docs, testing_docs, 'count')\n",
    "print('The shape of the encoded training dataset:', X_train.shape)\n",
    "print('The shape of the encoded validation dataset:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy measurement from iteration 1 >>> 83.33%\n",
      "Accuracy measurement from iteration 2 >>> 86.11%\n",
      "Accuracy measurement from iteration 3 >>> 85.00%\n",
      "Accuracy measurement from iteration 4 >>> 82.22%\n",
      "Accuracy measurement from iteration 5 >>> 84.17%\n",
      "Accuracy measurement from iteration 6 >>> 82.78%\n",
      "Accuracy measurement from iteration 7 >>> 82.50%\n",
      "Accuracy measurement from iteration 8 >>> 86.11%\n",
      "Accuracy measurement from iteration 9 >>> 87.50%\n",
      "Accuracy measurement from iteration 10 >>> 83.61%\n",
      "Average model accuracy from all iterations: 84.33% (1.69%)\n",
      "Total time for model fitting and cross validating: 0:02:32.864166\n"
     ]
    }
   ],
   "source": [
    "# Initialize the default model and get a baseline result\n",
    "startTimeModule = datetime.now()\n",
    "results = list()\n",
    "iteration = 0\n",
    "cv = RepeatedKFold(n_splits=n_folds, n_repeats=n_iterations, random_state=seedNum)\n",
    "for train_ix, val_ix in cv.split(X_train):\n",
    "    feature_train, feature_validation = X_train[train_ix], X_train[val_ix]\n",
    "    target_train, target_validation = y_train[train_ix], y_train[val_ix]\n",
    "    reset_random(seedNum)\n",
    "    alternate_model_1 = create_nn_model()\n",
    "    alternate_model_1.fit(feature_train, target_train, epochs=default_epochs, batch_size=default_batch_size, verbose=0)\n",
    "    model_metric = alternate_model_1.evaluate(feature_validation, target_validation, verbose=0)[1]\n",
    "    iteration = iteration + 1\n",
    "    print('Accuracy measurement from iteration %d >>> %.2f%%' % (iteration, model_metric*100))\n",
    "    results.append(model_metric)\n",
    "validation_score = np.mean(results)\n",
    "validation_variance = np.std(results)\n",
    "print('Average model accuracy from all iterations: %.2f%% (%.2f%%)' % (validation_score*100, validation_variance*100))\n",
    "print('Total time for model fitting and cross validating:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a) Alternate Model Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training dataset: (1800, 25768)\n",
      "The shape of the encoded validation dataset: (200, 25768)\n"
     ]
    }
   ],
   "source": [
    "# encode training and validation datasets\n",
    "X_train, X_test = encode_data(training_docs, testing_docs, 'tfidf')\n",
    "print('The shape of the encoded training dataset:', X_train.shape)\n",
    "print('The shape of the encoded validation dataset:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy measurement from iteration 1 >>> 83.33%\n",
      "Accuracy measurement from iteration 2 >>> 81.67%\n",
      "Accuracy measurement from iteration 3 >>> 83.33%\n",
      "Accuracy measurement from iteration 4 >>> 82.78%\n",
      "Accuracy measurement from iteration 5 >>> 79.44%\n",
      "Accuracy measurement from iteration 6 >>> 79.72%\n",
      "Accuracy measurement from iteration 7 >>> 80.83%\n",
      "Accuracy measurement from iteration 8 >>> 82.50%\n",
      "Accuracy measurement from iteration 9 >>> 86.39%\n",
      "Accuracy measurement from iteration 10 >>> 83.33%\n",
      "Average model accuracy from all iterations: 82.33% (1.94%)\n",
      "Total time for model fitting and cross validating: 0:02:34.509359\n"
     ]
    }
   ],
   "source": [
    "# Initialize the default model and get a baseline result\n",
    "startTimeModule = datetime.now()\n",
    "results = list()\n",
    "iteration = 0\n",
    "cv = RepeatedKFold(n_splits=n_folds, n_repeats=n_iterations, random_state=seedNum)\n",
    "for train_ix, val_ix in cv.split(X_train):\n",
    "    feature_train, feature_validation = X_train[train_ix], X_train[val_ix]\n",
    "    target_train, target_validation = y_train[train_ix], y_train[val_ix]\n",
    "    reset_random(seedNum)\n",
    "    alternate_model_2 = create_nn_model()\n",
    "    alternate_model_2.fit(feature_train, target_train, epochs=default_epochs, batch_size=default_batch_size, verbose=0)\n",
    "    model_metric = alternate_model_2.evaluate(feature_validation, target_validation, verbose=0)[1]\n",
    "    iteration = iteration + 1\n",
    "    print('Accuracy measurement from iteration %d >>> %.2f%%' % (iteration, model_metric*100))\n",
    "    results.append(model_metric)\n",
    "validation_score = np.mean(results)\n",
    "validation_variance = np.std(results)\n",
    "print('Average model accuracy from all iterations: %.2f%% (%.2f%%)' % (validation_score*100, validation_variance*100))\n",
    "print('Total time for model fitting and cross validating:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a) Alternate Model Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training dataset: (1800, 25768)\n",
      "The shape of the encoded validation dataset: (200, 25768)\n"
     ]
    }
   ],
   "source": [
    "# encode training and validation datasets\n",
    "X_train, X_test = encode_data(training_docs, testing_docs, 'freq')\n",
    "print('The shape of the encoded training dataset:', X_train.shape)\n",
    "print('The shape of the encoded validation dataset:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy measurement from iteration 1 >>> 88.06%\n",
      "Accuracy measurement from iteration 2 >>> 86.94%\n",
      "Accuracy measurement from iteration 3 >>> 85.56%\n",
      "Accuracy measurement from iteration 4 >>> 85.00%\n",
      "Accuracy measurement from iteration 5 >>> 87.22%\n",
      "Accuracy measurement from iteration 6 >>> 84.72%\n",
      "Accuracy measurement from iteration 7 >>> 84.72%\n",
      "Accuracy measurement from iteration 8 >>> 86.39%\n",
      "Accuracy measurement from iteration 9 >>> 86.39%\n",
      "Accuracy measurement from iteration 10 >>> 85.56%\n",
      "Average model accuracy from all iterations: 86.06% (1.07%)\n",
      "Total time for model fitting and cross validating: 0:02:34.788111\n"
     ]
    }
   ],
   "source": [
    "# Initialize the default model and get a baseline result\n",
    "startTimeModule = datetime.now()\n",
    "results = list()\n",
    "iteration = 0\n",
    "cv = RepeatedKFold(n_splits=n_folds, n_repeats=n_iterations, random_state=seedNum)\n",
    "for train_ix, val_ix in cv.split(X_train):\n",
    "    feature_train, feature_validation = X_train[train_ix], X_train[val_ix]\n",
    "    target_train, target_validation = y_train[train_ix], y_train[val_ix]\n",
    "    reset_random(seedNum)\n",
    "    alternate_model_3 = create_nn_model()\n",
    "    alternate_model_3.fit(feature_train, target_train, epochs=default_epochs, batch_size=default_batch_size, verbose=0)\n",
    "    model_metric = alternate_model_3.evaluate(feature_validation, target_validation, verbose=0)[1]\n",
    "    iteration = iteration + 1\n",
    "    print('Accuracy measurement from iteration %d >>> %.2f%%' % (iteration, model_metric*100))\n",
    "    results.append(model_metric)\n",
    "validation_score = np.mean(results)\n",
    "validation_variance = np.std(results)\n",
    "print('Average model accuracy from all iterations: %.2f%% (%.2f%%)' % (validation_score*100, validation_variance*100))\n",
    "print('Total time for model fitting and cross validating:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "6n7j3PwoR9P0"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 4 - Evaluate and Optimize Models completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRcXHaGhR9P1"
   },
   "source": [
    "# Task 5 - Finalize Model and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "p3tj_9tBR9P2"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 5 - Finalize Model and Make Predictions has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training dataset: (1800, 25768)\n",
      "The shape of the encoded validation dataset: (200, 25768)\n"
     ]
    }
   ],
   "source": [
    "# encode training and validation datasets\n",
    "X_train, X_test = encode_data(training_docs, testing_docs, 'freq')\n",
    "print('The shape of the encoded training dataset:', X_train.shape)\n",
    "print('The shape of the encoded validation dataset:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.6720 - accuracy: 0.5828\n",
      "Epoch 2/25\n",
      "113/113 [==============================] - 1s 8ms/step - loss: 0.5542 - accuracy: 0.8000\n",
      "Epoch 3/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.4412 - accuracy: 0.9150\n",
      "Epoch 4/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.3592 - accuracy: 0.9639\n",
      "Epoch 5/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.3007 - accuracy: 0.9806\n",
      "Epoch 6/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.2557 - accuracy: 0.9894\n",
      "Epoch 7/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.2209 - accuracy: 0.9944\n",
      "Epoch 8/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.1929 - accuracy: 0.9967\n",
      "Epoch 9/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.1699 - accuracy: 0.9983\n",
      "Epoch 10/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.1512 - accuracy: 0.9983\n",
      "Epoch 11/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.1358 - accuracy: 1.0000\n",
      "Epoch 12/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.1226 - accuracy: 1.0000\n",
      "Epoch 13/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.1114 - accuracy: 1.0000\n",
      "Epoch 14/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.1016 - accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0932 - accuracy: 1.0000\n",
      "Epoch 16/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0857 - accuracy: 1.0000\n",
      "Epoch 17/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0791 - accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0732 - accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0679 - accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0631 - accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0588 - accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0549 - accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0513 - accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0481 - accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0451 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb8157d3700>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the final model for evaluating the test dataset\n",
    "reset_random(seedNum)\n",
    "final_model = create_nn_model()\n",
    "final_model.fit(X_train, y_train, epochs=default_epochs, batch_size=default_batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_80 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the final model\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.865\n",
      "[[97  3]\n",
      " [24 76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88       100\n",
      "           1       0.96      0.76      0.85       100\n",
      "\n",
      "    accuracy                           0.86       200\n",
      "   macro avg       0.88      0.86      0.86       200\n",
      "weighted avg       0.88      0.86      0.86       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_predictions = final_model.predict(X_test, batch_size=default_batch, verbose=1)\n",
    "test_predictions = (final_model.predict(X_test) > 0.5).astype(\"int32\").ravel()\n",
    "print('Accuracy Score:', accuracy_score(y_test, test_predictions))\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "S47yeSrOR9P-"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 5 - Finalize Model and Make Predictions completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_kTqSp4R9QA",
    "outputId": "38f72f65-eea4-443f-cc58-62b4d17cb7b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for the script: 0:12:08.151219\n"
     ]
    }
   ],
   "source": [
    "print ('Total time for the script:',(datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "py_tensorflow_binaryclass_image_classification_example.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
